# Install
```bash
# Install virtual box
sudo apt update
sudo apt install virtualbox
VBoxManage --version
```
# How to **Enable Nested VT-x/AMD-V** On Virtualbox
```sh
# Command Prompt (CMD) Run as administrator
cd C:\Program Files\Oracle\VirtualBox
# Command:
VBoxManage modifyvm "YourVirtualMachineName" --nested-hw-virt on

# Example:
VBoxManage modifyvm "master01" --nested-hw-virt on
```
# Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
```bash
kubectl version --client
```
```bash
# set up autocomplete in bash into the current shell, bash-completion package should be installed first.
# add autocomplete permanently to your bash shell.
echo "source <(kubectl completion bash)" >> ~/.bashrc
```
```vim
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
```
```bash
source ~/.bashrc
```
```bash
cat ~/.bashrc
```
# Install [minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download)
```bash
minikube start
minikube start --driver=virtualbox
minikube status
minikube config get driver
```

---

# Install on windows
##### Download [`kubectl.exe`](https://kubernetes.io/releases/download/#binaries) 
- Create a folder in Drive `C:\kube`
- Move the the `kubectl.exe` to this folder
- Search for the `Edit the system environment variables`
- Environment Variables
- Click on Path > New > `C:\kube`
- Set this path for User variables and System variables
```bash
kubectl version --client
```
##### Download [`minikube.exe`](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download)
- Add the `minikube.exe` binary to your `PATH`.
- Make sure to run PowerShell as Administrator.
```bash
$oldPath = [Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine)
if ($oldPath.Split(';') -inotcontains 'C:\minikube'){
  [Environment]::SetEnvironmentVariable('Path', $('{0};C:\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine)
}
```
```bash
minikube start
minikube status
```
Minikube multi node
```bash
minikube start --nodes 3 -p <name> --forece
minikube node add --worker -p <name>
```

---

# [Deploy a Kubernetes Cluster using Kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
## "I ensure that all nodes are on the same network and can communicate with each other, assigning each one a unique and distinct hostname."
### These steps have to be performed on `all nodes`.
`set net.bridge.bridge-nf-call-iptables` to `1`: 
```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter
```
```bash
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sudo sysctl --system
```
---
# Check required ports
```bash
nc 127.0.0.1 6443 -v
```
---
# Disable Swap
Kubernetes need to be disabled SWAPs, because using Swap can make Kuberinets not make the right scheduling decisions.
- To disable swap, `sudo swapoff -a` can be used to disable swapping temporarily. To make this change persistent across reboots, make sure swap is disabled in config files like `/etc/fstab,` `systemd.swap,` depending how it was configured on your system.
- Comment the swap line in `/etc/fstab`
  ```bash
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  ```
  Make sure!
  ```bash
  sudo cat /etc/fstab
  ```

---

# [Installing a container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

| Runtime       | Path to Unix domain socket                      |
|--------------|-----------------------------------------------|
| containerd   | unix:///var/run/containerd/containerd.sock   |
| CRI-O        | unix:///var/run/crio/crio.sock              |
| Docker Engine (using cri-dockerd) | unix:///var/run/cri-dockerd.sock |


```bash
sudo apt install containerd -y

sudo mkdir -p /etc/containerd
containerd config default
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml

sudo systemctl restart containerd
```
---
Install `kubeadm`, `kubectl` and `kubelet` on all nodes:
```bash
sudo apt-get update

sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
- Note:
In releases older than Debian 12 and Ubuntu 22.04, directory `/etc/apt/keyrings` does not exist by default, and it should be created before the curl command.
    ```bash
    mkdir /etc/apt/keyrings
    ```
```bash
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
```
Install Latest Versions
```bash
sudo apt install -y kubelet kubeadm kubectl
```
Or Install Specific Versions
```bash
# To see the new version labels
sudo apt-cache madison kubeadm

sudo apt-get install -y kubelet=1.32.0-1.1 kubeadm=1.32.0-1.1 kubectl=1.32.0-1.1
```
Prevent Automatic Updates
```bash
sudo apt-mark hold kubelet kubeadm kubectl
```
Pull Images:
```bash
sudo kubeadm config images pull
```
---
# [Creating a cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)
### On control-plane (master) node:
To initialize the control-plane (master) node run:
```bash
kubeadm init <args>
```
| Argument | Description |
|:---|:---|
| `--control-plane-endpoint` | The shared endpoint (DNS name, Virtual IP, or Load Balancer address) that all control plane nodes and external clients will use to access the Kubernetes API server. Critical for High Availability (HA) clusters. |
| `--apiserver-advertise-address` | The IP address the API server will advertise to other nodes. Typically set to the private IP address of the machine where kubeadm is being run. |
| `--pod-network-cidr` | The CIDR range from which Pod IPs will be allocated. Must match your CNI plugin configuration (e.g., `10.244.0.0/16` for Flannel, `192.168.0.0/16` for Calico). |
| `--service-cidr` | The CIDR range for Kubernetes service cluster IPs. Default is `10.96.0.0/12` unless customized. |
| `--apiserver-cert-extra-sans` | Extra Subject Alternative Names (SANs) to be added to the API server certificate. Useful when the API server is accessed via additional IPs or DNS names (like a Load Balancer address). |
| `--upload-certs` | Uploads control plane certificates to a kubeadm-certs secret inside the cluster, enabling secure joining of additional control plane nodes without manual certificate copying. |

```bash
IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
Default
```bash
kubeadm init --apiserver-advertise-address=192.168.75.137 --pod-network-cidr=10.244.0.0/16
```
Or
```bash
ifconfig eth0
# Lookup for inet = 192.168.75.137
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.168.75.137 --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
if cluster dosen't work, reset cluster with kubeadm:
```bash
sudo kubeadm reset --force
```
To start using your cluster, you need to run the following as a regular user:
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Alternatively, if you are the root user, you can run:
```bash
export KUBECONFIG=/etc/kubernetes/admin.conf
```
```bash
kubeadm token create --print-join-command
```
### On worker node
```
kubeadm join 192.168.75.137:6443 --token sn2brb.c6fsi25tvscuw0ol \
        --discovery-token-ca-cert-hash sha256:6e4c42b70e8d743019f16541d960c417fe031bae8477a959649cb5459e5f6b84
```
---
# Install a Network Plugin `Flannel`
### On master node
```bash
curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
nano kube-flannel.yml
```
We are using a custom PodCIDR (`172.17.0.0/16`) instead of the default `10.244.0.0/16` when bootstrapping the Kubernetes cluster. However, the `Flannel` manifest by default is configured to use `10.244.0.0/16` as its network, which does not align with the specified PodCIDR. To resolve this, we need to update the `Network` field in the `kube-flannel-cfg` ConfigMap to match the custom PodCIDR defined during cluster initialization.
```yaml
net-conf.json: |
    {
      "Network": "10.244.0.0/16", # Update this to match the custom PodCIDR
      "Backend": {
        "Type": "vxlan"
      }
```
Locate the args section within the kube-flannel container definition. It should look like this: 
```yaml
  args:
  - --ip-masq
  - --kube-subnet-mgr
  - --iface=eth0
```
Add the additional argument `- --iface=eth0` to the existing list of arguments.
```bash
kubectl apply -f kube-flannel.yml
```
# Verify
```bash
kubectl get nodes
```
