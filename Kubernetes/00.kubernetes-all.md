# Install
```bash
# Install virtual box
sudo apt update
sudo apt install virtualbox
VBoxManage --version
```
# How to **Enable Nested VT-x/AMD-V** On Virtualbox
```sh
# Command Prompt (CMD) Run as administrator
cd C:\Program Files\Oracle\VirtualBox
# Command:
VBoxManage modifyvm "YourVirtualMachineName" --nested-hw-virt on

# Example:
VBoxManage modifyvm "master01" --nested-hw-virt on
```
# Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
```bash
kubectl version --client
```
```bash
source <(kubectl completion bash)
# set up autocomplete in bash into the current shell, bash-completion package should be installed first.
echo "source <(kubectl completion bash)" >> ~/.bashrc
# add autocomplete permanently to your bash shell.
```
```bash
nano ~/.bashrc
```
```vim
echo 'alias k=kubectl' >> ~/.bashrc
complete -o default -F __start_kubectl k
```
```bash
source ~/.bashrc
```
# Install [minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download)
```bash
minikube start
minikube start --driver=virtualbox
minikube status
minikube config get driver
```

---

# Install on windows
##### Download [`kubectl.exe`](https://kubernetes.io/releases/download/#binaries) 
- Create a folder in Drive `C:\kube`
- Move the the `kubectl.exe` to this folder
- Search for the `Edit the system environment variables`
- Environment Variables
- Click on Path > New > `C:\kube`
- Set this path for User variables and System variables
```bash
kubectl version --client
```
##### Download [`minikube.exe`](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download)
- Add the `minikube.exe` binary to your `PATH`.
- Make sure to run PowerShell as Administrator.
```bash
$oldPath = [Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine)
if ($oldPath.Split(';') -inotcontains 'C:\minikube'){
  [Environment]::SetEnvironmentVariable('Path', $('{0};C:\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine)
}
```
```bash
minikube start
minikube status
```

---

# [Deploy a Kubernetes Cluster using Kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
## "I ensure that all nodes are on the same network and can communicate with each other, assigning each one a unique and distinct hostname."
### These steps have to be performed on `all nodes`.
`set net.bridge.bridge-nf-call-iptables` to `1`: 
```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter
```
```bash
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sudo sysctl --system
```
---
# Check required ports
```bash
nc 127.0.0.1 6443 -v
```
---
# Disable Swap
- To disable swap, `sudo swapoff -a` can be used to disable swapping temporarily. To make this change persistent across reboots, make sure swap is disabled in config files like `/etc/fstab,` `systemd.swap,` depending how it was configured on your system.
---
# [Installing a container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

| Runtime       | Path to Unix domain socket                      |
|--------------|-----------------------------------------------|
| containerd   | unix:///var/run/containerd/containerd.sock   |
| CRI-O        | unix:///var/run/crio/crio.sock              |
| Docker Engine (using cri-dockerd) | unix:///var/run/cri-dockerd.sock |


```bash
sudo apt install containerd -y

sudo mkdir -p /etc/containerd
containerd config default
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml

sudo systemctl restart containerd
```
---
Install `kubeadm`, `kubectl` and `kubelet` on all nodes:
```bash
sudo apt-get update

sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
- Note:
In releases older than Debian 12 and Ubuntu 22.04, directory `/etc/apt/keyrings` does not exist by default, and it should be created before the curl command.
    ```bash
    mkdir /etc/apt/keyrings
    ```
```bash
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

# To see the new version labels
sudo apt-cache madison kubeadm

sudo apt-get install -y kubelet=1.32.0-1.1 kubeadm=1.32.0-1.1 kubectl=1.32.0-1.1

sudo apt-mark hold kubelet kubeadm kubectl
```
---
# Deploy Cluster
### On master node:
```bash
IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
Default
```bash
kubeadm init --apiserver-advertise-address=192.168.75.137 --pod-network-cidr=10.244.0.0/16
```
Or
```bash
ifconfig eth0
# Lookup for inet = 192.168.75.137
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.168.75.137 --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
To start using your cluster, you need to run the following as a regular user:
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
```bash
kubeadm token create --print-join-command
```
### On worker node
```
kubeadm join 192.168.75.137:6443 --token sn2brb.c6fsi25tvscuw0ol \
        --discovery-token-ca-cert-hash sha256:6e4c42b70e8d743019f16541d960c417fe031bae8477a959649cb5459e5f6b84
```
---
# Install a Network Plugin `Flannel`
### On master node
```bash
curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
nano kube-flannel.yml
```
We are using a custom PodCIDR (`172.17.0.0/16`) instead of the default `10.244.0.0/16` when bootstrapping the Kubernetes cluster. However, the `Flannel` manifest by default is configured to use `10.244.0.0/16` as its network, which does not align with the specified PodCIDR. To resolve this, we need to update the `Network` field in the `kube-flannel-cfg` ConfigMap to match the custom PodCIDR defined during cluster initialization.
```yaml
net-conf.json: |
    {
      "Network": "10.244.0.0/16", # Update this to match the custom PodCIDR
      "Backend": {
        "Type": "vxlan"
      }
```
Locate the args section within the kube-flannel container definition. It should look like this: 
```yaml
  args:
  - --ip-masq
  - --kube-subnet-mgr
  - --iface=eth0
```
Add the additional argument `- --iface=eth0` to the existing list of arguments.
```bash
kubectl apply -f kube-flannel.yml
```
# Verify
```bash
kubectl get nodes
```

---

# General Commands
```bash
kubectl get all -A
kubectl get all --all-namespaces
kubectl get all -n <namespace-name>
kubectl <command> <resource-type> --help
```
`resource-types`: [pod, node, namespace, deployment, rs(replicaSets), daemonset, service, role, rolebinding, pv, pvc, secret, configmap, ingress, job, cronjob, statefulset]
```bash
kubectl get pod --help
kubectl create pod --help
```
```bash
kubectl describe <resource-types> <resource-name> -n <namespace>
```

---

The `kubectl api-resources` command shows you a list of all the resources supported by Kubernetes.

### Example Output of `kubectl api-resources`:
```bash
NAME          SHORTNAMES   APIVERSION   NAMESPACED   KIND
pods          po           v1           true         Pod
services      svc          v1           true         Service
deployments   deploy       apps/v1      true         Deployment
configmaps    cm           v1           true         ConfigMap
secrets                    v1           true         Secret
namespaces    ns           v1           false        Namespace
nodes         no           v1           false        Node
```
- NAMESPACED = false => cluster-scoped

---

# Cluster
```bash
kubectl cluster-info
kubectl config view
kubectl config get-clusters # show all cluster
kubectl config use-context cluster1
# Switched to context "cluster1".
```

---

# Node
```bash
kubectl get nodes
kubectl get nodes -o wide
kubectl describe node <node-name>
kubectl get node <node-name> --show-labels
kubectl label node <node-name> key=value
kubectl describe node <node-name> | grep -i taints
```

---

# Pod
[kubectl run](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_run/)
```bash
kubectl get pods
watch kubectl get pods
kubectl get pods -o wide
kubectl get pods --all-namespaces
kubectl get pods --namespace kube-system
kubectl get pod <pod-name> -o yaml > my-new-pod.yaml
kubectl get pod <pod-name> -o yaml >&nbsp;my-new-pod.yaml

kubectl get pod --show-labels

kubectl run <pod-name> --image=<image_name>
kubectl run nginx --image=nginx --dry-run=client -o yaml
kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-definition.yaml
kubectl run nginx --image=nginx --port=8080
kubectl run redis -l tier=db --image=redis:alpine # -l means Label

kubectl describe pod <pod-name>
kubectl describe pod <pod-name> | grep -A5 State

kubectl create -f nginx-definition.yaml

kubectl edit pod <pod-name>

kubectl replace -f my-new-pod.yaml --force 
# This command will delete the existing one first and recreate a new one from the YAML file. 

kubectl delete pod <pod-name>
kubectl delete po <pod-name>

kubectl exec <pod-name> -c <container-name> -it -- bash
kubectl exec ubuntu-sleeper -- whoami
kubectl -n elastic-stack exec -it app -- cat /log/app.log
```
```bash
nano multi-container-pod.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - name: lemon
    image: busybox
    command:
      - sleep
      - "1000"
  - name: gold
    image: redis
```

---

| Container Restart Policy  |         |
|------------------|------------------|
| `Always`    | Automatically restarts the container after any termination. |
| `OnFailure` | Only restarts the container if it exits with an error (non-zero exit status). |
| `Never`     | Does not automatically restart the terminated container. |
- **For regular Pods**: The default value is `Always`.
- **For Jobs or CronJobs**: The default value is `OnFailure`.

---

# Static Pods
#### Pods that run directly on a node without being managed by the API server.
#### Static Pods directory `/etc/kubernetes/manifests/`
```bash
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
```

---

**kubectl [command] [TYPE] [NAME] -o **

Here are some of the commonly used formats:

1. `-o json` = Output a JSON formatted API object.
2. `-o name` = Print only the resource name and nothing else.
3. `-o wide` = Output in the plain-text format with any additional information.
4. `-o yaml` = Output a YAML formatted API object.

---

When writing a manifest in Kubernetes, you must include these 4 fields:

- **apiVersion**: The API version used by this object.
- **kind**: The type of object (Pod, Deployment, Service, etc.).
- **metadata**: Descriptive information such as name and labels.
- **spec**: The desired configuration and behavior for this object.

---

# [Limit Range](https://kubernetes.io/docs/concepts/policy/limit-range/)
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example-no-conflict-with-limitrange-cpu
spec:
  containers:
  - name: demo
    image: registry.k8s.io/pause:3.8
    resources:
      requests:
        cpu: 700m
      limits:
        cpu: 700m
```

---

# Types of Workloads

- **Deployment**: Running stateless applications.
- **ReplicaSet**: Maintaining a specified number of Pods.
- **StatefulSet**: Running stateful applications.
- **DaemonSet**: Running a Pod on all Nodes.
- **Job**: Running temporary tasks.
- **CronJob**: Running scheduled tasks.

---

# Deployments
```bash
kubectl get rs
kubectl get replicaset
kubectl get deploy
kubectl get deployment
kubectl describe deployment <deployment-name>
kubectl edit deployment <deployment-name>
kubectl explain deployment
```

---

```bash
nano nginx-deployment-definition.yaml
kubectl create -f nginx-deployment-definition.yaml
```
In the Deployment, the `spec` section will be for `Pods`.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx-frontend
  template:
    metadata:
      labels:
        name: nginx-frontend
    spec:
      containers:
      - name: nginx-container
        image: nginx
```
```bash
kubectl create deployment <deploy-name> --image=<image-name>
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
kubectl create deployment nginx --image=nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

kubectl scale deployment <deploy-name> --replicas=4
```

---

### **🔹 Deployment Strategies: Blue/Green & Canary**

#### **🚀 Blue/Green Deployment**
Blue/Green is a deployment strategy that reduces downtime and risk by running two identical environments:
- **Blue**: The current, active version.
- **Green**: The new version that is tested before switching traffic.

📌 **Steps:**
1. Deploy the **Green** version alongside the **Blue** version.
2. Test the **Green** version.
3. Switch traffic from **Blue** to **Green** (typically by updating a Load Balancer).
4. If issues arise, revert back to **Blue**.

✅ **Pros:**
- Minimal downtime.
- Easy rollback.
- Ensures new version is fully tested before release.

❌ **Cons:**
- Requires double the resources.
- More complex infrastructure management.

#### **🛠 Canary Deployment**
Canary deployment is a progressive rollout strategy where the new version is deployed to a small subset of users before a full release.

📌 **Steps:**
1. Deploy the new version to a small percentage (e.g., 5%) of traffic/users.
2. Monitor performance and logs.
3. Gradually increase traffic if no issues are detected.
4. Fully roll out the new version if everything works well.

✅ **Pros:**
- Reduces risk of full failure.
- Allows real-world testing with minimal impact.
- Can be automated with monitoring tools.

❌ **Cons:**
- More complex traffic routing.
- Rollbacks require additional handling.

---

### **🔹 Conclusion**  
If you need to **filter or group resources**, use **Labels**.  
If you need to **store additional descriptive metadata**, use **Annotations**. 🚀  
For **deployment strategies**, use **Blue/Green** for instant switches and **Canary** for gradual rollouts.

---

# Service
```bash
kubectl get svc
kubectl get service
kubectl describe service
```
### Types of **Service** in Kubernetes

| Service Type    | Description | Common Usage |
|----------------|--------------------------------------------------|------------------------------|
| **ClusterIP**  | The default type, assigns an internal IP within the cluster, accessible only inside the cluster. | Internal communication between services within the cluster |
| **NodePort**   | Opens a port on each Node, allowing access from outside the cluster. | External access without a Load Balancer |
| **LoadBalancer** | Creates a public IP and a Load Balancer from the cloud provider. | Load distribution and direct external access |
| **ExternalName** | Maps requests to an external domain name (e.g., `example.com`). | Redirecting requests to external services |

---

### Differences between **nodePort, port, and targetPort** in Kubernetes

| Parameter      | Description |
|---------------|--------------------------------------------------|
| **nodePort**  | The port opened on each Node, allowing external access (ranges between 30000-32767). |
| **port**      | The port defined in the Service that receives traffic. |
| **targetPort** | The port running inside the Pod that processes incoming requests. |

Example YAML configuration:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: example-service
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30080
```

In this example:
- Requests arrive at **port 80** on the Service.
- They are forwarded to **port 8080** inside the corresponding Pod.
- The Service is externally accessible on **port 30080** on each Node.

---

```bash
nano service-definition.yaml
kubectl apply -f service-definition.yaml
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort
```
```bash
kubectl run nginx --image=nginx --port=80 --expose # create a service for this pod

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

kubectl run redis -l tier=db --image=redis:alpine # -l means Label
kubectl expose pod redis --port=6379 --target-port=6379 --name=redis-service --type=ClusterIP
```

```
# if messaging is deployment
kubectl expose deployment messaging --name=messaging-service --port=6379 --target-port=6379 --type=ClusterIP
# if messaging is pod
kubectl expose pod messaging --name=messaging-service --port=6379 --target-port=6379 --type=ClusterIP
```

---

---

### Communication Between Pods in Different Namespaces
- By default, Pods in different namespaces cannot directly discover each other. To establish communication, you must use the **full DNS name** of the service.

---

### Exposing Applications

There are several ways to expose applications in Kubernetes:

1. **NodePort**:
   - Exposes the service on a static port on each node’s IP.
   - Accessible from outside the cluster using `<NodeIP>:<NodePort>`.

2. **LoadBalancer**:
   - Provisions an external load balancer (e.g., in cloud environments) to expose the service.
   - Automatically assigns an external IP.

3. **Ingress**:
   - Provides HTTP/HTTPS routing to services based on hostnames or paths.
   - Requires an Ingress controller to be installed in the cluster.

---

### How to Check if a Service is Pod-to-Pod
To determine if a service is Pod-to-Pod, you can check the following:

1. **Service Type**:
   - The service type should be `ClusterIP`.
   - Command:  
     ```bash
     kubectl get service <service-name> -n <namespace>
     ```

2. **Service Selector**:
   - The service selector should target internal Pods.

3. **Service Endpoints**:
   - The service endpoints should include the internal IPs of the Pods.
   - Command:  
     ```bash
     kubectl get endpoints <service-name> -n <namespace>
     ```

4. **DNS Testing from Inside a Pod**:
   - Access one of the Pods:  
     ```bash
     kubectl exec -it <pod-name> -n <namespace> -- /bin/bash
     ```
   - Send a request to the service:  
     ```bash
     curl http://<service-name>.<namespace>.svc.cluster.local
     ```
   - If the service is Pod-to-Pod, you should receive a response.

5. **Pod Logs**:
   - Check the logs of the Pods to verify internal communication:  
     ```bash
     kubectl logs <pod-name> -n <namespace>
     ```

---

### Multi-Port Service
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-port-deployment
  labels:
    app: multi-port-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: multi-port-app
  template:
    metadata:
      labels:
        app: multi-port-app
    spec:
      containers:
      - name: multi-port-container
        image: nginx:stable
        ports:
        - containerPort: 80   # HTTP
        - containerPort: 443  # HTTPS
        - containerPort: 8080 # Custom Port
---
apiVersion: v1
kind: Service
metadata:
  name: multi-port-service
spec:
  selector:
    app: multi-port-app
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443
  - name: custom-port
    protocol: TCP
    port: 8080
    targetPort: 8080
```

---

## LoadBalancer
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-loadbalancer
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80 # The input port of the service (LoadBalancer).
    targetPort: 80 # The port inside the Nginx container.
```
```bash
kubectl get services
```
In the browser or similar tools, enter the following address: `http://<EXTERNAL-IP>`

---

## ExternalName in Kubernetes
**ExternalName** is a type of Service in Kubernetes that, instead of connecting to Pods within the cluster, is used to redirect requests to an external service (outside the cluster). In simple terms, when clients access the service, Kubernetes redirects their requests to another domain name (DNS) that exists outside the cluster.

---

## When is ExternalName Useful?
- When an application inside the cluster needs to connect to a database or services outside the cluster, such as:
  - Connecting to a MySQL database running on another server.
  - Using an external API like Google Maps or any other API.
- When you want to simplify access to external services within the cluster.

### Example YAML for External Service:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-external-service
spec:
  type: ExternalName
  externalName: mysql.external.com
```
### Example YAML for Internal Service:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: ExternalName
  externalName: my-service1.prod.svc.cluster.local
```
### Where is ExternalName Good and Where is it Not?
- Good For:
  - When you have a fixed external service with a DNS address.
  - When you want to simplify the naming of external services.

- Limitations:
  - ExternalName only works with services that have a DNS address. If the external service only has an IP, you cannot use ExternalName.
  - ExternalName does not perform load balancing. If the external service has multiple servers, Kubernetes will only connect to the specified DNS address.

#### Summary
ExternalName is a special type of Service in Kubernetes used to redirect requests to an external service (outside the cluster).
This type of service uses DNS to redirect requests.
It is mainly used for connecting to external services or simplifying access to external services within the cluster.

---

## External IPs in Kubernetes
When you have a service in Kubernetes that you want to access from outside the cluster (i.e., from an external network), you can use **External IPs**. In simple terms, External IP allows you to assign a specific IP to your service, and requests to that IP will be routed to the service.

## When is it Useful?
- When you have an external server or device on the network that needs to connect directly to a service inside Kubernetes.
- When you want to restrict access to your service to specific predefined IPs.
- When you don’t need a LoadBalancer or don’t want to use one.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  externalIPs:
  - 192.168.1.100
```
### How to Access the Service?
* If you are on a network that has access to the IP 192.168.1.100, you can send a request using a browser or curl:`http://192.168.1.100`
* If everything is set up correctly, the default Nginx page will be displayed.

### Where is External IP Good and Where is it Not?
Good For:
- When you have a specific IP that needs to connect directly to a Kubernetes service.
- When you have a pre-defined network infrastructure with specific IPs.

Limitations:
- External IP is usually not suitable for cloud environments. In the cloud, it’s better to use LoadBalancer or Ingress.
- Managing External IPs can be challenging in complex environments because Kubernetes does not have full control over those IPs.

#### Summary
- External IP allows you to expose a service inside Kubernetes through a specific IP from outside the cluster.
- This method is mainly used for internal networks or specific infrastructures.
- For cloud or public environments, it’s better to use LoadBalancer or Ingress.

---

# Namespace
#### Provides a way to logically isolate Kubernetes resources within a cluster.
```bash
kubectl create namespace devops
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: devops
spec:
  containers:
  - name: nginx-container
    image: nginx
```
---
# Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
kubectl config view --minify | grep namespace
kubectl config set-context --current --namespace=default
```

---

# Kubernetes DNS

Kubernetes DNS helps services and Pods communicate with each other without needing to use IP addresses.

## How does DNS work in Kubernetes?

When you create a Service in Kubernetes, a DNS record is automatically created for it. This record allows other services to easily access it using the service name. For example:

Assume you have a service named `my-service` in a namespace called `my-namespace`.  
The DNS record for this service will look like this:  
`my-service.my-namespace.svc.cluster.local`  

- `my-service`: The name of the service.  
- `my-namespace`: The namespace of the service.  
- `svc`: Indicates that this is a Service.  
- `cluster.local`: The default domain for your cluster.  

## DNS Structure for Service Communication

1. **Within the same namespace**: If services are in the same namespace, you can use just the service name:  
   `my-service`  

2. **Across different namespaces**: If services are in different namespaces, you must use the full format:  
   `my-service.my-namespace.svc.cluster.local`  

## CoreDNS

DNS in Kubernetes is managed by an add-on called **CoreDNS**. This service is installed by default in most Kubernetes distributions and is responsible for maintaining DNS records for services and Pods.

---

## Examples of DNS in Action

1. **Communication between two services in the same namespace**:  
   Assume you have two services, `frontend` and `backend`, both in the `default` namespace. In this case, `frontend` can connect to `backend` using the address:  
   `backend`  

2. **Communication between two services in different namespaces**:  
   If `frontend` is in the `frontend-namespace` namespace and `backend` is in the `backend-namespace` namespace, you must use the full address:  
   `backend.backend-namespace.svc.cluster.local`  

---

## Practical Tips
- Namespaces are the most important objects in Kubernetes.
- Namespaces directly impact **Network Policies**, **Resource Quotas**, and **RBAC** (access permissions). Therefore, using namespaces is essential for better cluster management.  
- For services that always need a fixed address, Kubernetes DNS solves this problem, so you no longer need to worry about changing IPs.
- Each namespace (ns) is an isolated environment, but the **kubelet** has access to them, which means the kubelet operates independently of these namespaces. While **storage** is not scoped within namespaces, almost everything else falls under the scope of namespaces.

---

# Labels and Selectors
#### Used to categorize and filter Kubernetes resources.
#### **Labels are in the form of `key: value`.**
---
#### If labels and selectors are not the same, the resources will not be connected correctly!
- `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
- `selector` in `Service` must match `labels` in `Pods` to send them traffic.
- `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
- `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.

| **Use Case**                  | **Must Match**                                      |
|--------------------------------|----------------------------------------------------|
| Deployment → ReplicaSet → Pod  | `selector.matchLabels` in Deployment and `labels` in Pod |
| Service → Pod                 | `selector` in Service and `labels` in Pod         |
| NetworkPolicy → Pod           | `podSelector.matchLabels` in NetworkPolicy and `labels` in Pod |
| Ingress → Service             | `backend.service.name` in Ingress and `metadata.name` in Service |

```bash
kubectl get pod -l env=dev
kubectl get pod -l env=dev,bu=finance,tier=frontend
```

```bash
kubectl get pods --selector env=dev
kubectl get all --selector env=prod --no-headers | wc -l
kubectl get all --selector env=prod,bu=finance,tier=frontend
```
Define the labels in the `metadata` section.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app
  labels:
    app: frontend
    environment: production
spec:
  containers:
  - name: nginx
    image: nginx
```
**`selector` in Service and `labels` in Pod Must Match**
```bash
kubectl -n <namespace-name> describe svc <service-name> | grep -i selector
kubectl -n <namespace-name> describe pod <pod-name> | grep -i label
```
Adding Labels to Existing Resources.
```bash
kubectl label pod my-app version=v1
```
Delete Label
```bash
kubectl label pod my-app version-
```

---

## **Annotations**

### **🔹 What are Annotations?**  
Annotations in Kubernetes allow you to attach arbitrary non-identifying metadata to objects. Unlike labels, annotations are not used for selection or filtering but serve for informational purposes such as automation, monitoring, and metadata storage.

---

### **Differences Between Labels and Annotations**  
| Feature           | Labels | Annotations |
|------------------|--------|------------|
| **Purpose**      | Identification & grouping of resources | Adding descriptive metadata |
| **Used in Selectors** | ✅ Yes | ❌ No |
| **Key/Value Length Limit** | Limited | Can be long |
| **Common Use Cases** | Filtering, grouping, enforcing policies | Storing descriptions, logs, hashes, or automation data |

---

### **Example of Annotations in a Pod**  
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  annotations:
    kubernetes.io/description: "This pod runs the main backend service."
    monitoring.example.com/logs: "enabled"
spec:
  containers:
  - name: nginx
    image: nginx
```
**Explanation:**  
- `kubernetes.io/description`: Provides an explanation of the pod's function.
- `monitoring.example.com/logs`: Enables logging for monitoring purposes.

---

### **Use Cases of Annotations**  
**Monitoring & Logging:** Used for **Prometheus**, **Fluentd**, and other monitoring tools.  
**CI/CD Management:** Stores information like **commit SHA**, **Docker image version**, etc.  
**Automation:** Allows adding extra metadata for resource control.  
**Storing Non-Filterable Metadata:** Holds data that is not used in **selectors**.

---

### **Conclusion**  
If you need to **filter or group resources**, use **Labels**.  
If you need to **store additional descriptive metadata**, use **Annotations**.

---

# Taints and Tolerations
```bash
kubectl describe node node01 | grep -i taints
kubectl taint nodes node01 spray=mortein:NoSchedule
```
### with this yaml file this pod assign to node01
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
```
### node/controlplane untainted
```bash
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
```

---

# Node Affinity
#### Specifies rules to influence where Pods are scheduled.
```bash
kubectl label node <node-name> key=value
kubectl label node node01 color=blue
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
```


---

# Difference Between `label` and `taint` in Kubernetes

| Feature   | **Label** | **Taint** |
|-----------|----------|----------|
| **Purpose** | Used for **grouping and filtering** resources like `Pods`, `Nodes`, `PVCs`, `Services`, etc. | Used to **restrict scheduling** of pods on specific nodes. |
| **Applies To** | Can be applied to **Pods, Nodes, PVCs, Services, etc.** | Only applies to **Nodes**. |
| **Main Use Case** | - Organizing resources <br> - Filtering with **Selectors** <br> - Used in **Node Affinity** | - Preventing **undesired pods** from running on specific nodes <br> - Enforcing **special scheduling conditions** |
| **How Pods are Scheduled on Nodes?** | Pods are scheduled on nodes with specific labels using `nodeSelector` or `nodeAffinity`. | Only pods with matching `tolerations` can be scheduled on tainted nodes. |
| **Command to Apply** | ```sh kubectl label node node01 env=production ``` | ```sh kubectl taint node node01 key=value:Effect ``` |

**Label** is used to **identify and categorize nodes and pods**.  
**Taint** is used to **restrict pod scheduling on nodes**.

---

# DaemonSets
```bash
kubectl get ds
kubectl get daemonsets
kubectl get daemonsets -A
kubectl get daemonsets --all-namespaces
kubectl get ds -n kube-system
kubectl describe daemonset <daemonset-name> -n <namespace-name>
kubectl describe daemonset kube-proxy -n kube-system
```
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: registry.k8s.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
```

---

# Static Pods
```bash
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
```

---

# Kubelet
```bash
kubelet --version

ps -ef |  grep /usr/bin/kubelet
cat /var/lib/kubelet/config.yaml
grep -i staticpod /var/lib/kubelet/config.yaml # To find static.yaml file path
ps -aux | grep kubelet | grep --color container-runtime-endpoint
```

---

# Manual Scheduling
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: controlplane #Manual Scheduling
  containers:
  -  image: nginx
     name: nginx
```

---

# Multiple Schedulers
```bash
kubectl get pods --namespace=kube-system
kubectl describe pod kube-scheduler-controlplane --namespace=kube-system
```
```bash
nano my-scheduler.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.32.0  # changed
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
```
```bash
nano my-scheduler-config.yaml
```
```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false
```
```bash
nano my-scheduler-configmap.yaml
```
```yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
```
```bash
nano nginx-pod.yaml
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx
```
---

# Secret
#### Stores sensitive data such as passwords and API keys.
Note: 
1. Secrets are not Encrypted. Only encoded.
2. Secrets are not Encrypted in ETCD.
3. Anyone able to create pods/deployments in the same namespace can access the secrets..
Imperative
```bash
kubectl get secrets
kubectl describe secrets <secret-name>

kubectl create secret generic db-secret <secret-name> --from-literal=<key>=<value>
kubectl create secret generic db-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd

kubectl create secret generic db-secret <secret-name> --from-file=<path-to-file>
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
```
```bash
kubectl -n webhook-demo create secret tls webhook-server-tls \
    --cert "/root/keys/webhook-server-tls.crt" \
    --key "/root/keys/webhook-server-tls.key"
```
---
Declarative
```bash
nano secret-data.yaml
```
```bash
echo -n mysql | base64
# Output is bXlzcWw=
echo -n root | base64
# Output is cm9vdA==
echo -n paswrd | base64
# Output is cGFzd3Jk
```
```yaml
apiVersion: v1 
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk
```
```bash
kubectl create -f secret-data.yaml
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - image: simple-webapp-color
    name: simple-webapp-color
    envFrom:
    - secretRef:
        name: app-secret
```
# Secrets in Pods
ENV
```yaml
envFrom:
  - secretRef:
      name: app-secret
```
SINGLE ENV
```yaml
env:
  - name: DB_Password
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password        
```
Volume
```yaml
volumes:
- name: app-secret-volume
  secret:
    secretName: app-secret
```

---

# [Liveness, Readiness, and Startup Probes in Kubernetes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)

In **Kubernetes**, three types of **probes** are used to check the health of containers:
**1. What is a Container Probe?**
A Container Probe is essentially a health check mechanism used by Kubernetes to check the status of a container in three states:
* **Is the container healthy?** (Liveness)
* **Is the container ready to serve traffic?** (Readiness)
* **Is the container ready to start?** (Startup)
Each Probe performs a test, and based on the test result, Kubernetes decides what to do with the container. For example, it might restart the container, stop sending requests to it, or even terminate it.
**2. Type of Probe?**
## **a. Liveness Probe**
- Checks if the **container is still alive**.
- If the probe **fails**, Kubelet **restarts** the container.
- Useful for detecting **deadlocks**.
- **Example:**
  ```yaml
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 3
    periodSeconds: 5
  ```

## **b. Readiness Probe**
- Checks if the **container is ready** to receive traffic.
- If the probe **fails**, the Pod is **removed from the service endpoints**.
- Useful for **delayed service initialization**.
- When the application crashes, the container is restarted. During this period the service directs users to the available POD, since the POD status is not READY.
- **Example:**
  ```yaml
  readinessProbe:
    tcpSocket:
      port: 8080
    initialDelaySeconds: 3
    periodSeconds: 5
  ```

## **c. Startup Probe**
- Ensures that the **application has started successfully**.
- If the probe **fails**, Kubelet **restarts** the container.
- Useful for **applications that take a long time to start**.
- **Example:**
  ```yaml
  startupProbe:
    exec:
      command: [ "cat", "/tmp/healthy" ]
    initialDelaySeconds: 30
    periodSeconds: 10
  ```

**3. Methods of Executing Probes**
Kubernetes provides three different methods for executing Probes. These methods determine how Kubernetes checks the container's status, which we call Handlers:

**a. HTTP Get Action**
* Kubernetes sends an HTTP request to a specific endpoint (inside the container).
* If the HTTP response code is 200-399, the Probe is successful.
* If an error code (4xx or 5xx) is returned or the request times out, the Probe fails.
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
```
- Kubernetes sends an HTTP request to `/health` on port 8080 every 10 seconds.
- If no response is received within 5 seconds after the container starts, the Probe fails.

**b. TCP Socket**
* Kubernetes establishes a TCP connection to a specific port.
* If the connection is successful, the Probe is successful.
* If the connection fails or times out, the Probe fails.
```yaml
readinessProbe:
  tcpSocket:
    port: 3306
  initialDelaySeconds: 5
  periodSeconds: 10
```
- Here, Kubernetes establishes a TCP connection to port 3306 every 10 seconds. If the connection fails, the Probe fails.

**c. Command Probe (Exec Action)**
* Kubernetes executes a command inside the container.
* If the command executes with exit code 0, the Probe is successful.
* If the command executes with an error code, the Probe fails.
```yaml
livenessProbe:
  exec:
    command:
      - cat
      - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 10
```
- Here, Kubernetes executes the command cat /tmp/healthy every 10 seconds.
- If the file /tmp/healthy exists, the Probe is successful. Otherwise, the Probe fails.

**4. Important Parameters in Probes**
You can use these parameters to configure each Probe:
| Parameter             | Description   |
|-----------------------|---------------|
|initialDelaySeconds    | The time Kubernetes should wait before executing the first probe (e.g., during application startup). |
|periodSeconds          | The interval between each check. |
|timeoutSeconds         | The time Kubernetes waits to receive a response. |
|failureThreshold       | The number of consecutive failures that must occur for the container to be considered unhealthy. |
|successThreshold       | The number of consecutive successes that must occur for the container to be considered healthy or ready (usually set to 1). |


---

## **Key Differences**

| **Probe Type**  | **Purpose** | **Failure Result** | **Best For** |
|------------|------|--------------------------------|------------|
| **Liveness** | Checks if the container is alive | Container **restarts** | Apps that may freeze or deadlock |
| **Readiness** | Checks if the container is ready | Pod is **removed from service** | Apps that take time to initialize |
| **Startup** | Ensures successful startup | Container **restarts** | Apps that require a long startup time |

**Liveness** ensures the **container is running**,  
**Readiness** ensures the **service is available**,  
**Startup** ensures the **application starts correctly**.

---

Ex:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-1
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80
```
---

# Logging & Monitoring
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl top node
kubectl top node --sort-by='memory' --no-headers | head -1
kubectl top pod
kubectl top pod --sort-by='memory' --no-headers | head -1
kubectl top pod <pod-name>

kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>

kubectl logs <pod-name> -c <container-name> | grep WARNING > /opt/test.txt
```

---

# Rolling Updates and Rollbacks
```bash
kubectl create -f nginx-deployment.yaml --record
# We can use the – -record flag to save the command used to create/update a deployment against the revision number.
nano nginx-deployment.yaml # for example change image
kubectl apply -f nginx-deployment.yaml
# or
kubectl set image <resource-type> <resource-name> <container-name>=<new-image>
kubectl set image deployment nginx-deployment nginx-container=nginx:1.25
# info
kubectl rollout status deployment <deployment-name> -n <namespace>
kubectl rollout status deployment nginx-deployment
kubectl rollout history deployment nginx-deployment
kubectl rollout history deployment nginx-deployment --revision=3
# undo
kubectl rollout undo deployment nginx-deployment
kubectl rollout undo deployment nginx-deployment --to-revision=1
# To rollback to specific revision we will use the --to-revision flag.
```
### Important Notes About Rollback

1. **Saving Version History**: Kubernetes, by default, only saves the history of the latest versions. If you want to store more versions, you need to set the `revisionHistoryLimit` in the Deployment. Example:
```yaml
spec:
  revisionHistoryLimit: 5
```
2. **CHANGE-CAUSE**: To record the reason for each change in the history, you can use `annotations`. Example:
```bash
kubectl annotate deployment <deployment-name> kubernetes.io/change-cause="Updated image to nginx:1.22"
```
You can also include CHANGE-CAUSE directly in the manifest. Example:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: devops
  annotations:
    kubernetes.io/change-cause: "Updated image to nginx:1.22 for security fixes"
```
### RollingUpdate  
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```
### Recreate
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```
**A Complete Example of Pause and Unpause**
Let's assume you have a Deployment named `nginx-deployment`. Now you want to change the Nginx version, but first, you want to pause it to make sure everything is correct.

**1. Pausing the Deployment**
First, you pause the Deployment so that Kubernetes does not apply the changes yet:
```bash
kubectl rollout pause deployment nginx-deployment
# deployment.apps/nginx-deployment paused
```
**2. Applying Changes**
Now, for example, you want to change the Nginx version from 1.22 to 1.23:
```Bash
kubectl set image deployment/nginx-deployment nginx=nginx:1.23
# deployment.apps/nginx-deployment image updated
```
But since the Deployment is paused, Kubernetes saves this change but does not execute it yet.

**3. Checking the Status**
You want to make sure that the changes are done correctly and the Deployment is paused. Check the Deployment status:
```Bash
kubectl rollout status deployment nginx-deployment
# deployment "nginx-deployment" is paused
```
Here, Kubernetes tells you that your Deployment is in the paused state.

**4. Unpausing the Deployment**
When you are sure everything is okay, you unpause the Deployment:
```Bash
kubectl rollout resume deployment nginx-deployment
# deployment.apps/nginx-deployment resumed
```
Now Kubernetes will proceed with the rollout and apply the changes you made.

**5. Checking the Rollout Status**
If you want to see the rollout status, run this command:
```Bash
kubectl rollout status deployment nginx-deployment
# Waiting for deployment "nginx-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
# deployment "nginx-deployment" successfully rolled out
```
**Summary**
- `Pause`: Means telling Kubernetes, "Hold on for now, don't apply the changes."
- `Unpause`: Means telling Kubernetes, "Now apply the changes."
This feature allows you to make changes with more control and step-by-step.

---

# Commands and Arguments
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "green"]
```
# env
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color
```
---

# Configmaps
#### Stores key-value configuration data for Pods.
```bash
kubectl get cm
kubectl get configmaps
kubectl describe configmaps
kubectl describe cm

kubectl create configmap <config-name> --from-literal=<key>=<value>
kubectl create configmap  webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

kubectl create configmap <config-name> --from-file=<path-to-file>

```
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color
```

---

# ConfigMap in Pods
```bash
nano config-map.yaml
```
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod
```
```bash
kubectl create -f config-map.yaml
```
ENV
```yaml
envFrom:
  - configMapRef:
      name: app-config 
```
SINGLE ENV
```yaml
env:
  - name: APP_COLOR
    valueFrom:
      configMapRefKey:
        name: app-config
        key: APP_COLOR
```
Volume
```yaml
volumes:
- name: app-config-volume
  configMap:
    name: app-config
```

---

# [initContainer](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)
- initContainer is for initial setup, while sidecar is used for ongoing tasks such as logging, proxying, and monitoring.
- initContainer runs before the main container starts.
- It is useful for checking prerequisites, downloading data, configuring settings, and waiting for other services to be ready.
- If an initContainer fails, the Pod will not start.
- Use a shared Volume to exchange data between the initContainer and the main container.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app-container
    image: myapp:latest
  initContainers:
  - name: check-db
    image: busybox
    command: ['sh', '-c', 'until nc -z db-service 3306; do echo waiting for database; sleep 2; done;']
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: \['sh', '-c', 'echo The app is running! && sleep 3600'\]
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;'\]
  - name: init-mydb
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;'\]
```

---

# Horizontal Scaling & Horizontal Pod Autoscaler (HPA)
```bash
kubectl join ...
kubectl scale ...
kubectl edit ...
kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80
```
Or user the yaml file:
```bash
nano autoscale.yml
```
```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 0
```
```bash
kubectl apply -f autoscale.yml
```
```bash
kubectl get hpa
kubectl describe hpa <deployment-name>
kubectl describe hpa nginx-deployment
kubectl events hpa nginx-deployment | grep -i "ScalingReplicaSet"
kubectl events hpa nginx-deployment | grep -i "FailedGetResourceMetric"
```
Another Ex:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kkapp-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
```
## Explanation of HPA File

- **scaleTargetRef**: This section specifies which Deployment the HPA will target. In this example, `kkapp-deploy` is the target of the HPA.
- **minReplicas and maxReplicas**:
  - `minReplicas`: The minimum number of Pods that should always be running (here, 2).
  - `maxReplicas`: The maximum number of Pods that Kubernetes can create (here, 10).
- **metrics**: Here, we define the criteria based on which the HPA should perform scaling.
  - `type: Resource`: This means that resources like CPU or Memory will be used for decision-making.
  - `averageUtilization`: The goal is that if the average CPU usage exceeds 50%, Kubernetes will increase the number of Pods.
**The `behavior` section in the Horizontal Pod Autoscaler (HPA) file** allows you to control scaling behavior (increasing or decreasing the number of Pods) more precisely. This section includes settings that specify how and how fast to scale up (increase) and scale down (decrease). In this example, only the `scaleDown` section is configured.
**Description of the `behavior` section:**
**`scaleDown`:**
* This section relates to settings for decreasing the number of Pods (scale down).
**`stabilizationWindowSeconds`:**
* This value (here, 300 seconds or 5 minutes) creates a "stabilization window." This means that the HPA waits 5 minutes before deciding to reduce Pods to ensure that the resource reduction is real and not due to short-term fluctuations.
**`policies`:**
* This section contains policies that specify how Pods are reduced.
    * `type: Percent`: This policy operates based on a percentage of the current Pods.
    * `value: 10`: At each step, a maximum of 10% of the current Pods are reduced.
    * `periodSeconds: 60`: This reduction occurs every 60 seconds.
**Summary:**
* **`stabilizationWindowSeconds`:** The HPA waits 5 minutes before reducing Pods to ensure the stability of the resource reduction.
* **`policies`:** The HPA reduces a maximum of 10% of the current Pods every 60 seconds at each step.
These settings help you prevent sudden and excessive Pod reduction and ensure that resource reduction occurs slowly and steadily.

**Important Points:**
* **Setting Resources in Deployment:** Always specify `resources.requests` and `resources.limits` in your Deployment, as HPA makes decisions based on these values.
* **HPA Only for CPU and Memory:** By default, HPA works with CPU and Memory consumption. If you want to use other metrics (such as request count or latency), you need to use Custom Metrics.

**Summary**
* With HPA, you can automatically increase or decrease the number of Deployment Pods based on specified metrics, such as CPU consumption.
* Set the minimum and maximum number of Pods using `minReplicas` and `maxReplicas`.
* Kubernetes always tries to maintain the status based on the target you set (such as 50% CPU consumption).
* **Scaling Time:** HPA does not work instantly. It usually takes a few minutes to apply changes because it needs to monitor the status.

---

# Vertical Pod Autoscaling (VPA)
```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
# To check the installed VPA CRDs
kubectl get crds | grep verticalpodautoscaler

kubectl get deployments -n kube-system | grep vpa 
# vpa-admission-controller, vpa-recommender, vpa-updater

kubectl get vpa
```
```yaml
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: flask-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: flask-app-4
  updatePolicy:
    updateMode: "Off"  # You can set this to "Auto" if you want automatic updates
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
        maxAllowed:
          cpu: 1000m
        controlledResources: ["cpu"]
```
Another Ex:
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: "*"
        minAllowed:
          cpu: "100m"
          memory: "100Mi"
        maxAllowed:
          cpu: "2"
          memory: "4Gi"
```
---

# OS Upgrades

```bash
kubectl get nodes
kubectl get deployments
kubectl get pods -o wide
```
### Before maintenance or removing a Node.
- Evacuates all regular Pods (except DaemonSets).
- Marks the Node as Unschedulable.
```bash
kubectl drain <node-name> --ignore-daemonsets
```
### To temporarily stop scheduling new Pods on a Node.
- Existing Pods remain.
- Node becomes Unschedulable, but no Pods are evicted.
```bash
kubectl cordon <node-name> 
```
### To re-enable scheduling on a Node after an issue is resolved.
- Node becomes Schedulable again.
- New Pods can be scheduled on it.
```bash
kubectl uncordon <node-name>
```

---

# Cluster upgrade
```bash
kubectl get nodes
# node names: controlplane, node01
kubectl describe nodes  controlplane | grep -i taint
# Taints:             <none>
kubectl describe nodes  node01 | grep -i taint
# Taints:             <none>
# This means that both nodes have the ability to schedule workloads on them.
kubeadm upgrade plan
kubectl drain controlplane --ignore-daemonsets
kubectl get nodes
# NAME           STATUS                     ROLES           AGE   VERSION
# controlplane   Ready,SchedulingDisabled   control-plane   23m   v1.31.0
# node01         Ready                      <none>          22m   v1.31.0
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
kubeadm upgrade plan v1.32.0
kubeadm upgrade apply v1.32.0

apt-get install kubelet=1.32.0-1.1

systemctl daemon-reload
systemctl restart kubelet

kubectl uncordon controlplane

# ---

kubectl drain node01 --ignore-daemonsets

ssh node01
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
# Upgrade the node 
kubeadm upgrade node
apt-get install kubelet=1.32.0-1.1
systemctl daemon-reload
systemctl restart kubelet
# Type `exit` or `logout` or enter `CTRL + d` to go back to the controlplane node.
kubectl uncordon node01
```

---

# Backup and Restore
## Types:
- Resource Configuration
  ```bash
  kubectl get all --all-namespaces - yaml > all-deploy-service.yaml
  ```
- ETCD Cluster
- Persistent Volumes
## etcd
```bash
kubectl -n kube-system logs etcd-controlplane | grep -i 'etcd-version'
kubectl -n kube-system describe pod etcd-controlplane | grep Image:
kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'
```
### Take a snapshot
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db
```
### Restore the snapshot
```bash
ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db
```
##### Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the `--data-dir`.


##### Next, update the `/etc/kubernetes/manifests/etcd.yaml`:

##### We have now restored the etcd snapshot to a new path on the controlplane - `/var/lib/etcd-from-backup`, so, the only change to be made in the YAML file, is to change the hostPath for the volume called `etcd-data` from old directory (`/var/lib/etcd`) to the new directory (`/var/lib/etcd-from-backup`).
```
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
```
##### With this change, `/var/lib/etcd` on the container points to `/var/lib/etcd-from-backup` on the controlplane (which is what we want).

##### When this file is updated, the `ETCD` pod is automatically re-created as this is a static pod placed under the `/etc/kubernetes/manifests` directory.

- ##### Note 1: As the ETCD pod has changed it will automatically restart, and also `kube-controller-manager` and `kube-scheduler`. Wait 1-2 to mins for this pods to restart. You can run the command: `watch "crictl ps | grep etcd"` to see when the ETCD pod is restarted.

- ##### Note 2: If the etcd pod is not getting `Ready 1/1`, then restart it by `kubectl delete pod -n kube-system etcd-controlplane` and wait 1 minute.

- ##### Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.


##### If you do change `--data-dir` to `/var/lib/etcd-from-backup` in the ETCD YAML file, make sure that the `volumeMounts` for `etcd-data` is updated as well, with the `mountPath` pointing to `/var/lib/etcd-from-backup` (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

---

# ETCD - Commands
```bash
etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set

etcdctl snapshot save
etcdctl endpoint health
etcdctl get
etcdctl put

kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / \
  --prefix --keys-only --limit=10 / \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --cert /etc/kubernetes/pki/etcd/server.crt \
  --key /etc/kubernetes/pki/etcd/server.key"

kubectl -n kube-system describe pod etcd-controlplane | grep data-dir
ps -ef | grep --color=auto etcd
```

---

# Backup 
```bash
kubectl describe pod -n kube-system etcd-controlplane
kubectl describe pod -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls
#  --advertise-client-urls   =>   --endpoints
kubectl describe pod -n kube-system etcd-controlplane  | grep pki
#  --trusted-ca-file         =>   --cacert
#  --cert-file               =>   --cert
#  --key-file                =>   --key

ETCDCTL_API=3 etcdctl \
  --endpoints=https://192.168.139.36:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/cluster1.db
```

---

# Check the members of the cluster:
```bash
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
   member list
```

---

# TLS Security
```bash
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text
ls -l /etc/kubernetes/pki/etcd/server* | grep .crt

crictl ps -a | grep kube-apiserver # like docker ps -a but in container.d
crictl logs --tail=2 <container-id>

nano /etc/kubernetes/manifests/kube-apiserver.yaml
nano /etc/kubernetes/manifests/etcd.yaml
```
```bash
cat saeed.csr | base64 -w 0
```
```yaml
---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: saeed
spec:
  groups:
  - system:authenticated
  request: <Paste the base64 encoded value of the CSR file>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
```
```bash
kubectl apply -f saeed-csr.yaml
kubectl get csr
kubectl certificate approve saeed

kubectl get csr <csr-name> -o yaml
# Reject that request
kubectl certificate deny <csr-name>
kubectl delete csr <csr-name>
```
---

# kube config
## What is kubeconfig?

**kubeconfig** is a configuration file used by **kubectl** to interact with a **Kubernetes API Server**. It contains details about clusters, users, namespaces, and authentication mechanisms, allowing `kubectl` to communicate with the appropriate cluster.

### 1. Default kubeconfig Path
By default, kubeconfig is stored in:
```sh
~/.kube/config
```
This file enables `kubectl` to manage the cluster without requiring manual authentication each time.

### 2. Viewing kubeconfig
To display the current kubeconfig settings, use:
```sh
kubectl config view
```
This shows all clusters, users, and contexts configured in the file.

### 3. Components of kubeconfig
A sample `kubeconfig` file:
```yaml
apiVersion: v1
kind: Config
clusters:
- name: my-cluster
  cluster:
    server: https://192.168.1.100:6443
    certificate-authority: /path/to/ca.crt
contexts:
- name: my-context
  context:
    cluster: my-cluster
    user: my-user
    namespace: default
current-context: my-context
users:
- name: my-user
  user:
    client-certificate: /path/to/client.crt
    client-key: /path/to/client.key
```

#### **Key Sections:**
- **clusters**: Defines Kubernetes clusters and API server endpoints.
- **users**: Contains authentication credentials.
- **contexts**: Maps a user to a cluster and namespace.
- **current-context**: Specifies the active context used by `kubectl`.

### 4. Managing kubeconfig
#### Change the Active Context:
```sh
kubectl config use-context my-context
```

#### Add a New Cluster:
```sh
kubectl config set-cluster my-new-cluster --server=https://192.168.1.200:6443 --certificate-authority=/path/to/ca.crt
```

#### Add a New User:
```sh
kubectl config set-credentials new-user --client-certificate=/path/to/client.crt --client-key=/path/to/client.key
```

#### Add a New Context:
```sh
kubectl config set-context new-context --cluster=my-new-cluster --user=new-user
```

### 5. Using a Custom kubeconfig File
If kubeconfig is stored in a non-default location, specify it using:
```sh
export KUBECONFIG=/path/to/kubeconfig
kubectl get nodes
```
Or use it directly:
```sh
kubectl --kubeconfig=/path/to/kubeconfig get pods
```

### Summary
- **kubeconfig** is a configuration file that allows `kubectl` to authenticate and interact with Kubernetes clusters.
- The default location is `~/.kube/config`, but multiple kubeconfig files can be used.
- Clusters, users, and contexts can be managed using `kubectl config` commands.
- Custom kubeconfig files can be specified with the `KUBECONFIG` environment variable or the `--kubeconfig` flag.

Proper management of kubeconfig simplifies cluster access and administration.

---

### Example
`cat my-kube-config`
```yaml
apiVersion: v1
kind: Config

clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key

current-context: test-user@development
preferences: {}
```
```bash
# To use context from kubeconfig file
kubectl config --kubeconfig=/root/my-kube-config use-context research
# Output => Switched to context "research"
# Show current-context
kubectl config --kubeconfig=/root/my-kube-config current-context
# Output => research
cat /root/my-kube-config
# Look current-context => current-context: research
```
## Set the my-kube-config file as the default kubeconfig file 
```bash
mv /root/my-kube-config root/.kube/config
```
Or
```bash
nano ~/.bashrc
```
Add the following line to export the variable:
```bash
export KUBECONFIG=/root/my-kube-config
```
```bash
source ~/.bashrc
```
#### Some issue
```bash
kubectl get pod
# error: unable to read client-cert /etc/kubernetes/pki/users/dev-user/developer-user.crt for dev-user due to open /etc/kubernetes/pki/users/dev-user/developer-user.crt: no such file or directory
```
```bash
ls /etc/kubernetes/pki/users/dev-user/
# dev-user.crt   dev-user.csr   dev-user.key
nano my-kube-config
# /etc/kubernetes/pki/users/dev-user/developer-user.crt ===> /etc/kubernetes/pki/users/dev-user/dev-user.crt
```

---

# Troubleshooting and Fixing `admin.kubeconfig`

A kubeconfig file named `admin.kubeconfig` is located in `/root/CKA`. There is an issue with the configuration. Follow these steps to diagnose and fix it.

## **1️⃣ Check the Configuration File**
Inspect the kubeconfig file:
```sh
cat /root/CKA/admin.kubeconfig
```
Or view it in a readable format:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig
```

## **2️⃣ Verify Contexts and Clusters**
Check available contexts and the current context:
```sh
kubectl config get-contexts --kubeconfig=/root/CKA/admin.kubeconfig
kubectl config current-context --kubeconfig=/root/CKA/admin.kubeconfig
```
If the current context is missing or incorrect, set it:
```sh
kubectl config use-context <correct-context-name> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **3️⃣ Check API Server Connectivity**
Ensure the API server is reachable:
```sh
kubectl cluster-info --kubeconfig=/root/CKA/admin.kubeconfig
```
If the connection fails, check the API server address:
```sh
grep "server:" /root/CKA/admin.kubeconfig
```
```
kubectl describe pod -n kube-system kube-apiserver-controlplane kube-system

kubectl describe pod -n kube-system kube-apiserver-controlplane kube-system | grep endpoint
```
Update it if necessary:
```sh
kubectl config set-cluster <cluster-name> --server=https://<control-plane-ip>:6443 --kubeconfig=/root/CKA/admin.kubeconfig
```

## **4️⃣ Validate Certificates**
Check if certificate files exist:
```sh
grep "certificate-authority" /root/CKA/admin.kubeconfig
grep "client-certificate" /root/CKA/admin.kubeconfig
grep "client-key" /root/CKA/admin.kubeconfig
```
If any are missing, update them:
```sh
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/pki/admin.crt \
  --client-key=/etc/kubernetes/pki/admin.key \
  --kubeconfig=/root/CKA/admin.kubeconfig
```

## **5️⃣ Verify User Authentication**
Check if the token is valid:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig -o jsonpath='{.users[*].user.token}'
```
If missing or incorrect, set a new token:
```sh
kubectl config set-credentials admin --token=<correct-token> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **6️⃣ Test Kubernetes Access**
Run the following command to verify that everything works:
```sh
kubectl get nodes --kubeconfig=/root/CKA/admin.kubeconfig
```
If the command succeeds, the issue is resolved! 🚀


# RBAC (Role-based Access Control)
```bash
kubectl describe pod kube-apiserver-controlplane -n kube-system
# looking for   =>   --authorization-mode=

kubectl get roles
kubectl get roles --all-namespaces
kubectl get roles -A
kubectl describe role <role-name> -n kube-system
kubectl describe role kube-proxy -n kube-system

kubectl get rolebinding
kubectl get rolebinding --all-namespaces
kubectl get rolebinding -A
kubectl describe rolebinding <rolebinding-name> -n kube-system
kubectl describe rolebinding kube-proxy -n kube-system

kubectl get pods --as dev-user
kubectl auth can-i get pods
kubectl auth can-i get pods --as dev-user

kubectl edit role <role-name> -n <namespace-name>
```
## Create role andd rolebinding
```bash
kubectl create role <role-name> --namespace=default --verb=list,create,delete --resource=pods
kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods

kubectl create rolebinding <rolebinding-name> --namespace=default --role=developer --user=dev-user
kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
```
#### Or use this yaml file
```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

---

# Cluster Roles
```bash
kubectl get clusterroles --no-headers  | wc -l
kubectl get clusterroles --no-headers  -o json | jq '.items | length'
kubectl get clusterrolebindings --no-headers  | wc -l
kubectl get clusterrolebindings --no-headers  -o json | jq '.items | length'
kubectl describe clusterrolebinding cluster-admin
kubectl describe clusterrole cluster-admin
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```

---

# Service Accounts
### Service Accounts used by machine.
```bash
kubectl get sa
kubectl get serviceaccounts
kubectl get serviceaccount -n kube-system
kubectl describe serviceaccount <serviceaccount-name>
kubectl describe serviceaccount default

kubectl create serviceaccount <serviceaccount-name>
kubectl create serviceaccount dashboard-sa
kubectl create token dashboard-sa
```
Update the Deployment yaml file:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-dashboard
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
```
```bash
kubectl apply -f <FILE-NAME>.yaml
```
##### Or
```bash
kubectl set serviceaccount deploy/<deploy-name> <serviceaccount-name>
kubectl set serviceaccount deploy/web-dashboard dashboard-sa
```

---

# How to Create a New User in Kubernetes
In Kubernetes, users are not managed directly by the system. Instead, Kubernetes uses kubeconfig files and authentication mechanisms (like certificates, tokens, etc.) to manage users. Below are the steps to create a new user in Kubernetes.
---
1. Generate a Certificate for the New User
To create a new user, you need to generate an SSL certificate for authentication.

    Steps to Generate a Certificate:
    1. Generate a Private Key:
    ```bash
    openssl genpkey -algorithm RSA -out user.key
    ```
    2. Create a Certificate Signing Request (CSR):
    ```bash
    openssl req -new -key user.key -out user.csr -subj "/CN=<username>/O=<group>"
    ```
    - Replace `<username>` with the desired username.

    - Replace `<group>` with the group the user belongs to (optional).

    3. Sign the Certificate Using the Kubernetes CA:
    ```bash
    openssl x509 -req -in user.csr -CA /path/to/ca.crt -CAkey /path/to/ca.key -CAcreateserial -out user.crt -days 365
    ```
    - Replace `/path/to/ca.crt` and `/path/to/ca.key` with the paths to your Kubernetes CA files.

2. Add the User to the `kubeconfig` File
After generating the certificate, add the new user to the `kubeconfig` file.

    1. Add the User to kubeconfig:
    ```bash
    kubectl config set-credentials <username> --client-certificate=user.crt --client-key=user.key
    ```
    2. Create a New Context for the User:
    ```bash
    kubectl config set-context <context-name> --cluster=<cluster-name> --namespace=<namespace> --user=<username>
    ```
    - Replace `<context-name>` with a name for the new context.
    - Replace `<cluster-name>` with the name of the cluster the user will access.
    - Replace `<namespace>` with the default namespace for the user (optional).
    3.Switch to the New Context:
    ```bash
    kubectl config use-context <context-name>
    ```

3. Set Up Permissions (RBAC) for the New User
After creating the user, you need to assign the necessary permissions using **Role-Based Access Control (RBAC)**.

    1. Create a Role or ClusterRole:
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
    namespace: <namespace>
    name: <role-name>
    rules:
    - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "create"]
    ```
    - Replace `<namespace>` with the namespace where the role applies.
    - Replace `<role-name>` with a name for the role.

    2. Create a RoleBinding or ClusterRoleBinding:
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
    name: <rolebinding-name>
    namespace: <namespace>
    subjects:
    - kind: User
    name: <username>
    apiGroup: rbac.authorization.k8s.io
    roleRef:
    kind: Role
    name: <role-name>
    apiGroup: rbac.authorization.k8s.io
    ```
    - Replace `<rolebinding-name>` with a name for the RoleBinding.
    - Replace `<username>` with the username you created.
    - Replace `<role-name>` with the name of the role you created.

    3. Apply the RBAC Configuration:
    ```bash
    kubectl apply -f role.yaml
    kubectl apply -f rolebinding.yaml
    ```

4. Test the New User's Access
To verify that the new user is set up correctly, use the `kubectl auth can-i` command:
```bash
kubectl auth can-i create pods --as <username>
```
### Important Notes:
- Kubernetes does not manage users in an internal database by default. Users are defined through certificates, tokens, or other authentication methods.
- If you are using an external service like LDAP or OIDC for authentication, the process will differ.
- For internal users (e.g., Service Accounts), use `ServiceAccount` instead.

By following these steps, you can create a new user in Kubernetes and assign the necessary permissions.

---

# Image Security
```bash
kubectl create secret --help
kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
```

---

# Security Contexts
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  securityContext:
    runAsUser: 1010
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
```
### nano multi-pod.yaml 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]
```
### run as root 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]
```

---

# Types of Storage in Kubernetes
In Kubernetes, storage is divided into several categories, each designed for specific scenarios:

---

## 1. EmptyDir
**What is it?**
A temporary storage space that is created when a Pod is created and deleted when the Pod is removed.
**Where is it used?**
For storing temporary data that is only needed during the lifetime of the Pod.
**Example:**
Storing temporary files that do not need to be retained after the Pod is deleted.
**Advantages:**
- Very simple and fast.
**Disadvantages:**
- Data is lost when the Pod is deleted.
**emptyDir configuration example:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:
      sizeLimit: 500Mi
```

---

## 2. HostPath
**What is it?**
Storage space on the physical disk of the node where the Pod is running.
**Where is it used?**
For scenarios where data must remain on the same node, such as logs.
**Example:**
Storing application logs on the local disk of the node.
**Advantages:**
- Simple and straightforward.
**Disadvantages:**
- If the Pod is moved to another node, it loses access to the data.
**hostPath configuration example:**
```yaml
---
# This manifest mounts /data/foo on the host as /foo inside the
# single container that runs within the hostpath-example-linux Pod.
#
# The mount into the container is read-only.
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-example-linux
spec:
  os: { name: linux }
  nodeSelector:
    kubernetes.io/os: linux
  containers:
  - name: example-container
    image: registry.k8s.io/test-webserver
    volumeMounts:
    - mountPath: /foo
      name: example-volume
      readOnly: true
  volumes:
  - name: example-volume
    # mount /data/foo, but only if that directory already exists
    hostPath:
      path: /data/foo # directory location on host
      type: Directory # this field is optional
```

---

## 3. Persistent Volume (PV) and Persistent Volume Claim (PVC)
**What is it?**
These are fundamental concepts for managing storage in Kubernetes:
- **PV (Persistent Volume)**: A persistent storage space defined independently.
- **PVC (Persistent Volume Claim)**: A request from an application to use a PV.
**Where is it used?**
For storing important and persistent data.
**Example:**
Storing database data (e.g., MySQL or MongoDB).
**Advantages:**
- Persistent and independent of Pods.
- Can be used with various storage backends (e.g., NFS, AWS EBS, GCP Persistent Disk).
**Disadvantages:**
- Requires more configuration.

---

## 4. ConfigMap and Secret
**What is it?**
For storing indirect data such as:
- **ConfigMap**: For storing application configurations.
- **Secret**: For storing sensitive data like passwords and API keys.
**Where is it used?**
For storing configurations and sensitive data.
**Example:**
Storing database passwords or server addresses.
**Advantages:**
- Secure and practical.
**Disadvantages:**
- Not suitable for large data.
**ConfigMap configuration example:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox:1.28
      command: ['sh', '-c', 'echo "The app is running!" && tail -f /dev/null']
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level.conf
```

---

## 5. Cloud Volumes
**What is it?**
Storage that uses cloud services:
- AWS EBS (Elastic Block Store)
- Azure Disk
- Google Cloud Persistent Disk
**Where is it used?**
For clusters running on the cloud.
**Example:**
Storing important database data on AWS EBS.
**Advantages:**
- Scalable and reliable.
**Disadvantages:**
- Only works on the cloud.

---

## 6. Network File System (NFS)
**What is it?**
A network-based storage space that can be accessed from multiple nodes.
**Where is it used?**
When multiple Pods need access to shared data.
**Example:**
Storing user-uploaded files.
**Advantages:**
- Shareable across nodes.
**Disadvantages:**
- May have slower performance.
**NFS configuration example:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /my-nfs-data
      name: test-volume
  volumes:
  - name: test-volume
    nfs:
      server: my-nfs-server.example.com
      path: /my-nfs-volume
      readOnly: true
```

---

## Components of Storage in Kubernetes

### Persistent Volume (PV):
- A storage resource defined by the admin.
- Like an independent storage space ready for use.

### Persistent Volume Claim (PVC):
- Applications use PVCs to request storage space.
- For example, an application might say: “I need 5 GB of storage.”

### Storage Class:
- A template for defining types of PVs.
- For example, you can say:
  - “Any PV using this Storage Class must be on SSD.”

---

## Summary:
- Kubernetes offers various storage options for different use cases:
  - **EmptyDir**: Temporary storage.
  - **HostPath**: Local node storage.
  - **PV/PVC**: Persistent storage.
  - **ConfigMap/Secret**: For configurations and sensitive data.
  - **Cloud Volumes**: Cloud-based storage.
  - **NFS**: Network-based shared storage.

---

# Storage Engine

The Storage Engine determines how data is stored and managed. Depending on your needs, you can use different types of Storage Engines:

- **File Storage**: For regular files.
- **Block Storage**: For databases and large volumes of data.
- **Object Storage**: For large and unstructured data.
- **Relational and NoSQL**: For structured or unstructured data.
- **In-Memory**: For high-speed access.
- **Distributed**: For scalability and large systems.

## Comparison of Storage Engine Types

| Storage Engine Type | Application | Disadvantages | Advantages |
|---|---|---|---|
| File Storage | Storing regular files like photos and documents | Difficult to manage at large scale | Simple and understandable |
| Block Storage | Databases and large volumes of data | Complex management | High speed |
| Object Storage | Storing large data like videos and backups | Slower speed compared to Block Storage | High scalability |
| Relational Storage | Relational databases like MySQL | Not suitable for large data | Suitable for structured data |
| NoSQL Storage | Unstructured data like logs | Limited support for complex queries | High scalability |
| In-Memory Storage | Cache and real-time systems | High cost, temporary data | Extremely high speed |
| Distributed Storage | Cloud systems and large data | Complex management | Scalability and fault tolerance |

---

# Persistent Volume (PV) & Persistent Volume Claim (PVC)
- `PV`: Persistent volume configured by the administrator.
  It is independent of the Pod and is connected to the PVC.
- `PVC`: A request from a Pod to use a PV.
- A Pod cannot directly connect to a PV; it must use a PVC.
### Access Modes:             
- RWO - ReadWriteOnce
- ROX - ReadOnlyMany
- RWX - ReadWriteMany
- RWOP - ReadWriteOncePod
### Types of **Reclaim Policy**
- Retain -- manual reclamation
- Recycle -- basic scrub (rm -rf /thevolume/*)
- Delete -- delete the volume

| **Reclaim Policy** | **Description** | **Use Case** |
|----------------|--------------------------------------------------|------------------------------|
| **Retain**    | Keeps the PV and its data after PVC deletion, but the PV enters a **Released** state and requires manual cleanup. | When you want to preserve data and reuse the PV with a new PVC. |
| **Delete**    | Deletes the PV and its data when the PVC is deleted (if dynamically provisioned). | Suitable for cloud storage (AWS EBS, GCP PD) where storage should be freed. |
| **Recycle** *(Deprecated)* | Wipes the data (like `rm -rf /data/*`) and makes the PV available again. **(Removed in newer Kubernetes versions.)** | Used in older Kubernetes versions for PV resetting. |

#### Matching Based on Specifications (Binding)

Kubernetes automatically binds a PVC to a matching PV if the following specifications **match**:
- StorageClass (if used)
- Capacity (
  - The PV capacity can be greater than the requested PVC size, but it must not be smaller.
- Access Modes 
Binding PV to PVC
- When a PVC matches a PV, that PV gets bound to the PVC, and its status changes to Bound.

---

```bash
nano pod-vol.yaml
# This pod don't use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    # directory location on container
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
```

---

```bash
nano pv-definition.yaml
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
```
```bash
kubectl create -f pv-definition.yaml
```
```bash
nano pvc-definition.yaml
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
```
```bash
kubectl create -f pvc-definition.yaml
```
```bash
nano pod-pv.yaml
# This pod use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
```
---
```bash
kubectl get pv
kubectl get pvc
kubectl delete pvc <pvc-name>
kubectl delete pvc claim-log-1
```

---

# Storage Class
#### Defines different types of storage and allows dynamic provisioning.

A **Storage Class** in Kubernetes defines the type and properties of storage, such as disk speed, type, encryption, or IOPS. It is used to dynamically provision Persistent Volumes (PVs) for Pods.

### Key Components of a Storage Class:
- **Provisioner**: Specifies the provider (e.g., GCEPersistentDisk, AWS EBS).
- **Parameters**: Configuration options for the storage type (e.g., disk size, SSD/HDD).
- **ReclaimPolicy**: Defines what happens to the PV when it's deleted (e.g., Retain or Delete).
- **VolumeBindingMode**: Determines when the PV is bound to the Pod (immediate or lazy binding).

### Do You Need to Manually Create a Persistent Volume (PV)?
No, when using a **Storage Class**, Kubernetes automatically creates a PV when a Persistent Volume Claim (PVC) is made. You don't need to create a PV manually unless you have specific storage requirements that need to be handled manually. The Storage Class handles provisioning and management of the PV automatically.

```bash
kubectl get sc
kubectl describe sc local-storage
kubectl describe pvc local-pvc | grep -A3 Events
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
```
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

---

# Network
## Ingress → Traffic entering a Pod or network.
- Definition: Ingress refers to requests that enter a Pod or network from outside.
- In Kubernetes We have two types of Ingress controls:
  - `NetworkPolicy`: Restricts access between Pods and the network.
  - `Ingress Controller`: Controls access to services from outside the cluster (such as `nginx-ingress`)
## Egress → Traffic leaving a Pod or network.
- Definition: Egress refers to requests that leave a Pod or network.
  - In Kubernetes By default, Pods can connect to anywhere on the internet or internal network.
  - Egress Policy can restrict this access.

---

# **Network Policies**
```bash
kubectl get netpol
kubectl get networkpolicy
kubectl describe networkpolicy
kubectl get svc -n kube-system
```
## When Should You Use Network Policy?
1. **To Enhance Security**:
   - If you don’t want all Pods to freely communicate with each other, you should use Network Policy.
2. **To Restrict Access**:
   - For example, only specific Pods should be allowed to connect to a database.
3. **To Control Outbound Traffic**:
   - For instance, a Pod should only be able to connect to a specific API on the internet.
4. **To Prevent Attacks**:
   - If a Pod gets compromised, you can use Network Policy to restrict its access and prevent the attack from spreading.
Note: Netowrk Plolicy Cubernetis does not work in defects and has a knife to the Networking model you choose when installing.

---

## A Complete NetworkPolicy Manifest Example

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
          podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978
```
This NetworkPolicy defines a set of rules to control incoming (Ingress) and outgoing (Egress) network traffic for Pods with the label `role: db`. Let’s break it down line by line to understand what this Policy does.
1. `podSelector` Section:
- Here, we specify that this Policy applies only to Pods with the label `role: db`.
- Any Pod without this label will not be affected by this Policy.

2. `policyTypes` Section:
- We specify that this Policy controls both incoming (Ingress) and outgoing (Egress) traffic.
- This means we define what traffic can enter the Pod and what traffic can leave the Pod.

3. `ingress` Section (Incoming Traffic Control):
- `from:`:
    - We specify which traffic is allowed to enter the Pod:
        - `ipBlock:`:
            - Only traffic from IPs within the range 172.17.0.0/16 is allowed.
            - However, IPs in the range 172.17.1.0/24 are excluded and not allowed.
        - `namespaceSelector + podSelector:`:
            - Only Pods in Namespaces with the label project: myproject and with the label role: frontend are allowed to access this Pod.
- `ports:`:
    - Only traffic on TCP protocol and port 6379 (commonly used for Redis) is allowed.

4. `egress` Section (Outgoing Traffic Control):
- `to:`:
    - We specify which traffic is allowed to leave the Pod:
        - Only traffic destined for IPs within the range 10.0.0.0/24 is allowed.
- `ports:`:
    - Only traffic on TCP protocol and port 5978 is allowed.

### What Happens When You Apply This Policy?
Incoming Traffic (Ingress):
- Only traffic from IPs within the range `172.17.0.0/16` (except `172.17.1.0/24`) is allowed.
- Or, traffic from Pods with the label `role: frontend` in Namespaces with the label `project: myproject` is allowed.
- Only traffic on port `6379` is allowed.

Outgoing Traffic (Egress):
- Only traffic destined for IPs within the range `10.0.0.0/24` is allowed.
- Only traffic on port `5978` is allowed.

---

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```
```
- Policy Name: internal-policy
- Policy Type: Egress
- Egress Allow: payroll
- Payroll Port: 8080
- Egress Allow: mysql
- MySQL Port: 3306
```
- **Solutions that Support Network Policies:`Kube-router`, `Calico`, `Roman`, `Weave-net`**
- **Solutions that DO NOT Support Network Policies: `Flannel`**

---

# CNI (Container Network Interface)

CNI is a standard or protocol that helps Kubernetes manage the network for Pods. In simple terms, CNI specifies:
- How each Pod gets assigned an IP address.
- How Pods can communicate with each other and with the outside world (e.g., the internet).

---

## Types of CNI Plugins and Their Uses
Let’s explore some popular CNI plugins and their use cases.

### 1. Flannel
**What is it?**
Flannel is one of the simplest and most popular CNI plugins. It creates an overlay network that allows all Pods to communicate with each other.

**Where is it used?**
- For simple and small clusters.
- When you don’t need complex configurations.

**Advantages:**
- Very easy to install and set up.
- Great for getting started with Kubernetes.

**Disadvantages:**
- Lacks advanced features like security or fine-grained traffic control.

---

### 2. Calico
**What is it?**
Calico is a more advanced CNI that focuses on both networking and security. It supports Network Policies.

**Where is it used?**
- For large and complex clusters.
- When you need high security.

**Advantages:**
- Supports Network Policies for precise traffic control.
- Fast performance without needing an overlay.

**Disadvantages:**
- Configuration is more complex compared to Flannel.

---

### 3. Weave Net
**What is it?**
Weave Net is a simple and user-friendly CNI that easily establishes a network between Pods. It also supports security policies.

**Where is it used?**
- For medium-sized clusters.
- When you need simple setup and basic security.

**Advantages:**
- Easy to install.
- Supports Network Policies.

**Disadvantages:**
- Performance may slow down in very large clusters.

---

### 4. Cilium
**What is it?**
Cilium is a modern and advanced CNI that uses eBPF (a new technology in Linux). It is designed for high security and performance.

**Where is it used?**
- For large and complex clusters.
- When you need advanced security and high performance.

**Advantages:**
- Very high security.
- Fast performance.
- Supports Network Policies and advanced features.

**Disadvantages:**
- More complex than other CNIs.

---

### 5. AWS VPC CNI
**What is it?**
This CNI is specifically for clusters running on AWS. It uses Amazon’s VPC network.

**Where is it used?**
- When your cluster is running on AWS.

**Advantages:**
- Full integration with AWS.
- Utilizes VPC features.

**Disadvantages:**
- Only works on AWS.

---

### 6. Azure CNI
**What is it?**
This CNI is specifically for clusters running on Azure. It uses Azure Virtual Network.

**Where is it used?**
- When your cluster is running on Azure.

**Advantages:**
- Full integration with Azure.
- Utilizes Azure Networking features.

**Disadvantages:**
- Only works on Azure.

---

### 7. Google Cloud CNI
**What is it?**
This CNI is specifically for clusters running on Google Cloud. It uses Google’s VPC network.

**Where is it used?**
- When your cluster is running on Google Cloud.

**Advantages:**
- Full integration with Google Cloud.
- Utilizes GCP Networking features.

**Disadvantages:**
- Only works on Google Cloud.

---

## How to Choose the Right CNI?

- **For simple and small clusters**:
  - Use Flannel or Weave Net. They are simple and easy to use.

- **For large and complex clusters**:
  - Use Calico or Cilium. They offer advanced features like security and Network Policies.

- **For clusters on Cloud**:
  - If on AWS: Use AWS VPC CNI.
  - If on Azure: Use Azure CNI.
  - If on Google Cloud: Use Google Cloud CNI.

- **For advanced security**:
  - Use Cilium or Calico.

---

## Summary:
- CNI in Kubernetes is responsible for managing the network between Pods and nodes.
- There are various types of CNIs, each designed for specific scenarios:
  - **Flannel**: Simple and great for beginners.
  - **Calico**: Advanced and great for security.
  - **Weave Net**: Simple and user-friendly.
  - **Cilium**: Modern and great for security and performance.
  - **Cloud-Specific CNIs**: Like AWS VPC CNI, Azure CNI, and Google Cloud CNI.

---

| The CNI binaries are located under | `/opt/cni/bin` by default.|
|---|---|
| `ls /etc/cni/net.d/` | Identify the name of the plugin. |
| `cat /etc/cni/net.d/10-flannel.conflist` | Look at the `type` field |

---

```bash
kubectl exec <pod-name> -- ip route
```

---

What is the range of IP addresses configured for PODs on this cluster?
- `kubectl logs <weave-pod-name> -n kube-system` and look for `ipalloc-range`.

What is the IP Range configured for the services within the cluster?
- Inspect the setting on kube-api server by running on command
- `cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range`

What type of proxy is the kube-proxy configured to use?
- `kubectl logs <kube-proxy-pod-name> -n kube-system`

---

# Ingress Networking 
Summary of Steps:
1. Install Ingress Controller
2. Create Deployment and Service
3. Create Ingress Resource
4. Verify and Debug
5. Configure /etc/hosts (if needed)
6. Final Testing with curl or Browser

---

```bash
kubectl get ing
kubectl get ingress --all-namespaces

kubectl describe ingress --namespace <namespace-name>
kubectl describe ingress --namespace app-space

kubectl edit ingress --namespace <namespace-name>
kubectl edit ingress --namespace app-space
```

---

# Why Do We Need Ingress?
1. **Simpler Management of Incoming Traffic**:
   - Without Ingress, you would need to create a LoadBalancer or NodePort for each service. This becomes complex and costly, especially if you have many services.
2. **Routing Requests Based on Domain or Path**:
   - With Ingress, you can route requests to different services based on specific URLs or domains. For example:
     - `api.example.com` goes to the API service.
     - `app.example.com` goes to the web application.
3. **Advanced Features**:
   - Ingress can provide features like SSL/TLS (for HTTPS), Load Balancing, and even URL rewriting.

---

# How many ways can we define Ingress?
1. Ingress simple with a domain (Single Host)
This type of ingress is when you only have one domain and you want to direct all the requests to a specific service.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: single-host-ingress
spec:
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80
```
- Any request to test.com is directed to My-Service Service on Port 80.

---

2. Ingress with multiple domains (Multiple Hosts)
When you have a few domains and want each domain to be directed to a specific service.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-host-ingress
spec:
  rules:
  - host: foo.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: bar.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service2
            port:
              number: 80
```
- Requests to foo.test.com are directed to Service1 service.
- Requests to Bar.test.com are directed to Service2 service.

---

3. Ingress with different paths (Path-Based Routing)
When you have a domain, but you want to direct the PATH to different services.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: path-based-ingress
spec:
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/api"
        backend:
          service:
            name: api-service
            port:
              number: 80
      - pathType: Prefix
        path: "/blog"
        backend:
          service:
            name: blog-service
            port:
              number: 80
```
- Requests to example.com/api are redirected to the API-Service service.
- Requests to example.com/blog are directed to the blog-service service.

---

4. Ingress with https (TLS)
When you want requests to be directed to the services via HTTPS (SSL Certificate).
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: https-ingress
spec:
  tls:
  - hosts:
    - test.com
    secretName: tls-secret
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80
```
- Requests to https://test.com to the My-Service service.
- The SSL certificate uses TLS-SECRET.

---

5. Ingress with multiple paths and multiple domains (Complex Routing)
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: complex-ingress
spec:
  rules:
  - host: foo.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: bar.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/api"
        backend:
          service:
            name: service2
            port:
              number: 80
      - pathType: Prefix
        path: "/blog"
        backend:
          service:
            name: service3
            port:
              number: 80
```
- Requests to foo.test.com are directed to Service1 service.
- Requests to Bar.test.com/api are directed to Service2 service.
- Requests to Bar.test.com/blog are directed to Service3 service.

---

6. Ingress to rewrite the path (Path Rewrite)
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rewrite-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/app"
        backend:
          service:
            name: app-service
            port:
              number: 80
```

---

# Comparison of These Three Types of Controllers

| SCE Ingress Controller | AWS Load Balancer Controller | NGINX Ingress Controller | Feature |
|---|---|---|---|
| Less known             | AWS specific                 | Very popular             | Popularity |
| Specific networks       | AWS only                     | Everywhere (On-Prem and Cloud) | Infrastructure |
| Very high               | Excellent (Security Groups and IAM) | Good | Security |
| More complex            | Requires AWS configuration   | Simple                   | Ease of installation |
| Advanced security       | Auto Scaling, ALB/NLB        | Load Balancing, HTTPS    | Advanced features |

### Summary:
- **NGINX Ingress Controller**:
  - A popular and general-purpose controller that works on any infrastructure. It is simple, fast, and flexible.
- **AWS Load Balancer Controller**:
  - Specifically designed for clusters running on AWS. It uses AWS Load Balancers and is fully integrated with AWS.
- **SCE Ingress Controller**:
  - Designed for specific and high-security scenarios. It is mostly used in specialized networks and sensitive organizations.

---

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  name: ingress-wear-watch
  namespace: app-space
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port: 
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port: 
              number: 8080
        path: /stream
        pathType: Prefix
```
---
```bash
kubectl get svc -n critical-space
# Use this command to know the service and port details.
```
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282
```

---

```bash
kubectl create configmap <configmap-name> --namespace <namespace-name>
kubectl create configmap ingress-nginx-controller --namespace ingress-nginx

kubectl create serviceaccount ingress-nginx --namespace ingress-nginx
kubectl create serviceaccount ingress-nginx-admission --namespace ingress-nginx
```
We need to look at the Deployment's `namespace`, `containerPort`, and Service's `name`, `nodePort`.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
```

---

### Annotations are a way to configure and customize the Ingress Controller.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port: 
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080
```
---
1. Enable Path Rewrite
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
```
#### This annotation causes the request path to be rewritten to the / path.
---
2. Enable WebSocket
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
```
#### This annotation enables WebSocket and enables long-term communication.
---
3. Add Rate Limiting (limiting requests)
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "10"
```
#### This annotation limits the maximum number of requests to 10 requests per second.
---
4. Enable HTTPS Redirect
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
```
#### This annotation redirects all HTTP requests to HTTPS.
---
5. Configuring Load Balancer based on client's primary IP
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/load-balance: "ip_hash"
```
#### This annotation ensures that each client's requests are always sent to a specific server.
---
### CORS (Cross-Origin Resource Sharing) to control access to resources from different domains.
1. Enabling CORS for all requests
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type"
```
2. Only allowing a specific domain
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://example.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type, X-Requested-With"
```
3. Restricting access methods and setting Max Age
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://myapp.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-credentials: "true"
    nginx.ingress.kubernetes.io/cors-max-age: "600"
```

---

# IngressClass
- **IngressClass** specifies which Ingress Controller an Ingress resource should use. 
- When you have multiple Ingress Controllers or need specific configurations, **IngressClass** helps you manage traffic correctly. 
- Using **IngressClass** makes incoming traffic management simpler, more flexible, and more compatible with different infrastructures.

---

## Practical Example:
Assume you have a cluster with two types of Ingress Controllers installed:
- **NGINX Ingress Controller** for public services.
- **AWS Load Balancer Controller** for services running on AWS.

---

### Defining IngressClass for NGINX:
```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: nginx-ingress-class
spec:
  controller: k8s.io/ingress-nginx
```
### Defining IngressClass for AWS:
```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: aws-ingress-class
spec:
  controller: ingress.k8s.aws/alb
```
## Using IngressClass in Ingress:
For Public Services:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
spec:
  ingressClassName: nginx-ingress-class
  rules:
  - host: test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: public-service
            port:
              number: 80
```
For AWS Services:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: aws-ingress
spec:
  ingressClassName: aws-ingress-class
  rules:
  - host: aws.test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: aws-service
            port:
              number: 80
```
#### Summary:
- IngressClass helps you specify which Ingress Controller to use for an Ingress resource.
- It simplifies and enhances traffic management, especially when multiple Ingress Controllers are involved.
- By using IngressClass, you can ensure that traffic is routed correctly based on your infrastructure needs.

---

# Gateway API
```bash
# 1. Install the Gateway API resources
kubectl kustomize "https://github.com/nginx/nginx-gateway-fabric/config/crd/gateway-api/standard?ref=v1.5.1" | kubectl apply -f -
# 2. Deploy the NGINX Gateway Fabric CRDs
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/crds.yaml
# 3. Deploy NGINX Gateway Fabric
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/nodeport/deploy.yaml
# 4. Verify the Deployment
kubectl get pods -n nginx-gateway
# 5. View the nginx-gateway service
kubectl get svc -n nginx-gateway nginx-gateway -o yaml
# 6. Update the nginx-gateway service to expose ports 30080 for HTTP and 30081 for HTTPS
kubectl patch svc nginx-gateway -n nginx-gateway --type='json' -p='[
  {"op": "replace", "path": "/spec/ports/0/nodePort", "value": 30080},
  {"op": "replace", "path": "/spec/ports/1/nodePort", "value": 30081}
]'
```
---
```bash
nano gateway.yaml
```
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: nginx-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      port: 80
      protocol: HTTP
      allowedRoutes: 
       namespaces: 
        from: All
```
```bash
kubectl apply -f gateway.yaml
kubectl get gateways -n nginx-gateway
```

---

# HTTPRoute
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: frontend-route
  namespace: default
spec:
  parentRefs:
  - name: nginx-gateway
    namespace: nginx-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: frontend-svc
      port: 80
```

---

# Application Failure
### Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
```
# Check:
- Service
  - match the name of service and DB_HOST in deployment
  - match the `endpoint` and `port` of service and pod
  ```bash
  kubectl get pods -o wide
  kubectl describe svc <service-name>
  kubectl edit svc <service-name>
  ```
- Labels & Selector
  - If labels and selectors are not the same, the resources will not be connected correctly!
  - `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
  - `selector` in `Service` must match `labels` in `Pods` to send them traffic.
  - `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
  - `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.
  ```bash
  kubectl describe svc <service-name> | grep -i selector
  kubectl describe pod <pod-name> | grep -i label
  ```
- Deployment
  - Check the `env`
  ```bash
  kubectl describe deploy <deploy-name>
  kubectl edit deploy <deploy-name>
  ```
- Pod
  - Check the `Environment`
  ```bash
  kubectl describe pod <pod-name>
  kubectl logs <pod-name>
  kubectl logs <pod-name> -f --previous
  kubectl edit pod <pod-name>
  kubectl replace --force -f /tmp/kubectl-edit.yaml
  ```

---

# Control Plane Failure
If the control plane components are deployed as services, check the states of the services.
```bash
# On master nodes
service kube-apiserver status
service kube-scheduler status
service kube-controller-manager status
# On worker nodes
service kubelet status
service kube-proxy status
```
```bash
kubectl get pods -n kube-system
kubectl describe pod -n kube-system <pod-name>
kubectl logs -n kube-system <pod-name>
# If any Control Plane Components encounter an error, you can fix them using the YAML files in this directory.
cd /etc/kubernetes/manifests

# if pod is pending
kubectl describe pod -n kube-system kube-scheduler-controlplane
nano /etc/kubernetes/manifests/kube-scheduler.yaml

# if scale up/down not working
kubectl describe pod -n kube-system kube-controller-manager-controlplane
kubectl logs -n kube-system kube-controller-manager-controlplane
nano /etc/kubernetes/manifests/kube-controller-manager.yaml
```

---

# Worker Node Failure
```bash
kubectl get nodes
kubectl describe node <node-name>

# Check the possible CPU, memory and disk space on the nodes.
top
htop
df -h

# Check container run time
systemctl status containerd

# Check kubelet
service kubelet status
service kubelet start
service kubelet restart
#or
systemctl status kubelet
systemctl start kubelet
systemctl daemon-reload
systemctl restart kubelet

journalctl -u kubelet -f

nano /var/lib/kubelet/config.yaml
nano /etc/kubernetes/kubelet.conf

# Check the Kubelet Certificates
openssl x509 -in /var/lib/kubelet/worker-1.crt -text
```
# Check kube-proxy
```bash
service kube-proxy status
sudo journalctl -u kube-proxy
```
```bash
# kube-proxy pod
kubectl get pods -n kube-system | grep kube-proxy
kubectl -n kube-system logs <name_of_the_kube_proxy_pod>
kubectl -n kube-system describe configmap kube-proxy
```
```bash
kubectl -n kube-system edit ds kube-proxy
# Correct this path to /var/lib/kube-proxy/config.conf as per the ConfigMap and recreate the kube-proxy pod.
cat /var/lib/kube-proxy/config.conf
```
```yaml
spec:
    containers:
    - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
```
```bash
kubectl get pods -n kube-system | grep kube-proxy
```

---

# JSON PATH
```bash
kubectl get nodes -o json > /opt/outputs/nodes.json

kubectl get node node01 -o json > /opt/outputs/node01.json

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

kubectl config view --kubeconfig=my-kube-config  -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt

kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
```

---

# Job
**For tasks that need to be executed only once, such as database migrations, file processing, or sending batch emails.**
A **Job** is one of the workload types in Kubernetes used for running tasks or short-lived operations. Unlike **Deployment** or **DaemonSet**, which are designed for long-running applications, a **Job** is used for tasks that terminate automatically after completion.

---

## What is a Job Used For?
Imagine you have tasks that need to run only once or a specific number of times, such as:
- **Batch Processing**: For example, processing data from a file or database.
- **Running Scripts**: Like executing a script to apply specific configurations in your infrastructure.
- **Temporary Changes**: Such as restarting a specific service or cleaning up old logs.
- **Data Collection**: For example, taking a database backup or generating daily reports.

---

## How Does It Work?
A Job creates one or more Pods, and these Pods perform the specified task. When all Pods complete their tasks successfully, the Job is considered **Complete**. If a Pod fails or doesn’t complete its task, the Job automatically creates a new Pod to continue the work.

---

## Key Components of a Job
### 1. RestartPolicy
This determines how Kubernetes should behave when a Pod fails or crashes.
- **RestartPolicy = Never**: If a Pod fails, Kubernetes does not restart it. Instead, it creates a new Pod.
- **RestartPolicy = OnFailure**: If a Pod fails, Kubernetes restarts the same Pod.

### 2. Completions and Parallelism
- **Completions**: The number of times the Job must be successfully executed. For example, if `completions: 3`, the Job must succeed 3 times to be considered complete.
- **Parallelism**: The number of Pods that can run simultaneously. For example, if `parallelism: 2`, two Pods will run concurrently.

### 3. Active Deadline Seconds
This sets a time limit for the Job. If the Job does not complete within this time, Kubernetes stops it.

### 4. Backoff Limit
The number of times Kubernetes will attempt to retry a Job in case of failure. For example, if `backoffLimit: 4`, Kubernetes will stop the Job after 4 unsuccessful attempts.

---

## Practical Example of a Job

### Scenario:
Suppose you want to create a Job that runs a simple Bash script and prints a message.

### YAML File:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  completions: 3
  parallelism: 2
  backoffLimit: 4
  activeDeadlineSeconds: 60
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello, Kubernetes!"]
```

### Explanation of the YAML File:
* `kind`: Job: Specifies that this workload is a Job.

* `completions`: 3: The Job must succeed 3 times.
* `parallelism`: 2: A maximum of 2 Pods can run simultaneously.
* `backoffLimit`: 4: If a Pod fails, Kubernetes will retry the Job up to 4 times.
* `activeDeadlineSeconds`: 60: If the Job does not complete within 60 seconds, Kubernetes will stop it.
* `restartPolicy`: Never: If a Pod fails, Kubernetes will not restart it and will create a new Pod instead.
* `containers`: A container using the busybox image that prints the message: Hello, Kubernetes!.

## Summary
* Job is ideal for running short-lived tasks or batch operations.
* It ensures that tasks are completed successfully, even if Pods fail.
* You can control the number of completions, parallelism, retries, and time limits for a Job.

---

# Cleaning Up Pods in Kubernetes Jobs
In Kubernetes, when a Job is executed, the Pods created by the Job remain in the cluster after they finish their tasks (whether they succeed or fail). This means the Pods are not deleted and can still be seen in the following states:
- **Completed**: If the Pod successfully completed its task.
- **Failed**: If the Pod failed for any reason.
This behavior is designed to allow you to review execution history (logs, statuses, and outputs). However, it can lead to some issues:

---

## Why is Cleaning Up Pods Important?
1. **Accumulation of Old Pods (Resource Usage)**:
   - When old Pods are not deleted, a large number of Pods in the `Completed` or `Failed` state accumulate in the cluster.
   - This unnecessarily consumes storage space and management resources like ETCD.
2. **Reduced Cluster Readability and Management**:
   - The presence of many old Pods makes it harder to find active or running Pods.
   - For operations teams, it can be time-consuming to manually review and delete old Pods.
3. **Resource Limitations in the Cluster**:
   - Every Pod in the cluster (even in the `Completed` state) still consumes management resources. Excessive accumulation of such Pods can reduce cluster efficiency.

---

## How to Solve This Problem?
Kubernetes provides several solutions to address this issue, depending on your needs:

### 1. TTL for Jobs (TTL Controller)
Starting from Kubernetes v1.21, the **TTL Controller** feature was introduced. This feature allows Pods associated with a Job to be automatically deleted after the Job completes (whether successfully or unsuccessfully).
- **TTL (Time To Live)**: The amount of time Pods remain in the cluster after the Job finishes.

#### How to Configure TTL?
You can set the `ttlSecondsAfterFinished` field in the Job manifest. For example:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  ttlSecondsAfterFinished: 30
  template:
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello, Kubernetes!"]
      restartPolicy: Never
```
#### Explanation:
- `ttlSecondsAfterFinished: 30`: This means that 30 seconds after the Job reaches the `Completed` or `Failed` state, the Pods and Job will be automatically deleted.

### 2. Manually Deleting Jobs and Pods
If you have Jobs without TTL configured or need to clean them up manually, you can use the following commands:

Delete a Job:
```bash
kubectl delete job <job-name>
```
Delete Pods Associated with a Job:
```bash
kubectl delete pod -l job-name=<job-name>
```
#### Explanation:
- Deleting a Job does not automatically delete its associated Pods unless TTL is configured.
- The second command allows you to delete Pods associated with a specific Job.

### 3. Using CronJob for Periodic Cleanup
If you have many Jobs and don’t want to manage them manually or with TTL, you can create a CronJob to periodically clean up old Pods.

### 4. Configuring TTL Controller for the Entire Cluster
If you want the TTL Controller to be enabled by default for all Jobs in the cluster, you can activate this feature in the cluster settings. This requires access to the API Server configuration.

```bash
kubectl get job
```
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 3
  parallelism: 3
  backoffLimit: 25 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never # Does not automatically restart the terminated container.
```

---

# CronJob
**For tasks that need to be executed regularly or at specific times, such as regular backups, service status monitoring, or sending periodic reports.**
**CronJob** is one of the workload types in Kubernetes used for scheduling Jobs at specific times. If you have experience with `cron` in Linux, CronJob works similarly but within the Kubernetes ecosystem, leveraging Kubernetes' capabilities. CronJob is designed for tasks like scheduled processing, routine email sending, database backups, or running automated scripts. It overcomes the limitations of regular Jobs and allows you to manage Jobs with precise scheduling.

---

## What Problems Does CronJob Solve?

Before CronJob, if you wanted to run Jobs on a schedule, you had to design a separate mechanism or use external tools. CronJob solves this issue entirely and provides the following features:

1. **Precise Scheduling**:
   - CronJob allows you to specify the execution time using the cron format.
   - For example, daily, weekly, or even every minute.
2. **Control Over Old Jobs**:
   - You can define how many successful or failed Jobs are stored in the history.
3. **Automatic Job Management**:
   - You don’t need to manually handle Job reruns or manage old Jobs. Kubernetes handles this automatically.
4. **Native Kubernetes Integration**:
   - Job execution and management are handled natively within Kubernetes, eliminating the need for external tools.

---

## Important Note: CronJob Time Must Align with Kube Controller Manager
When configuring the CronJob schedule, you must ensure that the time zone of the Kubernetes server (i.e., `kube-controller-manager`) matches the time zone you set for the CronJob. If these are not aligned, the CronJob may run at the wrong time.

---

## Interval in CronJob
The interval or time schedule in CronJob is defined using the cron format. The cron format consists of 5 main fields:
```bash
* * * * *
- - - - -
| | | | |
| | | | +----- Day of Week (0 - 7) (0 or 7 = Sunday)
| | | +------- Month (1 - 12)
| | +--------- Day of Month (1 - 31)
| +----------- Hour (0 - 23)
+------------- Minute (0 - 59)
```
### Examples:
- `*/5 * * * *`: Runs every 5 minutes.
- `0 0 * * *`: Runs every day at midnight.
- `0 0 1 * *`: Runs on the first day of every month at midnight.

---

## History in CronJob
Kubernetes allows you to limit the number of successful and failed Jobs stored in the history. This is done using the following fields:
1. **`.spec.successfulJobsHistoryLimit`**:
   - The number of successfully completed Jobs to keep in history.
   - Default value: `3`.
2. **`.spec.failedJobsHistoryLimit`**:
   - The number of failed Jobs to keep in history.
   - Default value: `1`.
You can adjust these values in the CronJob definition.

---

## Where is CronJob Useful?
- **Taking Backups**: Take a database backup every night at midnight.
- **Cleaning Old Logs**: Delete old logs once a week.
- **Sending Reports**: Send a system status report every morning.
- **System Checks**: Run a script every 10 minutes to check the system status.

---

## Summary
- **CronJob** is ideal for running scheduled tasks in Kubernetes.
- It provides precise scheduling, automatic Job management, and native Kubernetes integration.
- You can control the history of successful and failed Jobs to keep your cluster clean and efficient.

```bash
kubectl get cronjob
```
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never
```
**Using these resources helps you manage various processes automatically and with proper scheduling.**

Another Ex:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dice
spec:
  schedule: "*/1 * * * *"  # runs every one minute
  jobTemplate:
    spec:
      completions: 1
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      activeDeadlineSeconds: 20 # If the task is not completed within 20 seconds the job will be fail and pods will be terminated.
      template:
        spec:
          containers:
          - name: dice
            image: kodekloud/throw-dice
          restartPolicy: Never
```

---

# StatefulSets
## Why Use StatefulSets?

**StatefulSets** in Kubernetes are used for applications that require persistent state and stable, unique network identifiers. They are designed for workloads that need to maintain state across pod restarts, like databases or services that need specific network identities.

### What is a StatefulSet?

A **StatefulSet** is a controller in Kubernetes used to manage stateful applications. Unlike **Deployments** (used for stateless apps), StatefulSets ensure that pods maintain their identity, persistent storage, and stable network names.

### Key Features:
- **Stable, Unique Network Identifiers**: Pods are assigned fixed names (`pod-0`, `pod-1`, ...) that remain consistent across restarts.
- **Persistent Storage**: Each pod gets its own persistent volume (PV) that retains data across pod restarts.
- **Ordered Deployment and Scaling**: Pods are created and terminated in a specific order (e.g., `pod-0` first, then `pod-1`).
- **DNS Names**: Each pod gets a unique DNS name that allows it to be easily discovered in the network.

### Use Cases:
- **Databases**: StatefulSets are ideal for distributed databases like **Cassandra** or **MongoDB** where each pod holds state.
- **Applications** that need stable network identities and persistent storage.

### Example:
For a database like MongoDB, each pod in the StatefulSet might hold part of the database's data and have a stable hostname, such as `mongodb-0`, `mongodb-1`, etc. These names ensure that nodes can communicate reliably and retain their data.

---

Here’s an example of a StatefulSet YAML file for a MongoDB deployment:
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: "mongo"  # Use the headless service for stable networking
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo
        image: mongo:latest
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: mongo-data
          mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: mongo-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
```
Explanation:
- `replicas`: 3: Creates 3 MongoDB pods.
- `volumeClaimTemplates`: Defines persistent storage for each pod. Each pod will get its own persistent volume (PVC).
- `statefulSet Name`: The StatefulSet is named `mongo`, and each pod will get a name like `mongo-0`, `mongo-1`, `mongo-2`.
- `Service`: The `serviceName: "mongo"` allows the StatefulSet to use a headless service to manage networking and DNS for stable pod communication.
Each MongoDB pod gets its own unique volume and network identity, which is ideal for stateful applications like databases.

## Headless Service in Kubernetes

A **Headless Service** in Kubernetes is a service without a cluster IP. Instead of routing traffic to a single IP, it allows direct access to individual pods via DNS. This is particularly useful for stateful applications like **StatefulSets**.

### Key Features:
- **No ClusterIP**: Set `clusterIP: None` to create a headless service with no shared IP.
- **DNS for Individual Pods**: Each pod gets a unique DNS record (e.g., `pod-0.service-name`, `pod-1.service-name`).
- **Direct Pod Access**: Allows applications to connect directly to individual pods, useful for distributed databases or services.

### Use Cases:
- **StatefulSets**: Ensures stable networking for applications like **MongoDB** or **Cassandra**.
- **Direct Pod Communication**: Enables direct communication between pods for applications that require specific network identities.

### Example:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: mongo
spec:
  clusterIP: None  # Headless service
  selector:
    app: mongo
  ports:
    - port: 27017
      targetPort: 27017
```
A headless service provides unique DNS names for each pod, ensuring stable and direct communication, ideal for stateful applications.

---

# Pod Management Policy in StatefulSet

## 1. OrderedReady (Default)
This is the default mode for StatefulSet. When this setting is active, Kubernetes manages Pods in a specific order. Specifically:

- **Creating Pods**: First, `pod-0` is created, and until this Pod reaches the `Ready` state, `pod-1` is not created. After `pod-1` is ready, `pod-2` is created, and so on.
- **Deleting Pods**: Opposite to creation, when you delete a StatefulSet or reduce the number of replicas, Kubernetes starts from the last Pod. For example, `pod-2` is deleted first, then `pod-1`, and finally `pod-0`.
- **Updating Pods**: During updates (e.g., changing the container image), Kubernetes also follows the order. For example, `pod-0` is updated first, then `pod-1`, and so on.

### When to Use OrderedReady?
This mode is ideal for applications that require a specific order for startup or shutdown. Examples include:
- Databases with replica sets (e.g., MongoDB or Cassandra).
- Applications where Pods need to communicate with each other in a specific order.

### What’s the Problem with OrderedReady?
- **Slowness**: Since Pods are created or updated one by one, it may take a long time for all Pods to become ready. For example, if you have 10 Pods, you must wait for each one to become ready sequentially.

---

## 2. Parallel
In this mode, Kubernetes manages Pods in parallel. Specifically:

- **Creating Pods**: All Pods are created simultaneously. For example, `pod-0`, `pod-1`, and `pod-2` start being created at the same time.
- **Deleting Pods**: All Pods can be deleted in parallel.
- **Updating Pods**: Pod updates can also be done in parallel (though this depends on other settings like `partition`).

### When to Use Parallel?
This mode is great for applications that do not require a specific order and need Pods to be created quickly. Examples include:
- Stateless applications that just need to start up quickly.
- When you have a large number of Pods and don’t want to wait for them to be created one by one.

### What’s the Problem with Parallel?
- **Lack of Order**: Since Pods are created simultaneously, applications that require a specific order may face issues.

---

## Comparison: OrderedReady vs. Parallel

### OrderedReady:
1. Kubernetes creates `pod-0` first.
2. When `pod-0` reaches the `Ready` state, `pod-1` is created.
3. When `pod-1` is ready, `pod-2` is created.

### Parallel:
1. Kubernetes creates all Pods (`pod-0`, `pod-1`, `pod-2`) simultaneously.
2. Each Pod becomes `Ready` as soon as it is prepared.

---

## Important Note About Changing Pod Management Policy
You **cannot** change this setting for an existing StatefulSet. If you created a StatefulSet with a specific `podManagementPolicy`, you cannot modify it later. To change it, you must delete the StatefulSet and recreate it with the new settings.

---

## Summary
- **OrderedReady**: Ideal for applications that require a specific order (e.g., databases), but slower.
- **Parallel**: Better for applications where order is not important, and speed is a priority.
The choice between these two depends on your application’s needs and your priorities.

---

# Update Strategy in StatefulSet

The **Update Strategy** in StatefulSet tells Kubernetes how to manage Pods during updates. This feature allows you to specify how and in what order Pods should be updated. There are two main types of update strategies: **RollingUpdate** and **OnDelete**.

---

## 1. RollingUpdate
This strategy allows Kubernetes to update Pods gradually and in a specific order. In this mode, Kubernetes updates Pods incrementally, changing only one Pod at a time. This ensures that your application remains available during the update.

### Features of RollingUpdate:
- **Incremental**: Only one Pod is updated at a time.
- **Availability**: A certain number of Pods are always running during the update, so your application remains available.
- **Configuration**: You can use parameters like `partition` to determine which Pods should be updated.

### Example of RollingUpdate:
Assume you have a StatefulSet with 3 Pods, and you want to update the `nginx` image to a newer version. With RollingUpdate, Kubernetes works as follows:
1. **Update pod-0**: Kubernetes first updates `web2-0`.
2. **Check status**: After `web2-0` reaches the `Ready` state, it moves to `web2-1`.
3. **Update pod-1**: Now, `web2-1` is updated.
4. **Check status**: When `web2-1` is ready, it moves to `web2-2`.
5. **Update pod-2**: Finally, `web2-2` is updated.
This process is gradual, and at least one Pod remains running at all times.

---

## 2. OnDelete
In this strategy, Pods are only updated when you manually delete them. This means you have more control over when and how the update happens, but your application may become unavailable during the update.

### Features of OnDelete:
- **Manual Control**: You must manually delete Pods for Kubernetes to create new ones with the updated version.
- **Potential Downtime**: During the update, the number of running Pods may decrease, and your application may become unavailable.

### Example of OnDelete:
Assume you have a StatefulSet with 3 Pods, and you want to update the `nginx` image. With OnDelete, you must manually delete the Pods:
1. **Delete pod-0**: You delete `web2-0`.
2. **Recreate**: Kubernetes creates a new Pod with the updated image.
3. **Delete pod-1**: Now, you delete `web2-1`.
4. **Recreate**: Kubernetes creates another new Pod.
5. **Delete pod-2**: Finally, you delete `web2-2`, and Kubernetes creates the last Pod.
In this case, you have more control, but your application may become unavailable during the update.

---

## Summary
- **RollingUpdate**: Ensures gradual updates and maintains application availability.
- **OnDelete**: Provides manual control over updates but may result in downtime.
The choice between these strategies depends on your application’s requirements and your need for control versus availability.

## Summary
- **OrderedReady**: Ideal for applications that require a specific order (e.g., databases), but slower.
- **Parallel**: Better for applications where order is not important, and speed is a priority.

---

# Understanding Partition in Kubernetes (StatefulSet)

The **Partition** concept in Kubernetes (especially in StatefulSets) is a feature that allows you to have more control over how Pods are updated. It is typically used with the **RollingUpdate** strategy and lets you specify which Pods should be updated and which should not.

## What is Partition?
- **Partition** is a number that tells Kubernetes from which Pod onward updates should be applied. Pods with an index **less than** this number will **not** be updated.
- This feature is particularly useful for controlling gradual updates and testing changes on a subset of Pods.

## Example:
Assume you have a StatefulSet with 5 Pods (`pod-0`, `pod-1`, `pod-2`, `pod-3`, `pod-4`) and you set the `partition` to `3`. In this case:
- Only Pods with an index **greater than or equal to 3** (i.e., `pod-3` and `pod-4`) will be updated.
- Pods with an index **less than 3** (i.e., `pod-0`, `pod-1`, `pod-2`) will **not** be updated.

## How to Use Partition:
In the StatefulSet manifest, you can specify the `partition` under the `updateStrategy` section. Example:
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 3  # Only Pods with index >= 3 will be updated
```

## Use Cases for Partition:
1. **Testing Changes**: You can test new changes on a subset of Pods first and, if everything works well, update the rest of the Pods.

2. **Controlled Rollouts**: If you want updates to happen gradually and with more control, you can use Partition.

3. **Stability Checks**: By updating Pods incrementally, you can ensure that new changes do not cause issues in the system.

## Practical Example:
Assume you have a StatefulSet with 5 Pods and want to test new changes on pod-3 and pod-4 first. Here’s how it works:
1. Set the `partition` to `3`.
2. Kubernetes will only update `pod-3` and `pod-4`.
3. If the changes are successful, you can set the partition to 0 to update the remaining Pods.

## Important Notes:
1. If you set the `partition` to `0`, **all Pods** will be updated.
2. If you set the `partition` to a **number greater than the number of Pods, no Pods** will be updated.

## Summary:
* Partition is a powerful tool for controlling updates in StatefulSets.
* It allows you to perform updates gradually and in a controlled manner.
* This feature is particularly useful for testing changes and reducing risks associated with updates.

---

# **Admission Controller in Kubernetes**

**Admission Controller** is an **internal component in Kubernetes** that intercepts API requests **before they are persisted in etcd** to validate, modify, or reject them. These controllers help enforce **policies** and enhance security within the cluster.

---

### **How Admission Controllers Work**

When an API request (such as creating, updating, or deleting a resource) is made, it follows these steps:

1️⃣ **Authentication & Authorization** → Verifies the user's identity and permissions.  
2️⃣ **Admission Controllers Execution** → Validates or modifies the request before saving it in etcd.  
3️⃣ **Persistence in etcd** → The final state is stored in etcd.  

---

### **Types of Admission Controllers**

Admission Controllers are categorized into two types:

1. **Mutating Admission Controllers** 🚀  
   - Can **modify** requests (e.g., adding default fields).  
   - Example: **MutatingAdmissionWebhook**, which can modify Pod specs before scheduling.  

2. **Validating Admission Controllers** 🔒  
   - Only **validate requests** and decide whether to accept or reject them.  
   - Example: **ValidatingAdmissionWebhook**, which enforces security policies.  

---

### **Example: Mutating Webhook to Add Labels to Pods**

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: add-label-webhook
webhooks:
  - name: add-label.k8s.io
    clientConfig:
      service:
        name: webhook-service
        namespace: default
        path: "/mutate"
    rules:
      - operations: ["CREATE"]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
    admissionReviewVersions: ["v1"]
    sideEffects: None
```
✅ This webhook adds a **specific label** to Pods before they are created.  

---

### **Common Admission Controllers in Kubernetes**

| **Admission Controller**         | **Type**     | **Description** |
|----------------------------------|-------------|--------------------------------------------------|
| **MutatingAdmissionWebhook**     | Mutating    | Allows modifying Kubernetes resource specs. |
| **ValidatingAdmissionWebhook**   | Validating  | Validates and rejects requests if necessary. |
| **NamespaceLifecycle**           | Validating  | Prevents deletion of system-critical namespaces. |
| **PodSecurity** *(New)*         | Validating  | Enforces security policies for Pods (replaces PSP). |
| **ResourceQuota**                | Validating  | Enforces resource limits in namespaces. |

---

### **Summary**
✅ **Admission Controllers** validate and modify API requests before persistence.  
✅ Two types: **Mutating (modifying)** and **Validating (enforcing rules)**.  
✅ Webhooks allow creating custom Admission Controllers.  
✅ Security policies like **PodSecurity** and **ResourceQuota** rely on Admission Controllers.  

---

# **API Version in Kubernetes**  

In Kubernetes, different resources (such as **Pod, Deployment, Service**) are defined as **API Objects**, and each has its own **API Version**.  

---

### **API Version Structure in Kubernetes**  

API versions in Kubernetes follow this format:  

```
<group>/<version>
```
Or for core APIs:
```
<version>
```

Examples:  
- **Pods** → `v1`  *(No group, as it belongs to the Core API)*  
- **Deployments** → `apps/v1`  
- **NetworkPolicies** → `networking.k8s.io/v1`  

---

### **API Version Types and Their Meaning**  

| **API Version**  | **Description** |
|-----------------|--------------------------------------------------|
| **alpha (e.g., v1alpha1)** | Experimental, unstable, may change. **Not recommended for production!** |
| **beta (e.g., v1beta1)** | More stable but still subject to changes. |
| **stable (e.g., v1)** | Fully stable and ready for production use. |

**Example:**  
- `networking.k8s.io/v1alpha1` → **Experimental version**  
- `apps/v1` → **Stable version**  

---

### **How to Check API Version of a Resource?**  

🔹 To list all available API versions:  
```bash
kubectl api-versions
```

🔹 To check the API version of a specific resource:  
```bash
kubectl explain deployment | grep -i "API Version"
```

---

### **Summary**  
✅ Every Kubernetes resource has an **API Version**.  
✅ **Alpha** versions are unstable, **Beta** versions are somewhat stable, and **v1** is fully stable.  
✅ Use **kubectl api-versions** to check available API versions.  

---

# kubectl port-forward
The `kubectl port-forward` command is a quick and secure way to directly access Pods or Services within a Kubernetes cluster. This command is very useful for local testing, debugging, or temporary access to internal services.
```bash
kubectl port-forward <resource-type>/<resource-name> [local-port]:<remote-port> [options]
```
`[local-port]:<remote-port>`:

- **`local-port`**: The port on your local machine (localhost) that will be connected to the resource inside the cluster.
- **`remote-port`**: The port inside the resource (Pod or Service) where the desired service is running.

`[options]`: Optional flags that you can add to the command. Some common options include:

- **`-n <namespace>`**: Specify the namespace if the resource is in a different namespace.
- **`--address`**: The local IP address that the command should listen on (default is `localhost`).
```bash
kubectl port-forward pod/my-pod 8080:80
kubectl port-forward svc/my-service 8080:80

kubectl port-forward pod/my-pod 8080:80 -n my-namespace
kubectl port-forward pod/my-pod 8080:80 --address 192.168.1.100
```
### Stopping Port Forwarding:
To stop Port Forwarding, you can terminate the command by pressing `Ctrl+C` in the terminal.

### After Running This Command:
- You can open your browser and navigate to `http://localhost:8080`.
- Any request sent to `localhost:8080` will be forwarded to port `80` inside the Pod, and the response will be displayed to you.

---

