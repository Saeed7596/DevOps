# Install
```bash
# Install virtual box
sudo apt update
sudo apt install virtualbox
VBoxManage --version
```
# How to **Enable Nested VT-x/AMD-V** On Virtualbox
```sh
# Command Prompt (CMD) Run as administrator
cd C:\Program Files\Oracle\VirtualBox
# Command:
VBoxManage modifyvm "YourVirtualMachineName" --nested-hw-virt on

# Example:
VBoxManage modifyvm "master01" --nested-hw-virt on
```
# Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
```bash
kubectl version --client
```
```bash
source <(kubectl completion bash)
# set up autocomplete in bash into the current shell, bash-completion package should be installed first.
echo "source <(kubectl completion bash)" >> ~/.bashrc
# add autocomplete permanently to your bash shell.
```
```bash
nano ~/.bashrc
```
```vim
echo 'alias k=kubectl' >> ~/.bashrc
complete -o default -F __start_kubectl k
```
```bash
source ~/.bashrc
```
# Install [minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download)
```bash
minikube start
minikube start --driver=virtualbox
minikube status
minikube config get driver
```

---

# Install on windows
##### Download [`kubectl.exe`](https://kubernetes.io/releases/download/#binaries) 
- Create a folder in Drive `C:\kube`
- Move the the `kubectl.exe` to this folder
- Search for the `Edit the system environment variables`
- Environment Variables
- Click on Path > New > `C:\kube`
- Set this path for User variables and System variables
```bash
kubectl version --client
```
##### Download [`minikube.exe`](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download)
- Add the `minikube.exe` binary to your `PATH`.
- Make sure to run PowerShell as Administrator.
```bash
$oldPath = [Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine)
if ($oldPath.Split(';') -inotcontains 'C:\minikube'){
  [Environment]::SetEnvironmentVariable('Path', $('{0};C:\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine)
}
```
```bash
minikube start
minikube status
```

---

# [Deploy a Kubernetes Cluster using Kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
## "I ensure that all nodes are on the same network and can communicate with each other, assigning each one a unique and distinct hostname."
### These steps have to be performed on `all nodes`.
`set net.bridge.bridge-nf-call-iptables` to `1`: 
```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sudo sysctl --system
```
---
# Check required ports
```bash
nc 127.0.0.1 6443 -v
```
---
# Disable Swap
- To disable swap, `sudo swapoff -a` can be used to disable swapping temporarily. To make this change persistent across reboots, make sure swap is disabled in config files like `/etc/fstab,` `systemd.swap,` depending how it was configured on your system.
---
# [Installing a container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

| Runtime       | Path to Unix domain socket                      |
|--------------|-----------------------------------------------|
| containerd   | unix:///var/run/containerd/containerd.sock   |
| CRI-O        | unix:///var/run/crio/crio.sock              |
| Docker Engine (using cri-dockerd) | unix:///var/run/cri-dockerd.sock |


```bash
sudo mkdir -p /etc/containerd
containerd config default
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml
```
---
Install `kubeadm`, `kubectl` and `kubelet` on all nodes:
```bash
sudo apt-get update

sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
- Note:
In releases older than Debian 12 and Ubuntu 22.04, directory `/etc/apt/keyrings` does not exist by default, and it should be created before the curl command.
    ```bash
    mkdir /etc/apt/keyrings
    ```
```bash
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

# To see the new version labels
sudo apt-cache madison kubeadm

sudo apt-get install -y kubelet=1.32.0-1.1 kubeadm=1.32.0-1.1 kubectl=1.32.0-1.1

sudo apt-mark hold kubelet kubeadm kubectl
```
---
# Deploy Cluster
### On master node:
```bash
IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
Default
```bash
kubeadm init --apiserver-advertise-address=192.168.75.137 --pod-network-cidr=10.244.0.0/16
```
Or
```bash
ifconfig eth0
# Lookup for inet = 192.168.75.137
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.168.75.137 --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
To start using your cluster, you need to run the following as a regular user:
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
```bash
kubeadm token create --print-join-command
```
### On worker node
```
kubeadm join 192.168.75.137:6443 --token sn2brb.c6fsi25tvscuw0ol \
        --discovery-token-ca-cert-hash sha256:6e4c42b70e8d743019f16541d960c417fe031bae8477a959649cb5459e5f6b84
```
---
# Install a Network Plugin `Flannel`
### On master node
```bash
curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
nano kube-flannel.yml
```
We are using a custom PodCIDR (`172.17.0.0/16`) instead of the default `10.244.0.0/16` when bootstrapping the Kubernetes cluster. However, the `Flannel` manifest by default is configured to use `10.244.0.0/16` as its network, which does not align with the specified PodCIDR. To resolve this, we need to update the `Network` field in the `kube-flannel-cfg` ConfigMap to match the custom PodCIDR defined during cluster initialization.
```yaml
net-conf.json: |
    {
      "Network": "10.244.0.0/16", # Update this to match the custom PodCIDR
      "Backend": {
        "Type": "vxlan"
      }
```
Locate the args section within the kube-flannel container definition. It should look like this: 
```yaml
  args:
  - --ip-masq
  - --kube-subnet-mgr
  - --iface=eth0
```
Add the additional argument `- --iface=eth0` to the existing list of arguments.
```bash
kubectl apply -f kube-flannel.yml
```
# Verify
```bash
kubectl get nodes
```

---

# General Commands
```bash
kubectl get all --all-namespaces
kubectl get all -n <namespace-name>
kubectl <command> <resource_type> --help
# resource types: [pod, node, namespace, deployment, rs(replicaSets), daemonset, service, role, rolebinding, pv, pvc, secret, configmap, ingress, job, cronjob, statefulset]
kubectl get pod --help
kubectl create pod --help
```

---

# Cluster
```bash
kubectl cluster-info
kubectl config view
kubectl config get-clusters # show all cluster
kubectl config use-context cluster1
# Switched to context "cluster1".
```

---

# Node
```bash
kubectl get nodes
kubectl get nodes -o wide
kubectl describe node <node-name>
kubectl get node <node-name> --show-labels
kubectl label node <node-name> key=value
kubectl describe node <node-name> | grep -i taints
```

---

# Pod
[kubectl run](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_run/)
```bash
kubectl get pods
watch kubectl get pods
kubectl get pods -o wide
kubectl get pods --all-namespaces
kubectl get pods --namespace kube-system
kubectl get pod <pod-name> -o yaml > my-new-pod.yaml
kubectl get pod <pod-name> -o yaml >&nbsp;my-new-pod.yaml

kubectl get pod --show-labels

kubectl run <pod-name> --image=<image_name>
kubectl run nginx --image=nginx --dry-run=client -o yaml
kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-definition.yaml
kubectl run nginx --image=nginx --port=8080
kubectl run redis -l tier=db --image=redis:alpine # -l means Label

kubectl describe pod <pod-name>
kubectl describe pod <pod-name> | grep -A5 State

kubectl create -f nginx-definition.yaml

kubectl edit pod <pod-name>

kubectl replace -f my-new-pod.yaml --force 
# This command will delete the existing one first and recreate a new one from the YAML file. 

kubectl delete pod <pod-name>
kubectl delete po <pod-name>

kubectl exec <pod-name> -c <container-name> -it -- bash
kubectl exec ubuntu-sleeper -- whoami
kubectl -n elastic-stack exec -it app -- cat /log/app.log
```
```bash
nano multi-container-pod.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - name: lemon
    image: busybox
    command:
      - sleep
      - "1000"
  - name: gold
    image: redis
```

---

| Container Restart Policy  |         |
|------------------|------------------|
| `Always`    | Automatically restarts the container after any termination. |
| `OnFailure` | Only restarts the container if it exits with an error (non-zero exit status). |
| `Never`     | Does not automatically restart the terminated container. |

---

# Static Pods
#### Pods that run directly on a node without being managed by the API server.
#### Static Pods directory `/etc/kubernetes/manifests/`
```bash
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
```

---

**kubectl [command] [TYPE] [NAME] -o **

Here are some of the commonly used formats:

1. `-o json` = Output a JSON formatted API object.
2. `-o name` = Print only the resource name and nothing else.
3. `-o wide` = Output in the plain-text format with any additional information.
4. `-o yaml` = Output a YAML formatted API object.

---

# [Limit Range](https://kubernetes.io/docs/concepts/policy/limit-range/)
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example-no-conflict-with-limitrange-cpu
spec:
  containers:
  - name: demo
    image: registry.k8s.io/pause:3.8
    resources:
      requests:
        cpu: 700m
      limits:
        cpu: 700m
```

---

# Deployments
```bash
kubectl get rs
kubectl get replicaset
kubectl get deploy
kubectl get deployment
kubectl describe deployment <deployment-name>
kubectl edit deployment <deployment-name>
kubectl explain deployment
```

---

```bash
nano nginx-deployment-definition.yaml
kubectl create -f nginx-deployment-definition.yaml
```
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx-frontend
  template:
    metadata:
      labels:
        name: nginx-frontend
    spec:
      containers:
      - name: nginx-container
        image: nginx
```
```bash
kubectl create deployment <deploy-name> --image=<image-name>
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
kubectl create deployment nginx --image=nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

kubectl scale deployment <deploy-name> --replicas=4
```

---

### **üîπ Deployment Strategies: Blue/Green & Canary**

#### **üöÄ Blue/Green Deployment**
Blue/Green is a deployment strategy that reduces downtime and risk by running two identical environments:
- **Blue**: The current, active version.
- **Green**: The new version that is tested before switching traffic.

üìå **Steps:**
1. Deploy the **Green** version alongside the **Blue** version.
2. Test the **Green** version.
3. Switch traffic from **Blue** to **Green** (typically by updating a Load Balancer).
4. If issues arise, revert back to **Blue**.

‚úÖ **Pros:**
- Minimal downtime.
- Easy rollback.
- Ensures new version is fully tested before release.

‚ùå **Cons:**
- Requires double the resources.
- More complex infrastructure management.

#### **üõ† Canary Deployment**
Canary deployment is a progressive rollout strategy where the new version is deployed to a small subset of users before a full release.

üìå **Steps:**
1. Deploy the new version to a small percentage (e.g., 5%) of traffic/users.
2. Monitor performance and logs.
3. Gradually increase traffic if no issues are detected.
4. Fully roll out the new version if everything works well.

‚úÖ **Pros:**
- Reduces risk of full failure.
- Allows real-world testing with minimal impact.
- Can be automated with monitoring tools.

‚ùå **Cons:**
- More complex traffic routing.
- Rollbacks require additional handling.

---

### **üîπ Conclusion**  
If you need to **filter or group resources**, use **Labels**.  
If you need to **store additional descriptive metadata**, use **Annotations**. üöÄ  
For **deployment strategies**, use **Blue/Green** for instant switches and **Canary** for gradual rollouts.

---

# Service
```bash
kubectl get svc
kubectl get service
kubectl describe service
```
### Types of **Service** in Kubernetes

| Service Type    | Description | Common Usage |
|----------------|--------------------------------------------------|------------------------------|
| **ClusterIP**  | The default type, assigns an internal IP within the cluster, accessible only inside the cluster. | Internal communication between services within the cluster |
| **NodePort**   | Opens a port on each Node, allowing access from outside the cluster. | External access without a Load Balancer |
| **LoadBalancer** | Creates a public IP and a Load Balancer from the cloud provider. | Load distribution and direct external access |
| **ExternalName** | Maps requests to an external domain name (e.g., `example.com`). | Redirecting requests to external services |

---

### Differences between **nodePort, port, and targetPort** in Kubernetes

| Parameter      | Description |
|---------------|--------------------------------------------------|
| **nodePort**  | The port opened on each Node, allowing external access (ranges between 30000-32767). |
| **port**      | The port defined in the Service that receives traffic. |
| **targetPort** | The port running inside the Pod that processes incoming requests. |

Example YAML configuration:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: example-service
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30080
```

In this example:
- Requests arrive at **port 80** on the Service.
- They are forwarded to **port 8080** inside the corresponding Pod.
- The Service is externally accessible on **port 30080** on each Node.

---

```bash
nano service-definition.yaml
kubectl apply -f service-definition.yaml
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort
```
```bash
kubectl run nginx --image=nginx --port=80 --expose # create a service for this pod

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

kubectl run redis -l tier=db --image=redis:alpine # -l means Label
kubectl expose pod redis --port=6379 --target-port=6379 --name=redis-service --type=ClusterIP
```

```
# if messaging is deployment
kubectl expose deployment messaging --name=messaging-service --port=6379 --target-port=6379 --type=ClusterIP
# if messaging is pod
kubectl expose pod messaging --name=messaging-service --port=6379 --target-port=6379 --type=ClusterIP
```

---

# Namespace
```bash
kubectl create ns <namespace-name>
kubectl create namespace <namespace-name>
kubectl get ns --no-headers | wc -l
kubectl -n <namespace-name> get pods --no-headers | wc -l
kubectl get pods --all-namespaces | grep <pod-name> # show the namespace of this pod-name
kubectl run redis --image=redis -n <namespace-name>
```
# Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
kubectl config view --minify | grep namespace
kubectl config set-context --current --namespace=default
```

---

# Labels and Selectors
#### Used to categorize and filter Kubernetes resources.
#### If labels and selectors are not the same, the resources will not be connected correctly!
- `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
- `selector` in `Service` must match `labels` in `Pods` to send them traffic.
- `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
- `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.

| **Use Case**                  | **Must Match**                                      |
|--------------------------------|----------------------------------------------------|
| Deployment ‚Üí ReplicaSet ‚Üí Pod  | `selector.matchLabels` in Deployment and `labels` in Pod |
| Service ‚Üí Pod                 | `selector` in Service and `labels` in Pod         |
| NetworkPolicy ‚Üí Pod           | `podSelector.matchLabels` in NetworkPolicy and `labels` in Pod |
| Ingress ‚Üí Service             | `backend.service.name` in Ingress and `metadata.name` in Service |

```bash
kubectl get pod -l env=dev
```
```bash
kubectl -n <namespace-name> describe svc <service-name> | grep -i selector
kubectl -n <namespace-name> describe pod <pod-name> | grep -i label
```
```bash
kubectl get pods --selector env=dev
kubectl get all --selector env=prod --no-headers | wc -l
kubectl get all --selector env=prod,bu=finance,tier=frontend
```

---

## **Annotations**

### **üîπ What are Annotations?**  
Annotations in Kubernetes allow you to attach arbitrary non-identifying metadata to objects. Unlike labels, annotations are not used for selection or filtering but serve for informational purposes such as automation, monitoring, and metadata storage.

---

### **Differences Between Labels and Annotations**  
| Feature           | Labels | Annotations |
|------------------|--------|------------|
| **Purpose**      | Identification & grouping of resources | Adding descriptive metadata |
| **Used in Selectors** | ‚úÖ Yes | ‚ùå No |
| **Key/Value Length Limit** | Limited | Can be long |
| **Common Use Cases** | Filtering, grouping, enforcing policies | Storing descriptions, logs, hashes, or automation data |

---

### **Example of Annotations in a Pod**  
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  annotations:
    kubernetes.io/description: "This pod runs the main backend service."
    monitoring.example.com/logs: "enabled"
spec:
  containers:
  - name: nginx
    image: nginx
```
**Explanation:**  
- `kubernetes.io/description`: Provides an explanation of the pod's function.
- `monitoring.example.com/logs`: Enables logging for monitoring purposes.

---

### **Use Cases of Annotations**  
**Monitoring & Logging:** Used for **Prometheus**, **Fluentd**, and other monitoring tools.  
**CI/CD Management:** Stores information like **commit SHA**, **Docker image version**, etc.  
**Automation:** Allows adding extra metadata for resource control.  
**Storing Non-Filterable Metadata:** Holds data that is not used in **selectors**.

---

### **Conclusion**  
If you need to **filter or group resources**, use **Labels**.  
If you need to **store additional descriptive metadata**, use **Annotations**.

---

# Taints and Tolerations
```bash
kubectl describe node node01 | grep -i taints
kubectl taint nodes node01 spray=mortein:NoSchedule
```
### with this yaml file this pod assign to node01
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
```
### node/controlplane untainted
```bash
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
```

---

# Node Affinity
#### Specifies rules to influence where Pods are scheduled.
```bash
kubectl label node <node-name> key=value
kubectl label node node01 color=blue
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
```


---

# Difference Between `label` and `taint` in Kubernetes

| Feature   | **Label** | **Taint** |
|-----------|----------|----------|
| **Purpose** | Used for **grouping and filtering** resources like `Pods`, `Nodes`, `PVCs`, `Services`, etc. | Used to **restrict scheduling** of pods on specific nodes. |
| **Applies To** | Can be applied to **Pods, Nodes, PVCs, Services, etc.** | Only applies to **Nodes**. |
| **Main Use Case** | - Organizing resources <br> - Filtering with **Selectors** <br> - Used in **Node Affinity** | - Preventing **undesired pods** from running on specific nodes <br> - Enforcing **special scheduling conditions** |
| **How Pods are Scheduled on Nodes?** | Pods are scheduled on nodes with specific labels using `nodeSelector` or `nodeAffinity`. | Only pods with matching `tolerations` can be scheduled on tainted nodes. |
| **Command to Apply** | ```sh kubectl label node node01 env=production ``` | ```sh kubectl taint node node01 key=value:Effect ``` |

**Label** is used to **identify and categorize nodes and pods**.  
**Taint** is used to **restrict pod scheduling on nodes**.

---

# DaemonSets
```bash
kubectl get ds
kubectl get daemonsets
kubectl get daemonsets -A
kubectl get daemonsets --all-namespaces
kubectl get ds -n kube-system
kubectl describe daemonset <daemonset-name> -n <namespace-name>
kubectl describe daemonset kube-proxy -n kube-system
```
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: registry.k8s.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
```

---

# Static Pods
```bash
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
```

---

# Kubelet
```bash
kubelet --version

ps -ef |  grep /usr/bin/kubelet
cat /var/lib/kubelet/config.yaml
grep -i staticpod /var/lib/kubelet/config.yaml # To find static.yaml file path
ps -aux | grep kubelet | grep --color container-runtime-endpoint
```

---

# Manual Scheduling
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: controlplane #Manual Scheduling
  containers:
  -  image: nginx
     name: nginx
```

---

# Multiple Schedulers
```bash
kubectl get pods --namespace=kube-system
kubectl describe pod kube-scheduler-controlplane --namespace=kube-system
```
```bash
nano my-scheduler.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.32.0  # changed
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
```
```bash
nano my-scheduler-config.yaml
```
```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false
```
```bash
nano my-scheduler-configmap.yaml
```
```yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
```
```bash
nano nginx-pod.yaml
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx
```
---

# Secret
#### Stores sensitive data such as passwords and API keys.
Note: 
1. Secrets are not Encrypted. Only encoded.
2. Secrets are not Encrypted in ETCD.
3. Anyone able to create pods/deployments in the same namespace can access the secrets..
Imperative
```bash
kubectl get secrets
kubectl describe secrets <secret-name>

kubectl create secret generic db-secret <secret-name> --from-literal=<key>=<value>
kubectl create secret generic db-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd

kubectl create secret generic db-secret <secret-name> --from-file=<path-to-file>
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
```
```bash
kubectl -n webhook-demo create secret tls webhook-server-tls \
    --cert "/root/keys/webhook-server-tls.crt" \
    --key "/root/keys/webhook-server-tls.key"
```
---
Declarative
```bash
nano secret-data.yaml
```
```bash
echo -n mysql | base64
# Output is bXlzcWw=
echo -n root | base64
# Output is cm9vdA==
echo -n paswrd | base64
# Output is cGFzd3Jk
```
```yaml
apiVersion: v1 
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk
```
```bash
kubectl create -f secret-data.yaml
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - image: simple-webapp-color
    name: simple-webapp-color
    envFrom:
    - secretRef:
        name: app-secret
```
# Secrets in Pods
ENV
```yaml
envFrom:
  - secretRef:
      name: app-secret
```
SINGLE ENV
```yaml
env:
  - name: DB_Password
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password        
```
Volume
```yaml
volumes:
- name: app-secret-volume
  secret:
    secretName: app-secret
```

---

# [Liveness, Readiness, and Startup Probes in Kubernetes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)

In **Kubernetes**, three types of **probes** are used to check the health of containers:

## **1. Liveness Probe**
- Checks if the **container is still alive**.
- If the probe **fails**, Kubelet **restarts** the container.
- Useful for detecting **deadlocks**.
- **Example:**
  ```yaml
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 3
    periodSeconds: 5
  ```

## **2. Readiness Probe**
- Checks if the **container is ready** to receive traffic.
- If the probe **fails**, the Pod is **removed from the service endpoints**.
- Useful for **delayed service initialization**.
- When the application crashes, the container is restarted. During this period the service directs users to the available POD, since the POD status is not READY.
- **Example:**
  ```yaml
  readinessProbe:
    tcpSocket:
      port: 8080
    initialDelaySeconds: 3
    periodSeconds: 5
  ```

## **3. Startup Probe**
- Ensures that the **application has started successfully**.
- If the probe **fails**, Kubelet **restarts** the container.
- Useful for **applications that take a long time to start**.
- **Example:**
  ```yaml
  startupProbe:
    exec:
      command: [ "cat", "/tmp/healthy" ]
    initialDelaySeconds: 30
    periodSeconds: 10
  ```

---

## **Key Differences**

| **Probe Type**  | **Purpose** | **Failure Result** | **Best For** |
|------------|------|--------------------------------|------------|
| **Liveness** | Checks if the container is alive | Container **restarts** | Apps that may freeze or deadlock |
| **Readiness** | Checks if the container is ready | Pod is **removed from service** | Apps that take time to initialize |
| **Startup** | Ensures successful startup | Container **restarts** | Apps that require a long startup time |

**Liveness** ensures the **container is running**,  
**Readiness** ensures the **service is available**,  
**Startup** ensures the **application starts correctly**.

---

Ex:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-1
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80
```
---

# Logging & Monitoring
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl top node
kubectl top node --sort-by='memory' --no-headers | head -1
kubectl top pod
kubectl top pod --sort-by='memory' --no-headers | head -1
kubectl top pod <pod-name>

kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>

kubectl logs <pod-name> -c <container-name> | grep WARNING > /opt/test.txt
```

---

# Rolling Updates and Rollbacks
```bash
kubectl create -f nginx-deployment.yaml --record
# We can use the ‚Äì -record flag to save the command used to create/update a deployment against the revision number.
nano nginx-deployment.yaml # for example change image
kubectl apply -f nginx-deployment.yaml
# or
kubectl set image <resource_type>/<resource_name> <container_name>=<new_image>
kubectl set image deployment/nginx-deployment nginx-container=nginx:1.25
# info
kubectl rollout status deployment/nginx-deployment
kubectl rollout history deployment/nginx-deployment
kubectl rollout history deployment nginx-deployment --revision=3
# undo
kubectl rollout undo deployment/nginx-deployment
kubectl rollout undo deployment nginx-deployment --to-revision=1
# To rollback to specific revision we will use the --to-revision flag.
```
### RollingUpdate  
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```
### Recreate
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```

---

# Commands and Arguments
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "green"]
```
# env
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color
```
---

# Configmaps
#### Stores key-value configuration data for Pods.
```bash
kubectl get cm
kubectl get configmaps
kubectl describe configmaps
kubectl describe cm

kubectl create configmap <config-name> --from-literal=<key>=<value>
kubectl create configmap  webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

kubectl create configmap <config-name> --from-file=<path-to-file>

```
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color
```

---

# ConfigMap in Pods
```bash
nano config-map.yaml
```
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod
```
```bash
kubectl create -f config-map.yaml
```
ENV
```yaml
envFrom:
  - configMapRef:
      name: app-config 
```
SINGLE ENV
```yaml
env:
  - name: APP_COLOR
    valueFrom:
      configMapRefKey:
        name: app-config
        key: APP_COLOR
```
Volume
```yaml
volumes:
- name: app-config-volume
  configMap:
    name: app-config
```

---

# [initContainer](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)
- initContainer is for initial setup, while sidecar is used for ongoing tasks such as logging, proxying, and monitoring.
- initContainer runs before the main container starts.
- It is useful for checking prerequisites, downloading data, configuring settings, and waiting for other services to be ready.
- If an initContainer fails, the Pod will not start.
- Use a shared Volume to exchange data between the initContainer and the main container.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app-container
    image: myapp:latest
  initContainers:
  - name: check-db
    image: busybox
    command: ['sh', '-c', 'until nc -z db-service 3306; do echo waiting for database; sleep 2; done;']
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: \['sh', '-c', 'echo The app is running! && sleep 3600'\]
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;'\]
  - name: init-mydb
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;'\]
```

---

# Horizontal Scaling & Horizontal Pod Autoscaler (HPA)
```bash
kubectl join ...
kubectl scale ...
kubectl edit ...
kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80
```
Or user the yaml file:
```bash
nano autoscale.yml
```
```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 0
```
```bash
kubectl apply -f autoscale.yml
```
```bash
kubectl get hpa
kubectl describe hpa <deployment-name>
kubectl describe hpa nginx-deployment
kubectl events hpa nginx-deployment | grep -i "ScalingReplicaSet"
kubectl events hpa nginx-deployment | grep -i "FailedGetResourceMetric"
```
Another Ex:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kkapp-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
```
---

# Vertical Pod Autoscaling (VPA)
```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
# To check the installed VPA CRDs
kubectl get crds | grep verticalpodautoscaler

kubectl get deployments -n kube-system | grep vpa 
# vpa-admission-controller, vpa-recommender, vpa-updater

kubectl get vpa
```
```yaml
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: flask-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: flask-app-4
  updatePolicy:
    updateMode: "Off"  # You can set this to "Auto" if you want automatic updates
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
        maxAllowed:
          cpu: 1000m
        controlledResources: ["cpu"]
```
Another Ex:
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: "*"
        minAllowed:
          cpu: "100m"
          memory: "100Mi"
        maxAllowed:
          cpu: "2"
          memory: "4Gi"
```
---

# OS Upgrades

```bash
kubectl get nodes
kubectl get deployments
kubectl get pods -o wide
```
### Before maintenance or removing a Node.
- Evacuates all regular Pods (except DaemonSets).
- Marks the Node as Unschedulable.
```bash
kubectl drain <node-name> --ignore-daemonsets
```
### To temporarily stop scheduling new Pods on a Node.
- Existing Pods remain.
- Node becomes Unschedulable, but no Pods are evicted.
```bash
kubectl cordon <node-name> 
```
### To re-enable scheduling on a Node after an issue is resolved.
- Node becomes Schedulable again.
- New Pods can be scheduled on it.
```bash
kubectl uncordon <node-name>
```

---

# Cluster upgrade
```bash
kubectl get nodes
# node names: controlplane, node01
kubectl describe nodes  controlplane | grep -i taint
# Taints:             <none>
kubectl describe nodes  node01 | grep -i taint
# Taints:             <none>
# This means that both nodes have the ability to schedule workloads on them.
kubeadm upgrade plan
kubectl drain controlplane --ignore-daemonsets
kubectl get nodes
# NAME           STATUS                     ROLES           AGE   VERSION
# controlplane   Ready,SchedulingDisabled   control-plane   23m   v1.31.0
# node01         Ready                      <none>          22m   v1.31.0
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
kubeadm upgrade plan v1.32.0
kubeadm upgrade apply v1.32.0

apt-get install kubelet=1.32.0-1.1

systemctl daemon-reload
systemctl restart kubelet

kubectl uncordon controlplane

# ---

kubectl drain node01 --ignore-daemonsets

ssh node01
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
# Upgrade the node 
kubeadm upgrade node
apt-get install kubelet=1.32.0-1.1
systemctl daemon-reload
systemctl restart kubelet
# Type `exit` or `logout` or enter `CTRL + d` to go back to the controlplane node.
kubectl uncordon node01
```

---

# Backup and Restore
## Types:
- Resource Configuration
  ```bash
  kubectl get all --all-namespaces - yaml > all-deploy-service.yaml
  ```
- ETCD Cluster
- Persistent Volumes
## etcd
```bash
kubectl -n kube-system logs etcd-controlplane | grep -i 'etcd-version'
kubectl -n kube-system describe pod etcd-controlplane | grep Image:
kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'
```
### Take a snapshot
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db
```
### Restore the snapshot
```bash
ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db
```
##### Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the `--data-dir`.


##### Next, update the `/etc/kubernetes/manifests/etcd.yaml`:

##### We have now restored the etcd snapshot to a new path on the controlplane - `/var/lib/etcd-from-backup`, so, the only change to be made in the YAML file, is to change the hostPath for the volume called `etcd-data` from old directory (`/var/lib/etcd`) to the new directory (`/var/lib/etcd-from-backup`).
```
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
```
##### With this change, `/var/lib/etcd` on the container points to `/var/lib/etcd-from-backup` on the controlplane (which is what we want).

##### When this file is updated, the `ETCD` pod is automatically re-created as this is a static pod placed under the `/etc/kubernetes/manifests` directory.

- ##### Note 1: As the ETCD pod has changed it will automatically restart, and also `kube-controller-manager` and `kube-scheduler`. Wait 1-2 to mins for this pods to restart. You can run the command: `watch "crictl ps | grep etcd"` to see when the ETCD pod is restarted.

- ##### Note 2: If the etcd pod is not getting `Ready 1/1`, then restart it by `kubectl delete pod -n kube-system etcd-controlplane` and wait 1 minute.

- ##### Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.


##### If you do change `--data-dir` to `/var/lib/etcd-from-backup` in the ETCD YAML file, make sure that the `volumeMounts` for `etcd-data` is updated as well, with the `mountPath` pointing to `/var/lib/etcd-from-backup` (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

---

# ETCD - Commands
```bash
etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set

etcdctl snapshot save
etcdctl endpoint health
etcdctl get
etcdctl put

kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / \
  --prefix --keys-only --limit=10 / \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --cert /etc/kubernetes/pki/etcd/server.crt \
  --key /etc/kubernetes/pki/etcd/server.key"

kubectl -n kube-system describe pod etcd-controlplane | grep data-dir
ps -ef | grep --color=auto etcd
```

---

# Backup 
```bash
kubectl describe pod -n kube-system etcd-controlplane
kubectl describe pod -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls
#  --advertise-client-urls   =>   --endpoints
kubectl describe pod -n kube-system etcd-controlplane  | grep pki
#  --trusted-ca-file         =>   --cacert
#  --cert-file               =>   --cert
#  --key-file                =>   --key

ETCDCTL_API=3 etcdctl \
  --endpoints=https://192.168.139.36:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/cluster1.db
```

---

# Check the members of the cluster:
```bash
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
   member list
```

---

# TLS Security
```bash
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text
ls -l /etc/kubernetes/pki/etcd/server* | grep .crt

crictl ps -a | grep kube-apiserver # like docker ps -a but in container.d
crictl logs --tail=2 <container-id>

nano /etc/kubernetes/manifests/kube-apiserver.yaml
nano /etc/kubernetes/manifests/etcd.yaml
```
```bash
cat saeed.csr | base64 -w 0
```
```yaml
---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: saeed
spec:
  groups:
  - system:authenticated
  request: <Paste the base64 encoded value of the CSR file>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
```
```bash
kubectl apply -f saeed-csr.yaml
kubectl get csr
kubectl certificate approve saeed

kubectl get csr <csr-name> -o yaml
# Reject that request
kubectl certificate deny <csr-name>
kubectl delete csr <csr-name>
```
---

# kube config
## What is kubeconfig?

**kubeconfig** is a configuration file used by **kubectl** to interact with a **Kubernetes API Server**. It contains details about clusters, users, namespaces, and authentication mechanisms, allowing `kubectl` to communicate with the appropriate cluster.

### 1. Default kubeconfig Path
By default, kubeconfig is stored in:
```sh
~/.kube/config
```
This file enables `kubectl` to manage the cluster without requiring manual authentication each time.

### 2. Viewing kubeconfig
To display the current kubeconfig settings, use:
```sh
kubectl config view
```
This shows all clusters, users, and contexts configured in the file.

### 3. Components of kubeconfig
A sample `kubeconfig` file:
```yaml
apiVersion: v1
kind: Config
clusters:
- name: my-cluster
  cluster:
    server: https://192.168.1.100:6443
    certificate-authority: /path/to/ca.crt
contexts:
- name: my-context
  context:
    cluster: my-cluster
    user: my-user
    namespace: default
current-context: my-context
users:
- name: my-user
  user:
    client-certificate: /path/to/client.crt
    client-key: /path/to/client.key
```

#### **Key Sections:**
- **clusters**: Defines Kubernetes clusters and API server endpoints.
- **users**: Contains authentication credentials.
- **contexts**: Maps a user to a cluster and namespace.
- **current-context**: Specifies the active context used by `kubectl`.

### 4. Managing kubeconfig
#### Change the Active Context:
```sh
kubectl config use-context my-context
```

#### Add a New Cluster:
```sh
kubectl config set-cluster my-new-cluster --server=https://192.168.1.200:6443 --certificate-authority=/path/to/ca.crt
```

#### Add a New User:
```sh
kubectl config set-credentials new-user --client-certificate=/path/to/client.crt --client-key=/path/to/client.key
```

#### Add a New Context:
```sh
kubectl config set-context new-context --cluster=my-new-cluster --user=new-user
```

### 5. Using a Custom kubeconfig File
If kubeconfig is stored in a non-default location, specify it using:
```sh
export KUBECONFIG=/path/to/kubeconfig
kubectl get nodes
```
Or use it directly:
```sh
kubectl --kubeconfig=/path/to/kubeconfig get pods
```

### Summary
- **kubeconfig** is a configuration file that allows `kubectl` to authenticate and interact with Kubernetes clusters.
- The default location is `~/.kube/config`, but multiple kubeconfig files can be used.
- Clusters, users, and contexts can be managed using `kubectl config` commands.
- Custom kubeconfig files can be specified with the `KUBECONFIG` environment variable or the `--kubeconfig` flag.

Proper management of kubeconfig simplifies cluster access and administration.

---

### Example
`cat my-kube-config`
```yaml
apiVersion: v1
kind: Config

clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key

current-context: test-user@development
preferences: {}
```
```bash
# To use context from kubeconfig file
kubectl config --kubeconfig=/root/my-kube-config use-context research
# Output => Switched to context "research"
# Show current-context
kubectl config --kubeconfig=/root/my-kube-config current-context
# Output => research
cat /root/my-kube-config
# Look current-context => current-context: research
```
## Set the my-kube-config file as the default kubeconfig file 
```bash
mv /root/my-kube-config root/.kube/config
```
Or
```bash
nano ~/.bashrc
```
Add the following line to export the variable:
```bash
export KUBECONFIG=/root/my-kube-config
```
```bash
source ~/.bashrc
```
#### Some issue
```bash
kubectl get pod
# error: unable to read client-cert /etc/kubernetes/pki/users/dev-user/developer-user.crt for dev-user due to open /etc/kubernetes/pki/users/dev-user/developer-user.crt: no such file or directory
```
```bash
ls /etc/kubernetes/pki/users/dev-user/
# dev-user.crt   dev-user.csr   dev-user.key
nano my-kube-config
# /etc/kubernetes/pki/users/dev-user/developer-user.crt ===> /etc/kubernetes/pki/users/dev-user/dev-user.crt
```

---

# Troubleshooting and Fixing `admin.kubeconfig`

A kubeconfig file named `admin.kubeconfig` is located in `/root/CKA`. There is an issue with the configuration. Follow these steps to diagnose and fix it.

## **1Ô∏è‚É£ Check the Configuration File**
Inspect the kubeconfig file:
```sh
cat /root/CKA/admin.kubeconfig
```
Or view it in a readable format:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig
```

## **2Ô∏è‚É£ Verify Contexts and Clusters**
Check available contexts and the current context:
```sh
kubectl config get-contexts --kubeconfig=/root/CKA/admin.kubeconfig
kubectl config current-context --kubeconfig=/root/CKA/admin.kubeconfig
```
If the current context is missing or incorrect, set it:
```sh
kubectl config use-context <correct-context-name> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **3Ô∏è‚É£ Check API Server Connectivity**
Ensure the API server is reachable:
```sh
kubectl cluster-info --kubeconfig=/root/CKA/admin.kubeconfig
```
If the connection fails, check the API server address:
```sh
grep "server:" /root/CKA/admin.kubeconfig
```
```
kubectl describe pod -n kube-system kube-apiserver-controlplane kube-system

kubectl describe pod -n kube-system kube-apiserver-controlplane kube-system | grep endpoint
```
Update it if necessary:
```sh
kubectl config set-cluster <cluster-name> --server=https://<control-plane-ip>:6443 --kubeconfig=/root/CKA/admin.kubeconfig
```

## **4Ô∏è‚É£ Validate Certificates**
Check if certificate files exist:
```sh
grep "certificate-authority" /root/CKA/admin.kubeconfig
grep "client-certificate" /root/CKA/admin.kubeconfig
grep "client-key" /root/CKA/admin.kubeconfig
```
If any are missing, update them:
```sh
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/pki/admin.crt \
  --client-key=/etc/kubernetes/pki/admin.key \
  --kubeconfig=/root/CKA/admin.kubeconfig
```

## **5Ô∏è‚É£ Verify User Authentication**
Check if the token is valid:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig -o jsonpath='{.users[*].user.token}'
```
If missing or incorrect, set a new token:
```sh
kubectl config set-credentials admin --token=<correct-token> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **6Ô∏è‚É£ Test Kubernetes Access**
Run the following command to verify that everything works:
```sh
kubectl get nodes --kubeconfig=/root/CKA/admin.kubeconfig
```
If the command succeeds, the issue is resolved! üöÄ


# RBAC (Role-based Access Control)
```bash
kubectl describe pod kube-apiserver-controlplane -n kube-system
# looking for   =>   --authorization-mode=

kubectl get roles
kubectl get roles --all-namespaces
kubectl get roles -A
kubectl describe role <role-name> -n kube-system
kubectl describe role kube-proxy -n kube-system

kubectl get rolebinding
kubectl get rolebinding --all-namespaces
kubectl get rolebinding -A
kubectl describe rolebinding <rolebinding-name> -n kube-system
kubectl describe rolebinding kube-proxy -n kube-system

kubectl get pods --as dev-user
kubectl auth can-i get pods
kubectl auth can-i get pods --as dev-user

kubectl edit role <role-name> -n <namespace-name>
```
## Create role andd rolebinding
```bash
kubectl create role <role-name> --namespace=default --verb=list,create,delete --resource=pods
kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods

kubectl create rolebinding <rolebinding-name> --namespace=default --role=developer --user=dev-user
kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
```
#### Or use this yaml file
```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

---

# Cluster Roles
```bash
kubectl get clusterroles --no-headers  | wc -l
kubectl get clusterroles --no-headers  -o json | jq '.items | length'
kubectl get clusterrolebindings --no-headers  | wc -l
kubectl get clusterrolebindings --no-headers  -o json | jq '.items | length'
kubectl describe clusterrolebinding cluster-admin
kubectl describe clusterrole cluster-admin
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```

---

# Service Accounts
### Service Accounts used by machine.
```bash
kubectl get sa
kubectl get serviceaccounts
kubectl get serviceaccount -n kube-system
kubectl describe serviceaccount <serviceaccount-name>
kubectl describe serviceaccount default

kubectl create serviceaccount <serviceaccount-name>
kubectl create serviceaccount dashboard-sa
kubectl create token dashboard-sa
```
Update the Deployment yaml file:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-dashboard
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
```
```bash
kubectl apply -f <FILE-NAME>.yaml
```
##### Or
```bash
kubectl set serviceaccount deploy/<deploy-name> <serviceaccount-name>
kubectl set serviceaccount deploy/web-dashboard dashboard-sa
```

---

# How to Create a New User in Kubernetes
In Kubernetes, users are not managed directly by the system. Instead, Kubernetes uses kubeconfig files and authentication mechanisms (like certificates, tokens, etc.) to manage users. Below are the steps to create a new user in Kubernetes.
---
1. Generate a Certificate for the New User
To create a new user, you need to generate an SSL certificate for authentication.

    Steps to Generate a Certificate:
    1. Generate a Private Key:
    ```bash
    openssl genpkey -algorithm RSA -out user.key
    ```
    2. Create a Certificate Signing Request (CSR):
    ```bash
    openssl req -new -key user.key -out user.csr -subj "/CN=<username>/O=<group>"
    ```
    - Replace `<username>` with the desired username.

    - Replace `<group>` with the group the user belongs to (optional).

    3. Sign the Certificate Using the Kubernetes CA:
    ```bash
    openssl x509 -req -in user.csr -CA /path/to/ca.crt -CAkey /path/to/ca.key -CAcreateserial -out user.crt -days 365
    ```
    - Replace `/path/to/ca.crt` and `/path/to/ca.key` with the paths to your Kubernetes CA files.

2. Add the User to the `kubeconfig` File
After generating the certificate, add the new user to the `kubeconfig` file.

    1. Add the User to kubeconfig:
    ```bash
    kubectl config set-credentials <username> --client-certificate=user.crt --client-key=user.key
    ```
    2. Create a New Context for the User:
    ```bash
    kubectl config set-context <context-name> --cluster=<cluster-name> --namespace=<namespace> --user=<username>
    ```
    - Replace `<context-name>` with a name for the new context.
    - Replace `<cluster-name>` with the name of the cluster the user will access.
    - Replace `<namespace>` with the default namespace for the user (optional).
    3.Switch to the New Context:
    ```bash
    kubectl config use-context <context-name>
    ```

3. Set Up Permissions (RBAC) for the New User
After creating the user, you need to assign the necessary permissions using **Role-Based Access Control (RBAC)**.

    1. Create a Role or ClusterRole:
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
    namespace: <namespace>
    name: <role-name>
    rules:
    - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "create"]
    ```
    - Replace `<namespace>` with the namespace where the role applies.
    - Replace `<role-name>` with a name for the role.

    2. Create a RoleBinding or ClusterRoleBinding:
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
    name: <rolebinding-name>
    namespace: <namespace>
    subjects:
    - kind: User
    name: <username>
    apiGroup: rbac.authorization.k8s.io
    roleRef:
    kind: Role
    name: <role-name>
    apiGroup: rbac.authorization.k8s.io
    ```
    - Replace `<rolebinding-name>` with a name for the RoleBinding.
    - Replace `<username>` with the username you created.
    - Replace `<role-name>` with the name of the role you created.

    3. Apply the RBAC Configuration:
    ```bash
    kubectl apply -f role.yaml
    kubectl apply -f rolebinding.yaml
    ```

4. Test the New User's Access
To verify that the new user is set up correctly, use the `kubectl auth can-i` command:
```bash
kubectl auth can-i create pods --as <username>
```
### Important Notes:
- Kubernetes does not manage users in an internal database by default. Users are defined through certificates, tokens, or other authentication methods.
- If you are using an external service like LDAP or OIDC for authentication, the process will differ.
- For internal users (e.g., Service Accounts), use `ServiceAccount` instead.

By following these steps, you can create a new user in Kubernetes and assign the necessary permissions.

---

# Image Security
```bash
kubectl create secret --help
kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
```

---

# Security Contexts
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  securityContext:
    runAsUser: 1010
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
```
### nano multi-pod.yaml 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]
```
### run as root 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]
```

---

# Persistent Volume (PV) & Persistent Volume Claim (PVC)
- `PV`: Persistent volume configured by the administrator.
  It is independent of the Pod and is connected to the PVC.
- `PVC`: A request from a Pod to use a PV.
- A Pod cannot directly connect to a PV; it must use a PVC.
### Access Modes:             
- RWO - ReadWriteOnce
- ROX - ReadOnlyMany
- RWX - ReadWriteMany
- RWOP - ReadWriteOncePod
### Types of **Reclaim Policy**
- Retain -- manual reclamation
- Recycle -- basic scrub (rm -rf /thevolume/*)
- Delete -- delete the volume

| **Reclaim Policy** | **Description** | **Use Case** |
|----------------|--------------------------------------------------|------------------------------|
| **Retain**    | Keeps the PV and its data after PVC deletion, but the PV enters a **Released** state and requires manual cleanup. | When you want to preserve data and reuse the PV with a new PVC. |
| **Delete**    | Deletes the PV and its data when the PVC is deleted (if dynamically provisioned). | Suitable for cloud storage (AWS EBS, GCP PD) where storage should be freed. |
| **Recycle** *(Deprecated)* | Wipes the data (like `rm -rf /data/*`) and makes the PV available again. **(Removed in newer Kubernetes versions.)** | Used in older Kubernetes versions for PV resetting. |

#### Matching Based on Specifications (Binding)

Kubernetes automatically binds a PVC to a matching PV if the following specifications **match**:
- StorageClass (if used)
- Capacity (
  - The PV capacity can be greater than the requested PVC size, but it must not be smaller.
- Access Modes 
Binding PV to PVC
- When a PVC matches a PV, that PV gets bound to the PVC, and its status changes to Bound.

---

```bash
nano pod-vol.yaml
# This pod don't use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    # directory location on container
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
```

---

```bash
nano pv-definition.yaml
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
```
```bash
kubectl create -f pv-definition.yaml
```
```bash
nano pvc-definition.yaml
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
```
```bash
kubectl create -f pvc-definition.yaml
```
```bash
nano pod-pv.yaml
# This pod use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
```
---
```bash
kubectl get pv
kubectl get pvc
kubectl delete pvc <pvc-name>
kubectl delete pvc claim-log-1
```

---

# Storage Class
#### Defines different types of storage and allows dynamic provisioning.

A **Storage Class** in Kubernetes defines the type and properties of storage, such as disk speed, type, encryption, or IOPS. It is used to dynamically provision Persistent Volumes (PVs) for Pods.

### Key Components of a Storage Class:
- **Provisioner**: Specifies the provider (e.g., GCEPersistentDisk, AWS EBS).
- **Parameters**: Configuration options for the storage type (e.g., disk size, SSD/HDD).
- **ReclaimPolicy**: Defines what happens to the PV when it's deleted (e.g., Retain or Delete).
- **VolumeBindingMode**: Determines when the PV is bound to the Pod (immediate or lazy binding).

### Do You Need to Manually Create a Persistent Volume (PV)?
No, when using a **Storage Class**, Kubernetes automatically creates a PV when a Persistent Volume Claim (PVC) is made. You don't need to create a PV manually unless you have specific storage requirements that need to be handled manually. The Storage Class handles provisioning and management of the PV automatically.

```bash
kubectl get sc
kubectl describe sc local-storage
kubectl describe pvc local-pvc | grep -A3 Events
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
```
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

---

# Network
## Ingress ‚Üí Traffic entering a Pod or network.
- Definition: Ingress refers to requests that enter a Pod or network from outside.
- In Kubernetes We have two types of Ingress controls:
  - `NetworkPolicy`: Restricts access between Pods and the network.
  - `Ingress Controller`: Controls access to services from outside the cluster (such as `nginx-ingress`)
## Egress ‚Üí Traffic leaving a Pod or network.
- Definition: Egress refers to requests that leave a Pod or network.
  - In Kubernetes By default, Pods can connect to anywhere on the internet or internal network.
  - Egress Policy can restrict this access.

---

# **Network Policies**
```bash
kubectl get netpol
kubectl get networkpolicy
kubectl describe networkpolicy
kubectl get svc -n kube-system
```
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```
```
- Policy Name: internal-policy
- Policy Type: Egress
- Egress Allow: payroll
- Payroll Port: 8080
- Egress Allow: mysql
- MySQL Port: 3306
```
**Solutions that Support Network Policies:`Kube-router`, `Calico`, `Roman`, `Weave-net`**
**Solutions that DO NOT Support Network Policies: `Flannel`**

---

## CNI (Container Network Interface),

| The CNI binaries are located under `/opt/cni/bin` by default.|
| `ls /etc/cni/net.d/` | Identify the name of the plugin. |
| `cat /etc/cni/net.d/10-flannel.conflist` | Look at the `type` field |

```bash
kubectl exec <pod-name> -- ip route
```
---
What is the range of IP addresses configured for PODs on this cluster?
`kubectl logs <weave-pod-name> -n kube-system` and look for `ipalloc-range`.

What is the IP Range configured for the services within the cluster?
Inspect the setting on kube-api server by running on command
`cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range`

What type of proxy is the kube-proxy configured to use?
`kubectl logs <kube-proxy-pod-name> -n kube-system`

---

# Ingress Networking 
Summary of Steps:
1. Install Ingress Controller
2. Create Deployment and Service
3. Create Ingress Resource
4. Verify and Debug
5. Configure /etc/hosts (if needed)
6. Final Testing with curl or Browser
```bash
kubectl get ingress --all-namespaces

kubectl describe ingress --namespace <namespace-name>
kubectl describe ingress --namespace app-space

kubectl edit ingress --namespace <namespace-name>
kubectl edit ingress --namespace app-space
```
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  name: ingress-wear-watch
  namespace: app-space
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port: 
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port: 
              number: 8080
        path: /stream
        pathType: Prefix
```
---
```bash
kubectl get svc -n critical-space
# Use this command to know the service and port details.
```
```yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282
```

---

```bash
kubectl create configmap <configmap-name> --namespace <namespace-name>
kubectl create configmap ingress-nginx-controller --namespace ingress-nginx

kubectl create serviceaccount ingress-nginx --namespace ingress-nginx
kubectl create serviceaccount ingress-nginx-admission --namespace ingress-nginx
```
We need to look at the Deployment's `namespace`, `containerPort`, and Service's `name`, `nodePort`.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
```
---
### Annotations are a way to configure and customize the Ingress Controller.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port: 
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080
```
---
1. Enable Path Rewrite
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
```
#### This annotation causes the request path to be rewritten to the / path.
---
2. Enable WebSocket
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
```
#### This annotation enables WebSocket and enables long-term communication.
---
3. Add Rate Limiting (limiting requests)
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "10"
```
#### This annotation limits the maximum number of requests to 10 requests per second.
---
4. Enable HTTPS Redirect
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
```
#### This annotation redirects all HTTP requests to HTTPS.
---
5. Configuring Load Balancer based on client's primary IP
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/load-balance: "ip_hash"
```
#### This annotation ensures that each client's requests are always sent to a specific server.
---
### CORS (Cross-Origin Resource Sharing) to control access to resources from different domains.
1. Enabling CORS for all requests
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type"
```
2. Only allowing a specific domain
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://example.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type, X-Requested-With"
```
3. Restricting access methods and setting Max Age
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://myapp.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-credentials: "true"
    nginx.ingress.kubernetes.io/cors-max-age: "600"
```

---

# Gateway API
```bash
# 1. Install the Gateway API resources
kubectl kustomize "https://github.com/nginx/nginx-gateway-fabric/config/crd/gateway-api/standard?ref=v1.5.1" | kubectl apply -f -
# 2. Deploy the NGINX Gateway Fabric CRDs
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/crds.yaml
# 3. Deploy NGINX Gateway Fabric
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/nodeport/deploy.yaml
# 4. Verify the Deployment
kubectl get pods -n nginx-gateway
# 5. View the nginx-gateway service
kubectl get svc -n nginx-gateway nginx-gateway -o yaml
# 6. Update the nginx-gateway service to expose ports 30080 for HTTP and 30081 for HTTPS
kubectl patch svc nginx-gateway -n nginx-gateway --type='json' -p='[
  {"op": "replace", "path": "/spec/ports/0/nodePort", "value": 30080},
  {"op": "replace", "path": "/spec/ports/1/nodePort", "value": 30081}
]'
```
---
```bash
nano gateway.yaml
```
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: nginx-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      port: 80
      protocol: HTTP
      allowedRoutes: 
       namespaces: 
        from: All
```
```bash
kubectl apply -f gateway.yaml
kubectl get gateways -n nginx-gateway
```

---

# HTTPRoute
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: frontend-route
  namespace: default
spec:
  parentRefs:
  - name: nginx-gateway
    namespace: nginx-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: frontend-svc
      port: 80
```

---

# Application Failure
### Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
```
# Check:
- Service
  - match the name of service and DB_HOST in deployment
  - match the `endpoint` and `port` of service and pod
  ```bash
  kubectl get pods -o wide
  kubectl describe svc <service-name>
  kubectl edit svc <service-name>
  ```
- Labels & Selector
  - If labels and selectors are not the same, the resources will not be connected correctly!
  - `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
  - `selector` in `Service` must match `labels` in `Pods` to send them traffic.
  - `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
  - `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.
  ```bash
  kubectl describe svc <service-name> | grep -i selector
  kubectl describe pod <pod-name> | grep -i label
  ```
- Deployment
  - Check the `env`
  ```bash
  kubectl describe deploy <deploy-name>
  kubectl edit deploy <deploy-name>
  ```
- Pod
  - Check the `Environment`
  ```bash
  kubectl describe pod <pod-name>
  kubectl logs <pod-name>
  kubectl logs <pod-name> -f --previous
  kubectl edit pod <pod-name>
  kubectl replace --force -f /tmp/kubectl-edit.yaml
  ```

---

# Control Plane Failure
If the control plane components are deployed as services, check the states of the services.
```bash
# On master nodes
service kube-apiserver status
service kube-scheduler status
service kube-controller-manager status
# On worker nodes
service kubelet status
service kube-proxy status
```
```bash
kubectl get pods -n kube-system
kubectl describe pod -n kube-system <pod-name>
kubectl logs -n kube-system <pod-name>
# If any Control Plane Components encounter an error, you can fix them using the YAML files in this directory.
cd /etc/kubernetes/manifests

# if pod is pending
kubectl describe pod -n kube-system kube-scheduler-controlplane
nano /etc/kubernetes/manifests/kube-scheduler.yaml

# if scale up/down not working
kubectl describe pod -n kube-system kube-controller-manager-controlplane
kubectl logs -n kube-system kube-controller-manager-controlplane
nano /etc/kubernetes/manifests/kube-controller-manager.yaml
```

---

# Worker Node Failure
```bash
kubectl get nodes
kubectl describe node <node-name>

# Check the possible CPU, memory and disk space on the nodes.
top
htop
df -h

# Check container run time
systemctl status containerd

# Check kubelet
service kubelet status
service kubelet start
service kubelet restart
#or
systemctl status kubelet
systemctl start kubelet
systemctl daemon-reload
systemctl restart kubelet

journalctl -u kubelet -f

nano /var/lib/kubelet/config.yaml
nano /etc/kubernetes/kubelet.conf

# Check the Kubelet Certificates
openssl x509 -in /var/lib/kubelet/worker-1.crt -text
```
# Check kube-proxy
```bash
service kube-proxy status
sudo journalctl -u kube-proxy
```
```bash
# kube-proxy pod
kubectl get pods -n kube-system | grep kube-proxy
kubectl -n kube-system logs <name_of_the_kube_proxy_pod>
kubectl -n kube-system describe configmap kube-proxy
```
```bash
kubectl -n kube-system edit ds kube-proxy
# Correct this path to /var/lib/kube-proxy/config.conf as per the ConfigMap and recreate the kube-proxy pod.
cat /var/lib/kube-proxy/config.conf
```
```yaml
spec:
    containers:
    - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
```
```bash
kubectl get pods -n kube-system | grep kube-proxy
```

---

# JSON PATH
```bash
kubectl get nodes -o json > /opt/outputs/nodes.json

kubectl get node node01 -o json > /opt/outputs/node01.json

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

kubectl config view --kubeconfig=my-kube-config  -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt

kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
```

---

# Job
**For tasks that need to be executed only once, such as database migrations, file processing, or sending batch emails.**
```bash
kubectl get job
```
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 3
  parallelism: 3
  backoffLimit: 25 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never # Does not automatically restart the terminated container.
```

---

# CronJob
**For tasks that need to be executed regularly or at specific times, such as regular backups, service status monitoring, or sending periodic reports.**
```bash
kubectl get cronjob
```
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never
```
**Using these resources helps you manage various processes automatically and with proper scheduling.**

Another Ex:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dice
spec:
  schedule: "*/1 * * * *"  # runs every one minute
  jobTemplate:
    spec:
      completions: 1
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      activeDeadlineSeconds: 20 # If the task is not completed within 20 seconds the job will be fail and pods will be terminated.
      template:
        spec:
          containers:
          - name: dice
            image: kodekloud/throw-dice
          restartPolicy: Never
```

---

## Why Use StatefulSets?

**StatefulSets** in Kubernetes are used for applications that require persistent state and stable, unique network identifiers. They are designed for workloads that need to maintain state across pod restarts, like databases or services that need specific network identities.

### What is a StatefulSet?

A **StatefulSet** is a controller in Kubernetes used to manage stateful applications. Unlike **Deployments** (used for stateless apps), StatefulSets ensure that pods maintain their identity, persistent storage, and stable network names.

### Key Features:
- **Stable, Unique Network Identifiers**: Pods are assigned fixed names (`pod-0`, `pod-1`, ...) that remain consistent across restarts.
- **Persistent Storage**: Each pod gets its own persistent volume (PV) that retains data across pod restarts.
- **Ordered Deployment and Scaling**: Pods are created and terminated in a specific order (e.g., `pod-0` first, then `pod-1`).
- **DNS Names**: Each pod gets a unique DNS name that allows it to be easily discovered in the network.

### Use Cases:
- **Databases**: StatefulSets are ideal for distributed databases like **Cassandra** or **MongoDB** where each pod holds state.
- **Applications** that need stable network identities and persistent storage.

### Example:
For a database like MongoDB, each pod in the StatefulSet might hold part of the database's data and have a stable hostname, such as `mongodb-0`, `mongodb-1`, etc. These names ensure that nodes can communicate reliably and retain their data.

---

Here‚Äôs an example of a StatefulSet YAML file for a MongoDB deployment:
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: "mongo"  # Use the headless service for stable networking
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo
        image: mongo:latest
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: mongo-data
          mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: mongo-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
```
Explanation:
- `replicas`: 3: Creates 3 MongoDB pods.
- `volumeClaimTemplates`: Defines persistent storage for each pod. Each pod will get its own persistent volume (PVC).
- `statefulSet Name`: The StatefulSet is named `mongo`, and each pod will get a name like `mongo-0`, `mongo-1`, `mongo-2`.
- `Service`: The `serviceName: "mongo"` allows the StatefulSet to use a headless service to manage networking and DNS for stable pod communication.
Each MongoDB pod gets its own unique volume and network identity, which is ideal for stateful applications like databases.

## Headless Service in Kubernetes

A **Headless Service** in Kubernetes is a service without a cluster IP. Instead of routing traffic to a single IP, it allows direct access to individual pods via DNS. This is particularly useful for stateful applications like **StatefulSets**.

### Key Features:
- **No ClusterIP**: Set `clusterIP: None` to create a headless service with no shared IP.
- **DNS for Individual Pods**: Each pod gets a unique DNS record (e.g., `pod-0.service-name`, `pod-1.service-name`).
- **Direct Pod Access**: Allows applications to connect directly to individual pods, useful for distributed databases or services.

### Use Cases:
- **StatefulSets**: Ensures stable networking for applications like **MongoDB** or **Cassandra**.
- **Direct Pod Communication**: Enables direct communication between pods for applications that require specific network identities.

### Example:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: mongo
spec:
  clusterIP: None  # Headless service
  selector:
    app: mongo
  ports:
    - port: 27017
      targetPort: 27017
```
A headless service provides unique DNS names for each pod, ensuring stable and direct communication, ideal for stateful applications.

---

### **Admission Controller in Kubernetes**

**Admission Controller** is an **internal component in Kubernetes** that intercepts API requests **before they are persisted in etcd** to validate, modify, or reject them. These controllers help enforce **policies** and enhance security within the cluster.

---

### **How Admission Controllers Work**

When an API request (such as creating, updating, or deleting a resource) is made, it follows these steps:

1Ô∏è‚É£ **Authentication & Authorization** ‚Üí Verifies the user's identity and permissions.  
2Ô∏è‚É£ **Admission Controllers Execution** ‚Üí Validates or modifies the request before saving it in etcd.  
3Ô∏è‚É£ **Persistence in etcd** ‚Üí The final state is stored in etcd.  

---

### **Types of Admission Controllers**

Admission Controllers are categorized into two types:

1. **Mutating Admission Controllers** üöÄ  
   - Can **modify** requests (e.g., adding default fields).  
   - Example: **MutatingAdmissionWebhook**, which can modify Pod specs before scheduling.  

2. **Validating Admission Controllers** üîí  
   - Only **validate requests** and decide whether to accept or reject them.  
   - Example: **ValidatingAdmissionWebhook**, which enforces security policies.  

---

### **Example: Mutating Webhook to Add Labels to Pods**

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: add-label-webhook
webhooks:
  - name: add-label.k8s.io
    clientConfig:
      service:
        name: webhook-service
        namespace: default
        path: "/mutate"
    rules:
      - operations: ["CREATE"]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
    admissionReviewVersions: ["v1"]
    sideEffects: None
```
‚úÖ This webhook adds a **specific label** to Pods before they are created.  

---

### **Common Admission Controllers in Kubernetes**

| **Admission Controller**         | **Type**     | **Description** |
|----------------------------------|-------------|--------------------------------------------------|
| **MutatingAdmissionWebhook**     | Mutating    | Allows modifying Kubernetes resource specs. |
| **ValidatingAdmissionWebhook**   | Validating  | Validates and rejects requests if necessary. |
| **NamespaceLifecycle**           | Validating  | Prevents deletion of system-critical namespaces. |
| **PodSecurity** *(New)*         | Validating  | Enforces security policies for Pods (replaces PSP). |
| **ResourceQuota**                | Validating  | Enforces resource limits in namespaces. |

---

### **Summary**
‚úÖ **Admission Controllers** validate and modify API requests before persistence.  
‚úÖ Two types: **Mutating (modifying)** and **Validating (enforcing rules)**.  
‚úÖ Webhooks allow creating custom Admission Controllers.  
‚úÖ Security policies like **PodSecurity** and **ResourceQuota** rely on Admission Controllers.  

---

Let me know if you have any questions! üöÄ

---

### **API Version in Kubernetes**  

In Kubernetes, different resources (such as **Pod, Deployment, Service**) are defined as **API Objects**, and each has its own **API Version**.  

---

### **API Version Structure in Kubernetes**  

API versions in Kubernetes follow this format:  

```
<group>/<version>
```
Or for core APIs:
```
<version>
```

Examples:  
- **Pods** ‚Üí `v1`  *(No group, as it belongs to the Core API)*  
- **Deployments** ‚Üí `apps/v1`  
- **NetworkPolicies** ‚Üí `networking.k8s.io/v1`  

---

### **API Version Types and Their Meaning**  

| **API Version**  | **Description** |
|-----------------|--------------------------------------------------|
| **alpha (e.g., v1alpha1)** | Experimental, unstable, may change. **Not recommended for production!** |
| **beta (e.g., v1beta1)** | More stable but still subject to changes. |
| **stable (e.g., v1)** | Fully stable and ready for production use. |

**Example:**  
- `networking.k8s.io/v1alpha1` ‚Üí **Experimental version**  
- `apps/v1` ‚Üí **Stable version**  

---

### **How to Check API Version of a Resource?**  

üîπ To list all available API versions:  
```bash
kubectl api-versions
```

üîπ To check the API version of a specific resource:  
```bash
kubectl explain deployment | grep -i "API Version"
```

---

### **Summary**  
‚úÖ Every Kubernetes resource has an **API Version**.  
‚úÖ **Alpha** versions are unstable, **Beta** versions are somewhat stable, and **v1** is fully stable.  
‚úÖ Use **kubectl api-versions** to check available API versions.  

Let me know if you have any questions! üöÄ

---
