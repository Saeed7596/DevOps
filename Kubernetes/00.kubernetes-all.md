# Install
```bash
# Install virtual box
sudo apt update
sudo apt install virtualbox
VBoxManage --version
```
# Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
```bash
kubectl version --client
```
```bash
source <(kubectl completion bash)
# set up autocomplete in bash into the current shell, bash-completion package should be installed first.
echo "source <(kubectl completion bash)" >> ~/.bashrc
# add autocomplete permanently to your bash shell.
```
```bash
nano ~/.bashrc
```
```vim
echo 'alias k=kubectl' >> ~/.bashrc
complete -o default -F __start_kubectl k
```
```bash
source ~/.bashrc
```
# Install [minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download)
```bash
minikube start
minikube start --driver=virtualbox
minikube status
minikube config get driver
```

---

# Install on windows
##### Download [`kubectl.exe`](https://kubernetes.io/releases/download/#binaries) 
- Create a folder in Drive `C:\kube`
- Move the the `kubectl.exe` to this folder
- Search for the `Edit the system environment variables`
- Environment Variables
- Click on Path > New > `C:\kube`
- Set this path for User variables and System variables
```bash
kubectl version --client
```
##### Download [`minikube.exe`](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download)
- Add the `minikube.exe` binary to your `PATH`.
- Make sure to run PowerShell as Administrator.
```bash
$oldPath = [Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine)
if ($oldPath.Split(';') -inotcontains 'C:\minikube'){
  [Environment]::SetEnvironmentVariable('Path', $('{0};C:\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine)
}
```
```bash
minikube start
minikube status
```

---

# [Deploy a Kubernetes Cluster using Kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
## "I ensure that all nodes are on the same network and can communicate with each other, assigning each one a unique and distinct hostname."
### These steps have to be performed on `all nodes`.
`set net.bridge.bridge-nf-call-iptables` to `1`: 
```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sudo sysctl --system
```
---
# Check required ports
```bash
nc 127.0.0.1 6443 -v
```
---
# Disable Swap
- To disable swap, `sudo swapoff -a` can be used to disable swapping temporarily. To make this change persistent across reboots, make sure swap is disabled in config files like `/etc/fstab,` `systemd.swap,` depending how it was configured on your system.
---
# [Installing a container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

| Runtime       | Path to Unix domain socket                      |
|--------------|-----------------------------------------------|
| containerd   | unix:///var/run/containerd/containerd.sock   |
| CRI-O        | unix:///var/run/crio/crio.sock              |
| Docker Engine (using cri-dockerd) | unix:///var/run/cri-dockerd.sock |


```bash
sudo mkdir -p /etc/containerd
containerd config default
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml
```
---
Install `kubeadm`, `kubectl` and `kubelet` on all nodes:
```bash
sudo apt-get update

sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
- Note:
In releases older than Debian 12 and Ubuntu 22.04, directory `/etc/apt/keyrings` does not exist by default, and it should be created before the curl command.
    ```bash
    mkdir /etc/apt/keyrings
    ```
```bash
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

# To see the new version labels
sudo apt-cache madison kubeadm

sudo apt-get install -y kubelet=1.32.0-1.1 kubeadm=1.32.0-1.1 kubectl=1.32.0-1.1

sudo apt-mark hold kubelet kubeadm kubectl
```
---
# Deploy Cluster
### On master node:
```bash
IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
Default
```bash
kubeadm init --apiserver-advertise-address=192.168.75.137 --pod-network-cidr=10.244.0.0/16
```
Or
```bash
ifconfig eth0
# Lookup for inet = 192.168.75.137
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.168.75.137 --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
To start using your cluster, you need to run the following as a regular user:
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
```bash
kubeadm token create --print-join-command
```
### On worker node
```
kubeadm join 192.168.75.137:6443 --token sn2brb.c6fsi25tvscuw0ol \
        --discovery-token-ca-cert-hash sha256:6e4c42b70e8d743019f16541d960c417fe031bae8477a959649cb5459e5f6b84
```
---
# Install a Network Plugin `Flannel`
### On master node
```bash
curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
nano kube-flannel.yml
```
We are using a custom PodCIDR (`172.17.0.0/16`) instead of the default `10.244.0.0/16` when bootstrapping the Kubernetes cluster. However, the `Flannel` manifest by default is configured to use `10.244.0.0/16` as its network, which does not align with the specified PodCIDR. To resolve this, we need to update the `Network` field in the `kube-flannel-cfg` ConfigMap to match the custom PodCIDR defined during cluster initialization.
```yaml
net-conf.json: |
    {
      "Network": "10.244.0.0/16", # Update this to match the custom PodCIDR
      "Backend": {
        "Type": "vxlan"
      }
```
Locate the args section within the kube-flannel container definition. It should look like this: 
```yaml
  args:
  - --ip-masq
  - --kube-subnet-mgr
  - --iface=eth0
```
Add the additional argument `- --iface=eth0` to the existing list of arguments.
```bash
kubectl apply -f kube-flannel.yml
```
# Verify
```bash
kubectl get nodes
```

---

# General Commands
```bash
kubectl get all --all-namespaces
kubectl get all -n <namespace-name>
kubectl <command> <resource_type> --help
# resource types: [pod, node, namespace, deployment, rs(replicaSets), daemonset, service, role, rolebinding, pv, pvc, secret, configmap, ingress, job, cronjob, statefulset]
kubectl get pod --help
kubectl create pod --help
```

---

# Cluster
```bash
kubectl config view
kubectl config get-clusters # show all cluster
kubectl config use-context cluster1
# Switched to context "cluster1".
```

---

# Node
```bash
kubectl get nodes
kubectl get nodes -o wide
kubectl describe node <node-name>
kubectl get node <node-name> --show-labels
kubectl label node <node-name> color=blue
kubectl describe node <node-name> | grep -i taints
```

---

# Pod
```bash
kubectl get pods
watch kubectl get pods
kubectl get pods -o wide
kubectl get pods --all-namespaces
kubectl get pods --namespace kube-system
kubectl get pod <pod-name> -o yaml > my-new-pod.yaml
kubectl get pod <pod-name> -o yaml >&nbsp;my-new-pod.yaml

kubectl get pod --show-labels

kubectl run <pod-name> --image=<image_name>
kubectl run nginx --image=nginx --dry-run=client -o yaml
kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-definition.yaml
kubectl run nginx --image=nginx --port=8080
kubectl run redis -l tier=db --image=redis:alpine # -l means Label

kubectl describe pod <pod-name>
kubectl describe pod <pod-name> | grep -A5 State

kubectl create -f nginx-definition.yaml

kubectl edit pod <pod-name>

kubectl replace -f my-new-pod.yaml --force 
# This command will delete the existing one first and recreate a new one from the YAML file. 

kubectl delete pod <pod-name>
kubectl delete po <pod-name>

kubectl exec <pod-name> -c <container-name> -it -- bash
kubectl exec ubuntu-sleeper -- whoami
kubectl -n elastic-stack exec -it app -- cat /log/app.log
```
```bash
nano multi-container-pod.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - name: lemon
    image: busybox
    command:
      - sleep
      - "1000"

  - name: gold
    image: redis
```

# Deployments
```bash
kubectl get rs
kubectl get replicaset
kubectl get deploy
kubectl get deployment
kubectl describe deployment <deployment-name>
kubectl edit deployment <deployment-name>
kubectl explain deployment
```

---

```bash
nano nginx-deployment-definition.yaml
kubectl create -f nginx-deployment-definition.yaml
```
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx-frontend
  template:
    metadata:
      labels:
        name: nginx-frontend
    spec:
      containers:
      - name: nginx-container
        image: nginx
```
```bash
kubectl create deployment <deploy-name> --image=<image-name>
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
kubectl create deployment nginx --image=nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

kubectl scale deployment <deploy-name> --replicas=4
```

---

# Service
```bash
kubectl get svc
kubectl get service
kubectl describe service
```
```bash
nano service-definition.yaml
kubectl apply -f service-definition.yaml
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort
```
```bash
kubectl run nginx --image=nginx --port=80 --expose # create a service for this pod

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

kubectl run redis -l tier=db --image=redis:alpine # -l means Label
kubectl expose pod redis --port=6379 --target-port=6379 --name=redis-service --type=ClusterIP
```

---

# Namespace
```bash
kubectl create ns <namespace-name>
kubectl create namespace <namespace-name>
kubectl get ns --no-headers | wc -l
kubectl -n <namespace-name> get pods --no-headers | wc -l
kubectl get pods --all-namespaces | grep <pod-name> # show the namespace of this pod-name
kubectl run redis --image=redis -n <namespace-name>
```
# Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
kubectl config view --minify | grep namespace
kubectl config set-context --current --namespace=default
```

---

# Labels and Selectors
#### Used to categorize and filter Kubernetes resources.
#### If labels and selectors are not the same, the resources will not be connected correctly!
- `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
- `selector` in `Service` must match `labels` in `Pods` to send them traffic.
- `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
- `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.

| **Use Case**                  | **Must Match**                                      |
|--------------------------------|----------------------------------------------------|
| Deployment → ReplicaSet → Pod  | `selector.matchLabels` in Deployment and `labels` in Pod |
| Service → Pod                 | `selector` in Service and `labels` in Pod         |
| NetworkPolicy → Pod           | `podSelector.matchLabels` in NetworkPolicy and `labels` in Pod |
| Ingress → Service             | `backend.service.name` in Ingress and `metadata.name` in Service |

```bash
kubectl -n <namespace-name> describe svc <service-name> | grep -i selector
kubectl -n <namespace-name> describe pod <pod-name> | grep -i label
```
```bash
kubectl get pods --selector env=dev
kubectl get all --selector env=prod --no-headers | wc -l
kubectl get all --selector env=prod,bu=finance,tier=frontend
```

---

# Taints and Tolerations
```bash
kubectl describe node node01 | grep -i taints
kubectl taint nodes node01 spray=mortein:NoSchedule
```
### with this yaml file this pod assign to node01
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
```
### node/controlplane untainted
```bash
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
```

---

# Node Affinity
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
```

---

# DaemonSets
```bash
kubectl get ds
kubectl get daemonsets
kubectl get daemonsets -A
kubectl get daemonsets --all-namespaces
kubectl get ds -n kube-system
kubectl describe daemonset <daemonset-name> -n <namespace-name>
kubectl describe daemonset kube-proxy -n kube-system
```
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: registry.k8s.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
```

---

# Static Pods
```bash
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
```

---

# Kubelet
```bash
kubelet --version

ps -ef |  grep /usr/bin/kubelet
cat /var/lib/kubelet/config.yaml
grep -i staticpod /var/lib/kubelet/config.yaml # To find static.yaml file path
ps -aux | grep kubelet | grep --color container-runtime-endpoint
```

---

# Manual Scheduling
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: controlplane #Manual Scheduling
  containers:
  -  image: nginx
     name: nginx
```

---

# Multiple Schedulers
```bash
kubectl get pods --namespace=kube-system
kubectl describe pod kube-scheduler-controlplane --namespace=kube-system
```
```bash
nano my-scheduler.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.32.0  # changed
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
```
```bash
nano my-scheduler-config.yaml
```
```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false
```
```bash
nano my-scheduler-configmap.yaml
```
```yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
```
```bash
nano nginx-pod.yaml
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx
```
---

# Secret
```bash
kubectl get secrets
kubectl describe secrets <secret-name>
kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
```
```bash
kubectl -n webhook-demo create secret tls webhook-server-tls \
    --cert "/root/keys/webhook-server-tls.crt" \
    --key "/root/keys/webhook-server-tls.key"
```

---

# Logging & Monitoring
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl top node
kubectl top node --sort-by='memory' --no-headers | head -1
kubectl top pod
kubectl top pod --sort-by='memory' --no-headers | head -1
kubectl top pod <pod-name>

kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>
```

---

# Rolling Updates and Rollbacks
```bash
kubectl create -f nginx-deployment.yaml --record
nano nginx-deployment.yaml # for example change image
kubectl apply -f nginx-deployment.yaml
# or
kubectl set image <resource_type>/<resource_name> <container_name>=<new_image>
kubectl set image deployment/nginx-deployment nginx-container=nginx:1.25
# info
kubectl rollout status deployment/nginx-deployment
kubectl rollout history deployment/nginx-deployment
# undo
kubectl rollout undo deployment/nginx-deployment
```
### RollingUpdate  
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```
### Recreate
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```

---

# Commands and Arguments
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "green"]
```
# env
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color
```
---

# Configmaps
```bash
kubectl get cm
kubectl get configmaps
kubectl describe configmaps
kubectl describe cm 
kubectl create configmap  webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard
```
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color
```

---

# initContainer
- initContainer is for initial setup, while sidecar is used for ongoing tasks such as logging, proxying, and monitoring.
- initContainer runs before the main container starts.
- It is useful for checking prerequisites, downloading data, configuring settings, and waiting for other services to be ready.
- If an initContainer fails, the Pod will not start.
- Use a shared Volume to exchange data between the initContainer and the main container.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app-container
    image: myapp:latest
  initContainers:
  - name: check-db
    image: busybox
    command: ['sh', '-c', 'until nc -z db-service 3306; do echo waiting for database; sleep 2; done;']
```

---

# Horizontal Scaling & Horizontal Pod Autoscaler (HPA)
```bash
kubectl join ...
kubectl scale ...
kubectl edit ...
kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80
```
Or user the yaml file:
```bash
nano autoscale.yml
```
```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 0
```
```bash
kubectl apply -f autoscale.yml
```
```bash
kubectl get hpa
kubectl describe hpa <deployment-name>
kubectl describe hpa nginx-deployment
kubectl events hpa nginx-deployment | grep -i "ScalingReplicaSet"
kubectl events hpa nginx-deployment | grep -i "FailedGetResourceMetric"
```

---

# Vertical Pod Autoscaling (VPA)
```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
# To check the installed VPA CRDs
kubectl get crds | grep verticalpodautoscaler

kubectl get deployments -n kube-system | grep vpa 
# vpa-admission-controller, vpa-recommender, vpa-updater

kubectl get vpa
```
```yaml
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: flask-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: flask-app-4
  updatePolicy:
    updateMode: "Off"  # You can set this to "Auto" if you want automatic updates
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
        maxAllowed:
          cpu: 1000m
        controlledResources: ["cpu"]
```

---

# OS Upgrades

```bash
kubectl get nodes
kubectl get deployments
kubectl get pods -o wide
```
### Before maintenance or removing a Node.
- Evacuates all regular Pods (except DaemonSets).
- Marks the Node as Unschedulable.
```bash
kubectl drain <node-name> --ignore-daemonsets
```
### To temporarily stop scheduling new Pods on a Node.
- Existing Pods remain.
- Node becomes Unschedulable, but no Pods are evicted.
```bash
kubectl cordon <node-name> 
```
### To re-enable scheduling on a Node after an issue is resolved.
- Node becomes Schedulable again.
- New Pods can be scheduled on it.
```bash
kubectl uncordon <node-name>
```

---

# Cluster upgrade
```bash
kubectl get nodes
# node names: controlplane, node01
kubectl describe nodes  controlplane | grep -i taint
# Taints:             <none>
kubectl describe nodes  node01 | grep -i taint
# Taints:             <none>
# This means that both nodes have the ability to schedule workloads on them.
kubeadm upgrade plan
kubectl drain controlplane --ignore-daemonsets
kubectl get nodes
# NAME           STATUS                     ROLES           AGE   VERSION
# controlplane   Ready,SchedulingDisabled   control-plane   23m   v1.31.0
# node01         Ready                      <none>          22m   v1.31.0
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
kubeadm upgrade plan v1.32.0
kubeadm upgrade apply v1.32.0

apt-get install kubelet=1.32.0-1.1

systemctl daemon-reload
systemctl restart kubelet

kubectl uncordon controlplane

# ---

kubectl drain node01 --ignore-daemonsets

ssh node01
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
# Upgrade the node 
kubeadm upgrade node
apt-get install kubelet=1.32.0-1.1
systemctl daemon-reload
systemctl restart kubelet
# Type `exit` or `logout` or enter `CTRL + d` to go back to the controlplane node.
kubectl uncordon node01
```

---

# Backup and Restore
## Types:
- Resource Configuration
  ```bash
  kubectl get all --all-namespaces - yaml > all-deploy-service.yaml
  ```
- ETCD Cluster
- Persistent Volumes
## etcd
```bash
kubectl -n kube-system logs etcd-controlplane | grep -i 'etcd-version'
kubectl -n kube-system describe pod etcd-controlplane | grep Image:
kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'
```
### Take a snapshot
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db
```
### Restore the snapshot
```bash
ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db
```
##### Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the `--data-dir`.


##### Next, update the `/etc/kubernetes/manifests/etcd.yaml`:

##### We have now restored the etcd snapshot to a new path on the controlplane - `/var/lib/etcd-from-backup`, so, the only change to be made in the YAML file, is to change the hostPath for the volume called `etcd-data` from old directory (`/var/lib/etcd`) to the new directory (`/var/lib/etcd-from-backup`).
```
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
```
##### With this change, `/var/lib/etcd` on the container points to `/var/lib/etcd-from-backup` on the controlplane (which is what we want).

##### When this file is updated, the `ETCD` pod is automatically re-created as this is a static pod placed under the `/etc/kubernetes/manifests` directory.

- ##### Note 1: As the ETCD pod has changed it will automatically restart, and also `kube-controller-manager` and `kube-scheduler`. Wait 1-2 to mins for this pods to restart. You can run the command: `watch "crictl ps | grep etcd"` to see when the ETCD pod is restarted.

- ##### Note 2: If the etcd pod is not getting `Ready 1/1`, then restart it by `kubectl delete pod -n kube-system etcd-controlplane` and wait 1 minute.

- ##### Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.


##### If you do change `--data-dir` to `/var/lib/etcd-from-backup` in the ETCD YAML file, make sure that the `volumeMounts` for `etcd-data` is updated as well, with the `mountPath` pointing to `/var/lib/etcd-from-backup` (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

---

# ETCD - Commands
```bash
etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set

etcdctl snapshot save
etcdctl endpoint health
etcdctl get
etcdctl put

kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / \
  --prefix --keys-only --limit=10 / \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --cert /etc/kubernetes/pki/etcd/server.crt \
  --key /etc/kubernetes/pki/etcd/server.key"

kubectl -n kube-system describe pod etcd-controlplane | grep data-dir
ps -ef | grep --color=auto etcd
```

---

# Backup 
```bash
kubectl describe pod -n kube-system etcd-controlplane
kubectl describe pod -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls
#  --advertise-client-urls   =>   --endpoints
kubectl describe pod -n kube-system etcd-controlplane  | grep pki
#  --trusted-ca-file         =>   --cacert
#  --cert-file               =>   --cert
#  --key-file                =>   --key

ETCDCTL_API=3 etcdctl \
  --endpoints=https://192.168.139.36:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/cluster1.db
```

---

# Check the members of the cluster:
```bash
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
   member list
```

---

# TLS Security
```bash
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text
ls -l /etc/kubernetes/pki/etcd/server* | grep .crt

crictl ps -a | grep kube-apiserver # like docker ps -a but in container.d
crictl logs --tail=2 <container-id>

nano /etc/kubernetes/manifests/kube-apiserver.yaml
nano /etc/kubernetes/manifests/etcd.yaml
```
```bash
cat saeed.csr | base64 -w 0
```
```yaml
---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: saeed
spec:
  groups:
  - system:authenticated
  request: <Paste the base64 encoded value of the CSR file>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
```
```bash
kubectl apply -f saeed-csr.yaml
kubectl get csr
kubectl certificate approve saeed

kubectl get csr <csr-name> -o yaml
# Reject that request
kubectl certificate deny <csr-name>
kubectl delete csr <csr-name>
```
---

# kube config
```bash
ls -a $HOME/.kube/
kubectl config view

kubectl config --kubeconfig=/root/my-kube-config use-context research
kubectl config --kubeconfig=/root/my-kube-config current-context
```

---

# Troubleshooting and Fixing `admin.kubeconfig`

A kubeconfig file named `admin.kubeconfig` is located in `/root/CKA`. There is an issue with the configuration. Follow these steps to diagnose and fix it.

## **1️⃣ Check the Configuration File**
Inspect the kubeconfig file:
```sh
cat /root/CKA/admin.kubeconfig
```
Or view it in a readable format:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig
```

## **2️⃣ Verify Contexts and Clusters**
Check available contexts and the current context:
```sh
kubectl config get-contexts --kubeconfig=/root/CKA/admin.kubeconfig
kubectl config current-context --kubeconfig=/root/CKA/admin.kubeconfig
```
If the current context is missing or incorrect, set it:
```sh
kubectl config use-context <correct-context-name> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **3️⃣ Check API Server Connectivity**
Ensure the API server is reachable:
```sh
kubectl cluster-info --kubeconfig=/root/CKA/admin.kubeconfig
```
If the connection fails, check the API server address:
```sh
grep "server:" /root/CKA/admin.kubeconfig
```
Update it if necessary:
```sh
kubectl config set-cluster <cluster-name> --server=https://<control-plane-ip>:6443 --kubeconfig=/root/CKA/admin.kubeconfig
```

## **4️⃣ Validate Certificates**
Check if certificate files exist:
```sh
grep "certificate-authority" /root/CKA/admin.kubeconfig
grep "client-certificate" /root/CKA/admin.kubeconfig
grep "client-key" /root/CKA/admin.kubeconfig
```
If any are missing, update them:
```sh
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/pki/admin.crt \
  --client-key=/etc/kubernetes/pki/admin.key \
  --kubeconfig=/root/CKA/admin.kubeconfig
```

## **5️⃣ Verify User Authentication**
Check if the token is valid:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig -o jsonpath='{.users[*].user.token}'
```
If missing or incorrect, set a new token:
```sh
kubectl config set-credentials admin --token=<correct-token> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **6️⃣ Test Kubernetes Access**
Run the following command to verify that everything works:
```sh
kubectl get nodes --kubeconfig=/root/CKA/admin.kubeconfig
```
If the command succeeds, the issue is resolved! 🚀


# RBAC (Role-based Access Control)
```bash
kubectl describe pod kube-apiserver-controlplane -n kube-system
# looking for   =>   --authorization-mode=

kubectl get roles
kubectl get roles --all-namespaces
kubectl get roles -A
kubectl describe role <role-name> -n kube-system
kubectl describe role kube-proxy -n kube-system

kubectl get rolebinding
kubectl get rolebinding --all-namespaces
kubectl get rolebinding -A
kubectl describe rolebinding <rolebinding-name> -n kube-system
kubectl describe rolebinding kube-proxy -n kube-system

kubectl get pods --as dev-user
kubectl auth can-i get pods
kubectl auth can-i get pods --as dev-user

kubectl edit role <role-name> -n <namespace-name>
```
## Create role andd rolebinding
```bash
kubectl create role <role-name> --namespace=default --verb=list,create,delete --resource=pods
kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods

kubectl create rolebinding <rolebinding-name> --namespace=default --role=developer --user=dev-user
kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
```
#### Or use this yaml file
```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

---

# Cluster Roles
```bash
kubectl get clusterroles --no-headers  | wc -l
kubectl get clusterroles --no-headers  -o json | jq '.items | length'
kubectl get clusterrolebindings --no-headers  | wc -l
kubectl get clusterrolebindings --no-headers  -o json | jq '.items | length'
kubectl describe clusterrolebinding cluster-admin
kubectl describe clusterrole cluster-admin
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```

---

# Service Accounts
### Service Accounts used by machine.
```bash
kubectl get sa
kubectl get serviceaccounts
kubectl get serviceaccount -n kube-system
kubectl describe serviceaccount <serviceaccount-name>
kubectl describe serviceaccount default

kubectl create serviceaccount <serviceaccount-name>
kubectl create serviceaccount dashboard-sa
kubectl create token dashboard-sa
```
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-dashboard
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
```
```bash
kubectl apply -f <FILE-NAME>.yaml
```
##### Or
```bash
kubectl set serviceaccount deploy/<deploy-name> <serviceaccount-name>
kubectl set serviceaccount deploy/web-dashboard dashboard-sa
```

---

# Image Security
```bash
kubectl create secret --help
kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
```

---

# Security Contexts
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  securityContext:
    runAsUser: 1010
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
```
### nano multi-pod.yaml 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]
```
### run as root 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]
```

---

# Network Policies
## Ingress → Traffic entering a Pod or network.
- Definition: Ingress refers to requests that enter a Pod or network from outside.
- In Kubernetes We have two types of Ingress controls:
  - `NetworkPolicy`: Restricts access between Pods and the network.
  - `Ingress Controller`: Controls access to services from outside the cluster (such as `nginx-ingress`)
## Egress → Traffic leaving a Pod or network.
- Definition: Egress refers to requests that leave a Pod or network.
  - In Kubernetes By default, Pods can connect to anywhere on the internet or internal network.
  - Egress Policy can restrict this access.

```bash
kubectl get netpol
kubectl get networkpolicy
kubectl describe networkpolicy
kubectl get svc -n kube-system
```
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```
```
- Policy Name: internal-policy
- Policy Type: Egress
- Egress Allow: payroll
- Payroll Port: 8080
- Egress Allow: mysql
- MySQL Port: 3306
```

---

# Persistent Volume (PV) & Persistent Volume Claim (PVC)
- `PV`: Persistent volume configured by the administrator.
  It is independent of the Pod and is connected to the PVC.
- `PVC`: A request from a Pod to use a PV.
- A Pod cannot directly connect to a PV; it must use a PVC.
### Access Modes:             
- RWO - ReadWriteOnce
- ROX - ReadOnlyMany
- RWX - ReadWriteMany
- RWOP - ReadWriteOncePod
### Reclaim Policy
- Retain -- manual reclamation
- Recycle -- basic scrub (rm -rf /thevolume/*)
- Delete -- delete the volume

---

```bash
nano pod-vol.yaml
# This pod don't use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
```

---

```bash
nano pv-definition.yaml
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
```
```bash
kubectl create -f pv-definition.yaml
```
```bash
nano pvc-definition.yaml
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
```
```bash
kubectl create -f pvc-definition.yaml
```
```bash
nano pod-pv.yaml
# This pod use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
```
---
```bash
kubectl get pv
kubectl get pvc
kubectl delete pvc <pvc-name>
kubectl delete pvc claim-log-1
```

---

# Storage Class
```bash
kubectl get sc
kubectl describe sc local-storage
kubectl describe pvc local-pvc | grep -A3 Events
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
```
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

---

# Network

## CNI (Container Network Interface),

| The CNI binaries are located under `/opt/cni/bin` by default.|
| `ls /etc/cni/net.d/` | Identify the name of the plugin. |
| `cat /etc/cni/net.d/10-flannel.conflist` | Look at the `type` field |

```bash
kubectl exec <pod-name> -- ip route
```
---
What is the range of IP addresses configured for PODs on this cluster?
`kubectl logs <weave-pod-name> -n kube-system` and look for `ipalloc-range`.

What is the IP Range configured for the services within the cluster?
Inspect the setting on kube-api server by running on command
`cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range`

What type of proxy is the kube-proxy configured to use?
`kubectl logs <kube-proxy-pod-name> -n kube-system`

---

# Ingress Networking 
```bash
kubectl get ingress --all-namespaces

kubectl describe ingress --namespace <namespace-name>
kubectl describe ingress --namespace app-space

kubectl edit ingress --namespace <namespace-name>
kubectl edit ingress --namespace app-space
```
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  name: ingress-wear-watch
  namespace: app-space
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port: 
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port: 
              number: 8080
        path: /stream
        pathType: Prefix
```
---
```bash
kubectl get svc -n critical-space
# Use this command to know the service and port details.
```
```yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282
```

---

```bash
kubectl create configmap <configmap-name> --namespace <namespace-name>
kubectl create configmap ingress-nginx-controller --namespace ingress-nginx

kubectl create serviceaccount ingress-nginx --namespace ingress-nginx
kubectl create serviceaccount ingress-nginx-admission --namespace ingress-nginx
```
We need to look at the Deployment's `namespace`, `containerPort`, and Service's `name`, `nodePort`.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
```
---
### Annotations are a way to configure and customize the Ingress Controller.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port: 
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080
```
---
1. Enable Path Rewrite
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
```
#### This annotation causes the request path to be rewritten to the / path.
---
2. Enable WebSocket
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
```
#### This annotation enables WebSocket and enables long-term communication.
---
3. Add Rate Limiting (limiting requests)
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "10"
```
#### This annotation limits the maximum number of requests to 10 requests per second.
---
4. Enable HTTPS Redirect
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
```
#### This annotation redirects all HTTP requests to HTTPS.
---
5. Configuring Load Balancer based on client's primary IP
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/load-balance: "ip_hash"
```
#### This annotation ensures that each client's requests are always sent to a specific server.
---
### CORS (Cross-Origin Resource Sharing) to control access to resources from different domains.
1. Enabling CORS for all requests
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type"
```
2. Only allowing a specific domain
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://example.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type, X-Requested-With"
```
3. Restricting access methods and setting Max Age
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://myapp.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-credentials: "true"
    nginx.ingress.kubernetes.io/cors-max-age: "600"
```

---

# Gateway API
```bash
# 1. Install the Gateway API resources
kubectl kustomize "https://github.com/nginx/nginx-gateway-fabric/config/crd/gateway-api/standard?ref=v1.5.1" | kubectl apply -f -
# 2. Deploy the NGINX Gateway Fabric CRDs
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/crds.yaml
# 3. Deploy NGINX Gateway Fabric
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/nodeport/deploy.yaml
# 4. Verify the Deployment
kubectl get pods -n nginx-gateway
# 5. View the nginx-gateway service
kubectl get svc -n nginx-gateway nginx-gateway -o yaml
# 6. Update the nginx-gateway service to expose ports 30080 for HTTP and 30081 for HTTPS
kubectl patch svc nginx-gateway -n nginx-gateway --type='json' -p='[
  {"op": "replace", "path": "/spec/ports/0/nodePort", "value": 30080},
  {"op": "replace", "path": "/spec/ports/1/nodePort", "value": 30081}
]'
```
---
```bash
nano gateway.yaml
```
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: nginx-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      port: 80
      protocol: HTTP
      allowedRoutes: 
       namespaces: 
        from: All
```
```bash
kubectl apply -f gateway.yaml
kubectl get gateways -n nginx-gateway
```

---

# HTTPRoute
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: frontend-route
  namespace: default
spec:
  parentRefs:
  - name: nginx-gateway
    namespace: nginx-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: frontend-svc
      port: 80
```

---

# Application Failure
### Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
```
# Check:
- Service
  - match the name of service and DB_HOST in deployment
  - match the `endpoint` and `port` of service and pod
  ```bash
  kubectl get pods -o wide
  kubectl describe svc <service-name>
  kubectl edit svc <service-name>
  ```
- Labels & Selector
  - If labels and selectors are not the same, the resources will not be connected correctly!
  - `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
  - `selector` in `Service` must match `labels` in `Pods` to send them traffic.
  - `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
  - `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.
  ```bash
  kubectl describe svc <service-name> | grep -i selector
  kubectl describe pod <pod-name> | grep -i label
  ```
- Deployment
  - Check the `env`
  ```bash
  kubectl describe deploy <deploy-name>
  kubectl edit deploy <deploy-name>
  ```
- Pod
  - Check the `Environment`
  ```bash
  kubectl describe pod <pod-name>
  kubectl logs <pod-name>
  kubectl logs <pod-name> -f --previous
  kubectl edit pod <pod-name>
  kubectl replace --force -f /tmp/kubectl-edit.yaml
  ```

---

# Control Plane Failure
If the control plane components are deployed as services, check the states of the services.
```bash
# On master nodes
service kube-apiserver status
service kube-scheduler status
service kube-controller-manager status
# On worker nodes
service kubelet status
service kube-proxy status
```
```bash
kubectl get pods -n kube-system
kubectl describe pod -n kube-system <pod-name>
kubectl logs -n kube-system <pod-name>
# If any Control Plane Components encounter an error, you can fix them using the YAML files in this directory.
cd /etc/kubernetes/manifests

# if pod is pending
kubectl describe pod -n kube-system kube-scheduler-controlplane
nano /etc/kubernetes/manifests/kube-scheduler.yaml

# if scale up/down not working
kubectl describe pod -n kube-system kube-controller-manager-controlplane
kubectl logs -n kube-system kube-controller-manager-controlplane
nano /etc/kubernetes/manifests/kube-controller-manager.yaml
```

---

# Worker Node Failure
```bash
kubectl get nodes
kubectl describe node <node-name>

# Check the possible CPU, memory and disk space on the nodes.
top
htop
df -h

# Check container run time
systemctl status containerd

# Check kubelet
service kubelet status
service kubelet start
service kubelet restart
#or
systemctl status kubelet
systemctl start kubelet
systemctl daemon-reload
systemctl restart kubelet

journalctl -u kubelet -f

nano /var/lib/kubelet/config.yaml
nano /etc/kubernetes/kubelet.conf

# Check the Kubelet Certificates
openssl x509 -in /var/lib/kubelet/worker-1.crt -text
```
# Check kube-proxy
```bash
service kube-proxy status
sudo journalctl -u kube-proxy
```
```bash
# kube-proxy pod
kubectl get pods -n kube-system | grep kube-proxy
kubectl -n kube-system logs <name_of_the_kube_proxy_pod>
kubectl -n kube-system describe configmap kube-proxy
```
```bash
kubectl -n kube-system edit ds kube-proxy
# Correct this path to /var/lib/kube-proxy/config.conf as per the ConfigMap and recreate the kube-proxy pod.
cat /var/lib/kube-proxy/config.conf
```
```yaml
spec:
    containers:
    - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
```
```bash
kubectl get pods -n kube-system | grep kube-proxy
```

---

# JSON PATH
```bash
kubectl get nodes -o json > /opt/outputs/nodes.json

kubectl get node node01 -o json > /opt/outputs/node01.json

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

kubectl config view --kubeconfig=my-kube-config  -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt

kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
```
