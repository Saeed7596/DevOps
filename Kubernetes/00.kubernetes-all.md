# Install
```bash
# Install virtual box
sudo apt update
sudo apt install virtualbox
VBoxManage --version
```
# How to **Enable Nested VT-x/AMD-V** On Virtualbox
```sh
# Command Prompt (CMD) Run as administrator
cd C:\Program Files\Oracle\VirtualBox
# Command:
VBoxManage modifyvm "YourVirtualMachineName" --nested-hw-virt on

# Example:
VBoxManage modifyvm "master01" --nested-hw-virt on
```
# Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
```bash
kubectl version --client
```
```bash
# set up autocomplete in bash into the current shell, bash-completion package should be installed first.
# add autocomplete permanently to your bash shell.
echo "source <(kubectl completion bash)" >> ~/.bashrc
```
```vim
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
```
```bash
source ~/.bashrc
```
```bash
cat ~/.bashrc
```
# Install [minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download)
```bash
minikube start
minikube start --driver=virtualbox
minikube status
minikube config get driver
```

---

# Install on windows
##### Download [`kubectl.exe`](https://kubernetes.io/releases/download/#binaries) 
- Create a folder in Drive `C:\kube`
- Move the the `kubectl.exe` to this folder
- Search for the `Edit the system environment variables`
- Environment Variables
- Click on Path > New > `C:\kube`
- Set this path for User variables and System variables
```bash
kubectl version --client
```
##### Download [`minikube.exe`](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download)
- Add the `minikube.exe` binary to your `PATH`.
- Make sure to run PowerShell as Administrator.
```bash
$oldPath = [Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine)
if ($oldPath.Split(';') -inotcontains 'C:\minikube'){
  [Environment]::SetEnvironmentVariable('Path', $('{0};C:\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine)
}
```
```bash
minikube start
minikube status
```
Minikube multi node
```bash
minikube start --nodes 3 -p <name> --forece
minikube node add --worker -p <name>
```

---

# [Deploy a Kubernetes Cluster using Kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
## "I ensure that all nodes are on the same network and can communicate with each other, assigning each one a unique and distinct hostname."
### These steps have to be performed on `all nodes`.
`set net.bridge.bridge-nf-call-iptables` to `1`: 
```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter
```
```bash
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sudo sysctl --system
```
---
# Check required ports
```bash
nc 127.0.0.1 6443 -v
```
---
# Disable Swap
Kubernetes need to be disabled SWAPs, because using Swap can make Kuberinets not make the right scheduling decisions.
- To disable swap, `sudo swapoff -a` can be used to disable swapping temporarily. To make this change persistent across reboots, make sure swap is disabled in config files like `/etc/fstab,` `systemd.swap,` depending how it was configured on your system.
- Comment the swap line in `/etc/fstab`
  ```bash
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  ```
  Make sure!
  ```bash
  sudo cat /etc/fstab
  ```

---

# [Installing a container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

| Runtime       | Path to Unix domain socket                      |
|--------------|-----------------------------------------------|
| containerd   | unix:///var/run/containerd/containerd.sock   |
| CRI-O        | unix:///var/run/crio/crio.sock              |
| Docker Engine (using cri-dockerd) | unix:///var/run/cri-dockerd.sock |


```bash
sudo apt install containerd -y

sudo mkdir -p /etc/containerd
containerd config default
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml

sudo systemctl restart containerd
```
---
Install `kubeadm`, `kubectl` and `kubelet` on all nodes:
```bash
sudo apt-get update

sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
- Note:
In releases older than Debian 12 and Ubuntu 22.04, directory `/etc/apt/keyrings` does not exist by default, and it should be created before the curl command.
    ```bash
    mkdir /etc/apt/keyrings
    ```
```bash
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
```
Install Latest Versions
```bash
sudo apt install -y kubelet kubeadm kubectl
```
Or Install Specific Versions
```bash
# To see the new version labels
sudo apt-cache madison kubeadm

sudo apt-get install -y kubelet=1.32.0-1.1 kubeadm=1.32.0-1.1 kubectl=1.32.0-1.1
```
Prevent Automatic Updates
```bash
sudo apt-mark hold kubelet kubeadm kubectl
```
Pull Images:
```bash
sudo kubeadm config images pull
```
---
# [Creating a cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)
### On control-plane (master) node:
To initialize the control-plane (master) node run:
```bash
kubeadm init <args>
```
| Argument | Description |
|:---|:---|
| `--control-plane-endpoint` | The shared endpoint (DNS name, Virtual IP, or Load Balancer address) that all control plane nodes and external clients will use to access the Kubernetes API server. Critical for High Availability (HA) clusters. |
| `--apiserver-advertise-address` | The IP address the API server will advertise to other nodes. Typically set to the private IP address of the machine where kubeadm is being run. |
| `--pod-network-cidr` | The CIDR range from which Pod IPs will be allocated. Must match your CNI plugin configuration (e.g., `10.244.0.0/16` for Flannel, `192.168.0.0/16` for Calico). |
| `--service-cidr` | The CIDR range for Kubernetes service cluster IPs. Default is `10.96.0.0/12` unless customized. |
| `--apiserver-cert-extra-sans` | Extra Subject Alternative Names (SANs) to be added to the API server certificate. Useful when the API server is accessed via additional IPs or DNS names (like a Load Balancer address). |
| `--upload-certs` | Uploads control plane certificates to a kubeadm-certs secret inside the cluster, enabling secure joining of additional control plane nodes without manual certificate copying. |

```bash
IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
Default
```bash
kubeadm init --apiserver-advertise-address=192.168.75.137 --pod-network-cidr=10.244.0.0/16
```
Or
```bash
ifconfig eth0
# Lookup for inet = 192.168.75.137
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.168.75.137 --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
```
if cluster dosen't work, reset cluster with kubeadm:
```bash
sudo kubeadm reset --force
```
To start using your cluster, you need to run the following as a regular user:
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Alternatively, if you are the root user, you can run:
```bash
export KUBECONFIG=/etc/kubernetes/admin.conf
```
```bash
kubeadm token create --print-join-command
```
### On worker node
```
kubeadm join 192.168.75.137:6443 --token sn2brb.c6fsi25tvscuw0ol \
        --discovery-token-ca-cert-hash sha256:6e4c42b70e8d743019f16541d960c417fe031bae8477a959649cb5459e5f6b84
```
---
# Install a Network Plugin `Flannel`
### On master node
```bash
curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
nano kube-flannel.yml
```
We are using a custom PodCIDR (`172.17.0.0/16`) instead of the default `10.244.0.0/16` when bootstrapping the Kubernetes cluster. However, the `Flannel` manifest by default is configured to use `10.244.0.0/16` as its network, which does not align with the specified PodCIDR. To resolve this, we need to update the `Network` field in the `kube-flannel-cfg` ConfigMap to match the custom PodCIDR defined during cluster initialization.
```yaml
net-conf.json: |
    {
      "Network": "10.244.0.0/16", # Update this to match the custom PodCIDR
      "Backend": {
        "Type": "vxlan"
      }
```
Locate the args section within the kube-flannel container definition. It should look like this: 
```yaml
  args:
  - --ip-masq
  - --kube-subnet-mgr
  - --iface=eth0
```
Add the additional argument `- --iface=eth0` to the existing list of arguments.
```bash
kubectl apply -f kube-flannel.yml
```
# Verify
```bash
kubectl get nodes
```

---

# General Commands
```bash
kubectl get all -A
kubectl get all --all-namespaces
kubectl get all -n <namespace-name>
kubectl <command> <resource-type> --help
```
`resource-types`: [pod, node, namespace, deployment, rs(replicaSets), daemonset, service, role, rolebinding, pv, pvc, secret, configmap, ingress, job, cronjob, statefulset]
```bash
kubectl get pod --help
kubectl create pod --help
```
```bash
kubectl describe <resource-types> <resource-name> -n <namespace>
```

---

The `kubectl api-resources` command shows you a list of all the resources supported by Kubernetes.

### Example Output of `kubectl api-resources`:
```bash
NAME          SHORTNAMES   APIVERSION   NAMESPACED   KIND
pods          po           v1           true         Pod
services      svc          v1           true         Service
deployments   deploy       apps/v1      true         Deployment
configmaps    cm           v1           true         ConfigMap
secrets                    v1           true         Secret
namespaces    ns           v1           false        Namespace
nodes         no           v1           false        Node
```
- NAMESPACED = false => cluster-scoped

---

# Cluster
```bash
kubectl cluster-info
kubectl config view
kubectl config get-clusters # show all cluster
kubectl config use-context cluster1
# Switched to context "cluster1".
```

---

# Node
```bash
kubectl get nodes
kubectl get nodes -o wide
kubectl describe node <node-name>
kubectl get node <node-name> --show-labels
kubectl label node <node-name> key=value
kubectl describe node <node-name> | grep -i taints
```

---

# Pod
[kubectl run](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_run/)
```bash
kubectl get pods
watch kubectl get pods
kubectl get pods -o wide
kubectl get pods --all-namespaces
kubectl get pods --namespace kube-system
kubectl get pod <pod-name> -o yaml > my-new-pod.yaml
kubectl get pod <pod-name> -o yaml >&nbsp;my-new-pod.yaml

kubectl get pod --show-labels

kubectl run <pod-name> --image=<image_name>
kubectl run nginx --image=nginx --dry-run=client -o yaml
kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-definition.yaml
kubectl run nginx --image=nginx --port=8080
kubectl run redis -l tier=db --image=redis:alpine # -l means Label

kubectl describe pod <pod-name>
kubectl describe pod <pod-name> | grep -A5 State

kubectl create -f nginx-definition.yaml

kubectl edit pod <pod-name>

kubectl replace -f my-new-pod.yaml --force 
# This command will delete the existing one first and recreate a new one from the YAML file. 

kubectl delete pod <pod-name>
kubectl delete po <pod-name>

kubectl exec <pod-name> -c <container-name> -it -- bash
kubectl exec ubuntu-sleeper -- whoami
kubectl -n elastic-stack exec -it app -- cat /log/app.log
```
```bash
nano multi-container-pod.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - name: lemon
    image: busybox
    command:
      - sleep
      - "1000"
  - name: gold
    image: redis
```

---

| Container Restart Policy  |         |
|------------------|------------------|
| `Always`    | Automatically restarts the container after any termination. |
| `OnFailure` | Only restarts the container if it exits with an error (non-zero exit status). |
| `Never`     | Does not automatically restart the terminated container. |
- **For regular Pods**: The default value is `Always`.
- **For Jobs or CronJobs**: The default value is `OnFailure`.

---

# Static Pods
#### Pods that run directly on a node without being managed by the API server.
#### Static Pods directory `/etc/kubernetes/manifests/`
```bash
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
```

---

**kubectl [command] [TYPE] [NAME] -o **

Here are some of the commonly used formats:

1. `-o json` = Output a JSON formatted API object.
2. `-o name` = Print only the resource name and nothing else.
3. `-o wide` = Output in the plain-text format with any additional information.
4. `-o yaml` = Output a YAML formatted API object.

---

When writing a manifest in Kubernetes, you must include these 4 fields:

- **apiVersion**: The API version used by this object.
- **kind**: The type of object (Pod, Deployment, Service, etc.).
- **metadata**: Descriptive information such as name and labels.
- **spec**: The desired configuration and behavior for this object.

---

# [Limit Range](https://kubernetes.io/docs/concepts/policy/limit-range/)
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example-no-conflict-with-limitrange-cpu
spec:
  containers:
  - name: demo
    image: registry.k8s.io/pause:3.8
    resources:
      requests:
        cpu: 700m
      limits:
        cpu: 700m
```

---

# Types of Workloads

- **Deployment**: Running stateless applications.
- **ReplicaSet**: Maintaining a specified number of Pods.
- **StatefulSet**: Running stateful applications.
- **DaemonSet**: Running a Pod on all Nodes.
- **Job**: Running temporary tasks.
- **CronJob**: Running scheduled tasks.

---

# Deployments
```bash
kubectl get rs
kubectl get replicaset
kubectl get deploy
kubectl get deployment
kubectl describe deployment <deployment-name>
kubectl edit deployment <deployment-name>
kubectl explain deployment
```

---

```bash
nano nginx-deployment-definition.yaml
kubectl create -f nginx-deployment-definition.yaml
```
In the Deployment, the `spec` section will be for `Pods`.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx-frontend
  template:
    metadata:
      labels:
        name: nginx-frontend
    spec:
      containers:
      - name: nginx-container
        image: nginx
```
```bash
kubectl create deployment <deploy-name> --image=<image-name>
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
kubectl create deployment nginx --image=nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

kubectl scale deployment <deploy-name> --replicas=4
```

---

### **🔹 Deployment Strategies: Blue/Green & Canary**

#### **🚀 Blue/Green Deployment**
Blue/Green is a deployment strategy that reduces downtime and risk by running two identical environments:
- **Blue**: The current, active version.
- **Green**: The new version that is tested before switching traffic.

📌 **Steps:**
1. Deploy the **Green** version alongside the **Blue** version.
2. Test the **Green** version.
3. Switch traffic from **Blue** to **Green** (typically by updating a Load Balancer).
4. If issues arise, revert back to **Blue**.

✅ **Pros:**
- Minimal downtime.
- Easy rollback.
- Ensures new version is fully tested before release.

❌ **Cons:**
- Requires double the resources.
- More complex infrastructure management.

#### **🛠 Canary Deployment**
Canary deployment is a progressive rollout strategy where the new version is deployed to a small subset of users before a full release.

📌 **Steps:**
1. Deploy the new version to a small percentage (e.g., 5%) of traffic/users.
2. Monitor performance and logs.
3. Gradually increase traffic if no issues are detected.
4. Fully roll out the new version if everything works well.

✅ **Pros:**
- Reduces risk of full failure.
- Allows real-world testing with minimal impact.
- Can be automated with monitoring tools.

❌ **Cons:**
- More complex traffic routing.
- Rollbacks require additional handling.

---

### **🔹 Conclusion**  
If you need to **filter or group resources**, use **Labels**.  
If you need to **store additional descriptive metadata**, use **Annotations**. 🚀  
For **deployment strategies**, use **Blue/Green** for instant switches and **Canary** for gradual rollouts.

---

# Service
```bash
kubectl get svc
kubectl get service
kubectl describe service
```
### Types of **Service** in Kubernetes

| Service Type    | Description | Common Usage |
|----------------|--------------------------------------------------|------------------------------|
| **ClusterIP**  | The default type, assigns an internal IP within the cluster, accessible only inside the cluster. | Internal communication between services within the cluster |
| **NodePort**   | Opens a port on each Node, allowing access from outside the cluster. | External access without a Load Balancer |
| **LoadBalancer** | Creates a public IP and a Load Balancer from the cloud provider. | Load distribution and direct external access |
| **ExternalName** | Maps requests to an external domain name (e.g., `example.com`). | Redirecting requests to external services |

---

# CoreDNS
When you create a Service in Kubernetes, a DNS record is automatically created by CoreDNS, which is the default DNS server used in most Kubernetes clusters.

## 🔍 Here's how it works:
Kubernetes runs CoreDNS as a cluster add-on (a set of Pods in the kube-system namespace).

When a new Service is created, Kubernetes updates its internal API and DNS records.

CoreDNS monitors the Kubernetes API for changes and automatically creates corresponding DNS entries.

## 📌 DNS Naming Format:
For a Service named my-service in namespace my-namespace, the DNS name is:

```pgsql
my-service.my-namespace.svc.cluster.local
```
So in short:

✅ CoreDNS is responsible for automatically creating the DNS records for Services in Kubernetes.

---

# ClusterIP
In Kubernetes, when we want to create a service that should not be exposed externally (like Redis), we use the `ClusterIP` type service (which is the default type if not specified). 
> ClusterIP exposes the service only internally within the cluster and not to the outside world (unlike NodePort or LoadBalancer).

---

## Example YAML for a ClusterIP Service
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-internal
  namespace: dev
spec:
  type: ClusterIP
  ports:
    - targetPort: 80
      port: 8080
  selector:
    app: nginx
```
## Notes on DNS in Kubernetes
* If it were the same inside the namespace, just the service name would be enough 
  - `http://nginx-internal:8080`
* To access a service from a different namespace, we should use the fully qualified domain name (FQDN) of the service.

Full Qualified Domain Name (FQDN) format:
```
<service-name>.<namespace>.svc.cluster.local
```
Example:
```pgsql
mysql.connect("db-service.dev.svc.cluster.local")
```

### Testing Internal Access to the Service using DNS
We can run a temporary pod (debugger) to test internal requests inside the cluster:
```bash
# Run a debug pod in the default namespace
kubectl -n default run debugger --image=alpine:3.11.3 --command -- sleep infinity

# Install curl inside the pod
kubectl -n default exec debugger -- apk add curl

# Access the nginx-internal service using DNS
kubectl -n default exec debugger -- curl http://nginx-internal.dev.svc.cluster.local:8080
```

---

# Headless Service
A Headless Service is a service without a ClusterIP. It is used mainly when you want to directly access individual Pods instead of load-balancing between them.

You can create a Headless Service by setting `clusterIP: None`.

Example:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: dev
spec:
  clusterIP: None     # Headless Service
  selector:
    app: mongo
  ports:
  - port: 27017
```
StatefulSet Example:
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
  namespace: dev
spec:
  serviceName: mongo  # important → Headless Service Name
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
        - name: mongo
          image: mongo
          ports:
            - containerPort: 27017
```

### Use Case:
Headless Service is very useful for Stateful applications like MongoDB ReplicaSet, Cassandra, etc., where each Pod needs to be addressable individually.

DNS Records created for Headless Service:
```
mongo-0.mongo.dev.svc.cluster.local
mongo-1.mongo.dev.svc.cluster.local
mongo-2.mongo.dev.svc.cluster.local
```

---

# MongoDB ReplicaSet Architecture

![MongoDB ReplicaSet Architecture](https://www.mongodb.com/docs/manual/_images/replica-set-architecture.bakedsvg.svg)

### Components:
- Primary: Handles all writes
- Secondary: Syncs data from Primary (Read-only)
- Arbiter (Optional): Participates in election but doesn't store data

---

# Example MongoDB Connection String
```python
mongodb://mongo-0.mongo.dev.svc.cluster.local:27017,mongo-1.mongo.dev.svc.cluster.local:27017,mongo-2.mongo.dev.svc.cluster.local:27017/?replicaSet=rs0
```
This connection string ensures automatic detection of Primary and failover handling.

---

### Differences between **nodePort, port, and targetPort** in Kubernetes

| Parameter      | Description |
|---------------|--------------------------------------------------|
| **nodePort**  | The port opened on each Node, allowing external access (ranges between 30000-32767). |
| **port**      | The port defined in the Service that receives traffic. |
| **targetPort** | The port running inside the Pod that processes incoming requests. |

Example YAML configuration:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: example-service
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30080
```

In this example:
- Requests arrive at **port 80** on the Service.
- They are forwarded to **port 8080** inside the corresponding Pod.
- The Service is externally accessible on **port 30080** on each Node.

---

```bash
nano service-definition.yaml
kubectl apply -f service-definition.yaml
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort
```
```bash
kubectl run nginx --image=nginx --port=80 --expose # create a service for this pod

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

kubectl run redis -l tier=db --image=redis:alpine # -l means Label
kubectl expose pod redis --port=6379 --target-port=6379 --name=redis-service --type=ClusterIP
```

```
# if messaging is deployment
kubectl expose deployment messaging --name=messaging-service --port=6379 --target-port=6379 --type=ClusterIP
# if messaging is pod
kubectl expose pod messaging --name=messaging-service --port=6379 --target-port=6379 --type=ClusterIP
```

---

---

### Communication Between Pods in Different Namespaces
- By default, Pods in different namespaces cannot directly discover each other. To establish communication, you must use the **full DNS name** of the service.

---

### Exposing Applications

There are several ways to expose applications in Kubernetes:

1. **NodePort**:
   - Exposes the service on a static port on each node’s IP.
   - Accessible from outside the cluster using `<NodeIP>:<NodePort>`.

2. **LoadBalancer**:
   - Provisions an external load balancer (e.g., in cloud environments) to expose the service.
   - Automatically assigns an external IP.

3. **Ingress**:
   - Provides HTTP/HTTPS routing to services based on hostnames or paths.
   - Requires an Ingress controller to be installed in the cluster.

---

### How to Check if a Service is Pod-to-Pod
To determine if a service is Pod-to-Pod, you can check the following:

1. **Service Type**:
   - The service type should be `ClusterIP`.
   - Command:  
     ```bash
     kubectl get service <service-name> -n <namespace>
     ```

2. **Service Selector**:
   - The service selector should target internal Pods.

3. **Service Endpoints**:
   - The service endpoints should include the internal IPs of the Pods.
   - Command:  
     ```bash
     kubectl get endpoints <service-name> -n <namespace>
     ```

4. **DNS Testing from Inside a Pod**:
   - Access one of the Pods:  
     ```bash
     kubectl exec -it <pod-name> -n <namespace> -- /bin/bash
     ```
   - Send a request to the service:  
     ```bash
     curl http://<service-name>.<namespace>.svc.cluster.local
     ```
   - If the service is Pod-to-Pod, you should receive a response.

5. **Pod Logs**:
   - Check the logs of the Pods to verify internal communication:  
     ```bash
     kubectl logs <pod-name> -n <namespace>
     ```

---

### Multi-Port Service
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-port-deployment
  labels:
    app: multi-port-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: multi-port-app
  template:
    metadata:
      labels:
        app: multi-port-app
    spec:
      containers:
      - name: multi-port-container
        image: nginx:stable
        ports:
        - containerPort: 80   # HTTP
        - containerPort: 443  # HTTPS
        - containerPort: 8080 # Custom Port
---
apiVersion: v1
kind: Service
metadata:
  name: multi-port-service
spec:
  selector:
    app: multi-port-app
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443
  - name: custom-port
    protocol: TCP
    port: 8080
    targetPort: 8080
```

---

## LoadBalancer
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-loadbalancer
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80 # The input port of the service (LoadBalancer).
    targetPort: 80 # The port inside the Nginx container.
```
```bash
kubectl get services
```
In the browser or similar tools, enter the following address: `http://<EXTERNAL-IP>`

---

## ExternalName in Kubernetes
**ExternalName** is a type of Service in Kubernetes that, instead of connecting to Pods within the cluster, is used to redirect requests to an external service (outside the cluster). In simple terms, when clients access the service, Kubernetes redirects their requests to another domain name (DNS) that exists outside the cluster.

---

## When is ExternalName Useful?
- When an application inside the cluster needs to connect to a database or services outside the cluster, such as:
  - Connecting to a MySQL database running on another server.
  - Using an external API like Google Maps or any other API.
- When you want to simplify access to external services within the cluster.

### Example YAML for External Service:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-external-service
spec:
  type: ExternalName
  externalName: mysql.external.com
```
### Example YAML for Internal Service:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: ExternalName
  externalName: my-service1.prod.svc.cluster.local
```
### Where is ExternalName Good and Where is it Not?
- Good For:
  - When you have a fixed external service with a DNS address.
  - When you want to simplify the naming of external services.

- Limitations:
  - ExternalName only works with services that have a DNS address. If the external service only has an IP, you cannot use ExternalName.
  - ExternalName does not perform load balancing. If the external service has multiple servers, Kubernetes will only connect to the specified DNS address.

#### Summary
ExternalName is a special type of Service in Kubernetes used to redirect requests to an external service (outside the cluster).
This type of service uses DNS to redirect requests.
It is mainly used for connecting to external services or simplifying access to external services within the cluster.

---

## External IPs in Kubernetes
When you have a service in Kubernetes that you want to access from outside the cluster (i.e., from an external network), you can use **External IPs**. In simple terms, External IP allows you to assign a specific IP to your service, and requests to that IP will be routed to the service.

## When is it Useful?
- When you have an external server or device on the network that needs to connect directly to a service inside Kubernetes.
- When you want to restrict access to your service to specific predefined IPs.
- When you don’t need a LoadBalancer or don’t want to use one.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  externalIPs:
  - 192.168.1.100
```
### How to Access the Service?
* If you are on a network that has access to the IP 192.168.1.100, you can send a request using a browser or curl:`http://192.168.1.100`
* If everything is set up correctly, the default Nginx page will be displayed.

### Where is External IP Good and Where is it Not?
Good For:
- When you have a specific IP that needs to connect directly to a Kubernetes service.
- When you have a pre-defined network infrastructure with specific IPs.

Limitations:
- External IP is usually not suitable for cloud environments. In the cloud, it’s better to use LoadBalancer or Ingress.
- Managing External IPs can be challenging in complex environments because Kubernetes does not have full control over those IPs.

#### Summary
- External IP allows you to expose a service inside Kubernetes through a specific IP from outside the cluster.
- This method is mainly used for internal networks or specific infrastructures.
- For cloud or public environments, it’s better to use LoadBalancer or Ingress.

---

# Namespace
#### Provides a way to logically isolate Kubernetes resources within a cluster.
```bash
kubectl create namespace devops
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: devops
spec:
  containers:
  - name: nginx-container
    image: nginx
```
---
# Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
kubectl config view --minify | grep namespace
kubectl config set-context --current --namespace=default
```

---

### 📌 Important Note: Namespace Scope of Kubernetes Resources

| Resource                | Namespace-scoped? |
|-------------------------|-------------------|
| Node                   | ❌ No              |
| PersistentVolume (PV)  | ❌ No              |
| PersistentVolumeClaim  | ✅ Yes             |
| Pod                    | ✅ Yes             |

* `Node` cannot belong to a specific Namespace.
* `PersistentVolume (PV)` is created independently of the Namespace.
  - But: `PersistentVolumeClaim` **(PVC)** is created within a namespace and is attached to the **PV**.

---

# Kubernetes DNS

Kubernetes DNS helps services and Pods communicate with each other without needing to use IP addresses.

## How does DNS work in Kubernetes?

When you create a Service in Kubernetes, a DNS record is automatically created for it. This record allows other services to easily access it using the service name. For example:

Assume you have a service named `my-service` in a namespace called `my-namespace`.  
The DNS record for this service will look like this:  
`my-service.my-namespace.svc.cluster.local`  

- `my-service`: The name of the service.  
- `my-namespace`: The namespace of the service.  
- `svc`: Indicates that this is a Service.  
- `cluster.local`: The default domain for your cluster.  

## DNS Structure for Service Communication

1. **Within the same namespace**: If services are in the same namespace, you can use just the service name:  
   `my-service`  

2. **Across different namespaces**: If services are in different namespaces, you must use the full format:  
   `my-service.my-namespace.svc.cluster.local`  

## CoreDNS

DNS in Kubernetes is managed by an add-on called **CoreDNS**. This service is installed by default in most Kubernetes distributions and is responsible for maintaining DNS records for services and Pods.

---

## Examples of DNS in Action

1. **Communication between two services in the same namespace**:  
   Assume you have two services, `frontend` and `backend`, both in the `default` namespace. In this case, `frontend` can connect to `backend` using the address:  
   `backend`  

2. **Communication between two services in different namespaces**:  
   If `frontend` is in the `frontend-namespace` namespace and `backend` is in the `backend-namespace` namespace, you must use the full address:  
   `backend.backend-namespace.svc.cluster.local`  

---

## Practical Tips
- Namespaces are the most important objects in Kubernetes.
- Namespaces directly impact **Network Policies**, **Resource Quotas**, and **RBAC** (access permissions). Therefore, using namespaces is essential for better cluster management.  
- For services that always need a fixed address, Kubernetes DNS solves this problem, so you no longer need to worry about changing IPs.
- Each namespace (ns) is an isolated environment, but the **kubelet** has access to them, which means the kubelet operates independently of these namespaces. While **storage** is not scoped within namespaces, almost everything else falls under the scope of namespaces.

---

# Labels and Selectors
#### Used to categorize and filter Kubernetes resources.
#### **Labels are in the form of `key: value`.**
---
#### If labels and selectors are not the same, the resources will not be connected correctly!
- `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
- `selector` in `Service` must match `labels` in `Pods` to send them traffic.
- `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
- `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.

| **Use Case**                  | **Must Match**                                      |
|--------------------------------|----------------------------------------------------|
| Deployment → ReplicaSet → Pod  | `selector.matchLabels` in Deployment and `labels` in Pod |
| Service → Pod                 | `selector` in Service and `labels` in Pod         |
| NetworkPolicy → Pod           | `podSelector.matchLabels` in NetworkPolicy and `labels` in Pod |
| Ingress → Service             | `backend.service.name` in Ingress and `metadata.name` in Service |

```bash
kubectl get pod -l env=dev
kubectl get pod -l env=dev,bu=finance,tier=frontend
```

```bash
kubectl get pods --selector env=dev
kubectl get all --selector env=prod --no-headers | wc -l
kubectl get all --selector env=prod,bu=finance,tier=frontend
```
Define the labels in the `metadata` section.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app
  labels:
    app: frontend
    environment: production
spec:
  containers:
  - name: nginx
    image: nginx
```
**`selector` in Service and `labels` in Pod Must Match**
```bash
kubectl -n <namespace-name> describe svc <service-name> | grep -i selector
kubectl -n <namespace-name> describe pod <pod-name> | grep -i label
```
Adding Labels to Existing Resources.
```bash
kubectl label pod my-app version=v1
```
Delete Label
```bash
kubectl label pod my-app version-
```

---

# Another Example:Add commentMore actions
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app         # ①
spec:
  selector:
    matchLabels:
      app: my-app       # ②
  template:
    metadata:
      labels:
        app: my-app     # ③
    spec:
      containers:
        - name: app
          image: myapp:latest
```
| **Locate**                                 | **Label**         | **Use Case**                                       |
|--------------------------------------------|-------------------|----------------------------------------------------|
| `metadata.labels` at the top of Deployment | `app: my-app (①)` | To identify the Deployment itself, tools like Helm or kubectl use this for listing or categorizing. |
| `spec.selector.matchLabels`                | `app: my-app (②)`  | Determines which Pods belong to this Deployment (to control scale/update) |
| `spec.template.metadata.labels`            | `app: my-app (③)`  | This label allows the Pods that are created to connect to the Deployment. |

**Note: `spec.template.metadata.labels` and `spec.selector.matchLabels` must be the same so that the Deployment can correctly identify and manage the Pods it creates. Otherwise, the Deployment cannot track or control the Pods associated with it.**

### In Service
**Note: The `spec.selector` in a `Service` must match the `metadata.labels` on the `target Pods` (or on the `template.metadata.labels` in a `Deployment`) so that the Service can correctly route traffic to those Pods. Otherwise, the Service will not select any Pods, and traffic will have no destination.**

---

## **Annotations**

### **🔹 What are Annotations?**  
Annotations in Kubernetes allow you to attach arbitrary non-identifying metadata to objects. Unlike labels, annotations are not used for selection or filtering but serve for informational purposes such as automation, monitoring, and metadata storage.

---

### **Differences Between Labels and Annotations**  
| Feature           | Labels | Annotations |
|------------------|--------|------------|
| **Purpose**      | Identification & grouping of resources | Adding descriptive metadata |
| **Used in Selectors** | ✅ Yes | ❌ No |
| **Key/Value Length Limit** | Limited | Can be long |
| **Common Use Cases** | Filtering, grouping, enforcing policies | Storing descriptions, logs, hashes, or automation data |

---

### **Example of Annotations in a Pod**  
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  annotations:
    kubernetes.io/description: "This pod runs the main backend service."
    monitoring.example.com/logs: "enabled"
spec:
  containers:
  - name: nginx
    image: nginx
```
**Explanation:**  
- `kubernetes.io/description`: Provides an explanation of the pod's function.
- `monitoring.example.com/logs`: Enables logging for monitoring purposes.

---

### **Use Cases of Annotations**  
**Monitoring & Logging:** Used for **Prometheus**, **Fluentd**, and other monitoring tools.  
**CI/CD Management:** Stores information like **commit SHA**, **Docker image version**, etc.  
**Automation:** Allows adding extra metadata for resource control.  
**Storing Non-Filterable Metadata:** Holds data that is not used in **selectors**.

---

### **Conclusion**  
If you need to **filter or group resources**, use **Labels**.  
If you need to **store additional descriptive metadata**, use **Annotations**.

---

# Taints and Tolerations
```bash
kubectl describe node node01 | grep -i taints
kubectl taint nodes node01 spray=mortein:NoSchedule
```
### with this yaml file this pod assign to node01
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
```
### node/controlplane untainted
```bash
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
```

---

# Node Affinity
#### Specifies rules to influence where Pods are scheduled.
# Summary:

## Node Affinity
Node Affinity allows you to specify which nodes Pods should or should not run on. This feature is used for resource management, performance optimization, and compliance with specific rules.

### Two Main Types:
1. **RequiredDuringSchedulingIgnoredDuringExecution**: Strict rule.
   - If no node with the specified label is found, the Pod will not be scheduled.
   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: gpu-pod
   spec:
     affinity:
       nodeAffinity:
         requiredDuringSchedulingIgnoredDuringExecution:
           nodeSelectorTerms:
           - matchExpressions:
             - key: gpu
               operator: In
               values:
               - "true"
     containers:
     - name: gpu-container
       image: nginx
    ```

2. **PreferredDuringSchedulingIgnoredDuringExecution**: Preferred rule.
    - Kubernetes tries to schedule the Pod on nodes that meet the conditions, but if none are found, it will run on other nodes.
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: zone-pod
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - "us-east-1"
      containers:
      - name: zone-container
        image: nginx    
    ```
Using Node Affinity, you can more precisely manage Pod placement.

---

## Node Anti-Affinity
Node Anti-Affinity allows you to specify which nodes Pods should not run on. This feature is used to increase availability, manage resources, and prevent excessive traffic on a single node.

### Two Main Types:
1. **RequiredDuringSchedulingIgnoredDuringExecution**: Strict rule.
    - If all nodes meet the specified conditions, the Pod will not be scheduled.
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: anti-affinity-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: zone
                operator: NotIn
                values:
                - "us-east-1"
      containers:
      - name: nginx-container
        image: nginx
    ```
2. **PreferredDuringSchedulingIgnoredDuringExecution**: Preferred rule.
    - Kubernetes tries to schedule the Pod on nodes that do not meet the specified conditions, but if none are found, it will run on other nodes.
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: preferred-anti-affinity-pod
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: zone
                operator: NotIn
                values:
                - "us-west-1"
      containers:
      - name: nginx-container
        image: nginx
    ```
Using Node Anti-Affinity, you can more precisely manage Pod placement.

---

### Summary:
- `Node Affinity`: Allows you to control which nodes Pods should run on.
- `Node Anti-Affinity`: Allows you to control which nodes Pods should **    ** run on.
- Both features help you manage Pod placement more precisely, ensuring better resource utilization and availability.

---

```bash
kubectl label node <node-name> key=value
kubectl label node node01 color=blue
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
```


---

# Difference Between `label` and `taint` in Kubernetes

| Feature   | **Label** | **Taint** |
|-----------|----------|----------|
| **Purpose** | Used for **grouping and filtering** resources like `Pods`, `Nodes`, `PVCs`, `Services`, etc. | Used to **restrict scheduling** of pods on specific nodes. |
| **Applies To** | Can be applied to **Pods, Nodes, PVCs, Services, etc.** | Only applies to **Nodes**. |
| **Main Use Case** | - Organizing resources <br> - Filtering with **Selectors** <br> - Used in **Node Affinity** | - Preventing **undesired pods** from running on specific nodes <br> - Enforcing **special scheduling conditions** |
| **How Pods are Scheduled on Nodes?** | Pods are scheduled on nodes with specific labels using `nodeSelector` or `nodeAffinity`. | Only pods with matching `tolerations` can be scheduled on tainted nodes. |
| **Command to Apply** | ```sh kubectl label node node01 env=production ``` | ```sh kubectl taint node node01 key=value:Effect ``` |

**Label** is used to **identify and categorize nodes and pods**.  
**Taint** is used to **restrict pod scheduling on nodes**.

---

# DaemonSets
```bash
kubectl get ds
kubectl get daemonsets
kubectl get daemonsets -A
kubectl get daemonsets --all-namespaces
kubectl get ds -n kube-system
kubectl describe daemonset <daemonset-name> -n <namespace-name>
kubectl describe daemonset kube-proxy -n kube-system
```
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: registry.k8s.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
```

---

# Static Pods
```bash
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
```

---

# Kubelet
```bash
kubelet --version

ps -ef |  grep /usr/bin/kubelet
cat /var/lib/kubelet/config.yaml
grep -i staticpod /var/lib/kubelet/config.yaml # To find static.yaml file path
ps -aux | grep kubelet | grep --color container-runtime-endpoint
```

---

# Manual Scheduling
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: controlplane #Manual Scheduling
  containers:
  -  image: nginx
     name: nginx
```

---

# Multiple Schedulers
```bash
kubectl get pods --namespace=kube-system
kubectl describe pod kube-scheduler-controlplane --namespace=kube-system
```
```bash
nano my-scheduler.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.32.0  # changed
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
```
```bash
nano my-scheduler-config.yaml
```
```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false
```
```bash
nano my-scheduler-configmap.yaml
```
```yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
```
```bash
nano nginx-pod.yaml
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx
```
---

# Secret
#### Stores sensitive data such as passwords and API keys.
# What is a Secret in Kubernetes?
A **Secret** in Kubernetes is an object used to store and manage sensitive information such as passwords, API keys, TLS certificates, and other confidential data. In simple terms, Secret helps you securely store and manage sensitive information in a Kubernetes cluster.

---

## Why Should We Use Secrets?
1. **Enhanced Security**:
   - Sensitive information like passwords or API keys should not be stored directly in configuration files or application code. Secrets store this information securely.
2. **Separation of Sensitive Information from Code**:
   - Using Secrets allows you to separate sensitive information from the application code, making the code cleaner and more secure.
3. **Centralized Management of Sensitive Information**:
   - Secrets enable you to manage sensitive information centrally and easily inject it into applications.
4. **Support for Base64 Encoding**:
   - Information in Secrets is encoded in Base64 by default. While this is not highly secure, it is better than storing information in plain text.

---

## What Can Secrets Contain?
Secrets can include various types of sensitive information, such as:

- **Passwords**: For databases or external services.
- **API Keys**: For accessing external services like AWS, Google Cloud, or Stripe.
- **TLS Certificates**: For secure communications.
- **Authentication Tokens**: For accessing internal or external services.

---

## Where is it Useful?
- **Connecting to Databases**:
  - Securely store database usernames and passwords and inject them into the application.
- **Connecting to External Services**:
  - Store API keys for accessing services like AWS, Google Cloud, or Stripe.
- **Secure Communications**:
  - Store TLS certificates for establishing secure HTTPS connections between services.
- **Managing Sensitive Information in Different Environments**:
  - You can have separate Secrets for Development, Staging, and Production environments.

---

## How Do Secrets Work?
Secrets store sensitive information, and you can inject this information into your application in one of the following ways:

1. **As Environment Variables**:
   - Information from Secrets is injected as environment variables into the application.
2. **As Files Inside the Container**:
   - Information from Secrets is placed as files in a specified path inside the container.

---

## Types of Secrets
### 1. Opaque (General-Purpose)
**What is it?**
This type of Secret is like a general-purpose box where you can store anything, from passwords to API keys or any other key:value data.
**When is it useful?**
When you have specific information that doesn’t fit into other categories, such as database passwords or API keys.
**Example**:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: YWRtaW4=   # Base64-encoded "admin"
  password: cGFzc3dvcmQ=   # Base64-encoded "password"
```

---

### 2. kubernetes.io/tls (For TLS Certificates)
#### What is it?
This type of Secret is designed to store TLS certificates (SSL Certificates) and private keys, used for secure HTTPS communications.
#### When is it useful?
When you want to secure a service or website with HTTPS.
**Example**:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: tls-secret
type: kubernetes.io/tls
data:
  tls.crt: <Base64-encoded certificate>
  tls.key: <Base64-encoded private key>
```

---

### 3. kubernetes.io/dockerconfigjson (For Docker Registry)
#### What is it?
This type of Secret is used to store login credentials (username and password) for Docker registries, where container images are stored.
#### When is it useful?
When you need to pull container images from a private registry.
**Example**:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: docker-registry-secret
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <Base64-encoded Docker config JSON>
```

---
### 4. kubernetes.io/basic-auth (For Basic Authentication)
#### What is it?
This type of Secret is used to store basic authentication information, such as usernames and passwords.
#### When is it useful?
When you have a service that uses simple username and password authentication.
**Example**:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: basic-auth-secret
type: kubernetes.io/basic-auth
data:
  username: YWRtaW4=   # Base64-encoded "admin"
  password: cGFzc3dvcmQ=   # Base64-encoded "password"
```

---
### 5. kubernetes.io/ssh-auth (For SSH Authentication)
#### What is it?
This type of Secret is used to store SSH private keys, which are used for secure connections to servers or other services.
#### When is it useful?
When an application or service needs to connect to another server via SSH.
**Example**:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ssh-auth-secret
type: kubernetes.io/ssh-auth
data:
  ssh-privatekey: <Base64-encoded SSH private key>
```

---

### 6. kubernetes.io/service-account-token (Service Account Token)
#### What is it?
This type of Secret is automatically created by Kubernetes and contains tokens for accessing the cluster API.
#### When is it useful?
When a Pod needs to communicate with the Kubernetes cluster API.
**Example**:
This type of Secret is usually created automatically and does not need to be manually defined.

---

### 7. Custom Types (Custom Secrets)
#### What is it?
If none of the above types fit your needs, you can define a custom Secret type. This is used for specific cases.
#### When is it useful?
When you have a specific application or service that requires a unique format for Secrets.
**Example**:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: custom-secret
type: my-custom-type
data:
  custom-key: <Base64-encoded value>
```

---

### Summary
* Secrets in Kubernetes are used to securely store and manage sensitive information like passwords, API keys, and TLS certificates.
* They help separate sensitive information from application code and provide centralized management.
* Secrets can be injected into applications as environment variables or files inside containers.
* There are various types of Secrets, including Opaque, TLS, Docker Registry, Basic Authentication, SSH Authentication, and Custom Secrets.

| Application | Secret Type |
|---|---|
| Storing general information like passwords or API keys | Opaque |
| Storing TLS certificates for HTTPS | kubernetes.io/tls |
| Storing login credentials for Docker registry | kubernetes.io/dockerconfigjson |
| Storing basic authentication information (username and password) | kubernetes.io/basic-auth |
| Storing SSH private keys for secure connections | kubernetes.io/ssh-auth |
| Storing access tokens for Kubernetes cluster API (automatically created) | kubernetes.io/service-account-token |
| For specific cases requiring custom formats | Custom Types |

---

# Example: Using Secret as a Hidden File (Dotfiles)
This example demonstrates how to use a Secret in Kubernetes to securely place sensitive information as a hidden file (dotfile) inside a container. Let's break it down step by step.

---

## What Do We Have Here?
- **A Secret named `dotfile-secret`**:
  - This Secret contains a hidden file named `.secret-file` with a Base64-encoded value (`dmFsdWUtMg0KDQo=`). When decoded, this value becomes `value-2`.
- **A Pod named `secret-dotfiles-pod`**:
  - This Pod has a container that mounts the Secret as a file in the path `/etc/secret-volume`. The container then lists the contents of this path using the `ls -l` command.

---

## Step-by-Step Manifest Breakdown
### 1. Defining the Secret
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  .secret-file: dmFsdWUtMg0KDQo=
```
### What does it do?
- A Secret named `dotfile-secret` is defined.
- This Secret contains a hidden file named `.secret-file` with a Base64-encoded value (`dmFsdWUtMg0KDQo=`). When decoded, this value becomes `value-2`.
Note:
- Files starting with a dot (`.`) are known as hidden files (dotfiles).

### 2. Defining the Pod
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-dotfiles-pod
spec:
  volumes:
    - name: secret-volume
      secret:
        secretName: dotfile-secret
  containers:
    - name: dotfile-test-container
      image: registry.k8s.io/busybox
      command:
        - ls
        - "-l"
        - "/etc/secret-volume"
      volumeMounts:
        - name: secret-volume
          readOnly: true
          mountPath: "/etc/secret-volume"
```
### What does it do?
- A Pod named `secret-dotfiles-pod` is defined, which includes:
1. Volume:
    - A Volume named `secret-volume` is defined, which uses the `dotfile-secret` Secret. This Volume injects the Secret's data into the container.

2. Container:
    - The container uses the `busybox` image (a lightweight image for testing). It runs the following command:
    ```bash
    ls -l /etc/secret-volume
    ```
    This command lists the contents of the `/etc/secret-volume` path.

3. Volume Mount:
    - The `secret-volume` Volume is mounted to the `/etc/secret-volume` path inside the container. This means the Secret files are placed in this path.

4. Read-Only Mode:
    - The Volume is mounted as read-only (`readOnly: true`), meaning the container cannot modify the Secret files.

### What Happens When This Pod Runs?
When this Pod runs, the container executes the command `ls -l /etc/secret-volume` and lists the contents of the `/etc/secret-volume` path. The output will look something like this:
```
-r--r--r--    1 root     root            8 Jan 25 12:00 .secret-file
```
### What does it mean?
- There is a hidden file named `.secret-file` in the `/etc/secret-volume` path.
- This file is read-only.
- The file size is 8 bytes (because it contains the value `value-2`).

### How to View the File Content?
If you want to view the content of the `.secret-file` inside the container, you can use the following command:

```bash
kubectl exec -it secret-dotfiles-pod -- cat /etc/secret-volume/.secret-file
```
Output:
```
value-2
```

### What Does This Example Show?
1. Managing Hidden Files:
    - This example shows how to securely place hidden files (dotfiles) inside a container.

2. Enhanced Security:
    - Sensitive information (like configuration files or private keys) is securely and read-only injected into the container.

3. Simplicity and Flexibility:
    - Kubernetes makes it easy to use Secrets as files inside containers.

### Summary
- What did we do?
    - We defined a Secret containing a hidden file and injected it as a Volume into a container. Then, we listed the contents of the file.
- Why is this important?
    - This method is very useful for managing sensitive files (like private keys or configuration files) and ensures the security of the information.

---

# ServiceAccount Token Secrets and Docker Config Secrets
Here, we have two main topics: **ServiceAccount Token Secrets** and **Docker Config Secrets**. Let's explore each with examples and explanations.

## 1. ServiceAccount Token Secrets
### What is a ServiceAccount?
- A ServiceAccount is a non-human account in Kubernetes used by applications or services running inside the cluster.
- This account allows Pods to communicate with the cluster API.

### What is a ServiceAccount Token Secret?
- This type of Secret contains a JWT (JSON Web Token) used for authentication and accessing the cluster API.
- Before Kubernetes 1.22: These tokens were long-lived and automatically created for each ServiceAccount.
- After Kubernetes 1.22: Short-lived and more secure tokens, called Bound ServiceAccount Tokens, replaced them. These tokens are automatically managed and do not require manual creation.

### Why Use ServiceAccount Token Secrets?
- When a Pod needs to communicate with the cluster API.
- If you cannot use short-lived tokens (e.g., for specific reasons), you can create a Secret of type `kubernetes.io/service-account-token`.

**Example: Creating a ServiceAccount Token Secret**
1. Create a ServiceAccount:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
    name: sa-name
```
2. Create a Secret for the ServiceAccount:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-sa-sample
  annotations:
    kubernetes.io/service-account.name: "sa-name"
type: kubernetes.io/service-account-token
data:
  extra: YmFyCg==   # Base64-encoded "bar"
```
### What does it do?
- This Secret is linked to the `sa-name` ServiceAccount. Kubernetes automatically populates the `token` field in this Secret.

3. Use the ServiceAccount in a Pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: sa-name
  containers:
  - name: my-container
    image: busybox
    command: ["sh", "-c", "cat /var/run/secrets/kubernetes.io/serviceaccount/token"]
```
### What does it do?
- This Pod uses the `sa-name` ServiceAccount. The JWT token stored in the Secret is automatically placed in the path `/var/run/secrets/kubernetes.io/serviceaccount/token`. The `cat` command prints the content of this file.

When this Pod runs, you can view the JWT token content by running:
```bash
kubectl logs my-pod
```
Output:
```
eyJhbGciOiJSUzI1NiIsImtpZCI6Ik5... (JWT token)
```

## 2. Docker Config Secrets
### What is a Docker Config Secret?
- This type of Secret is used to store login credentials (username and password) for Docker registries, where container images are stored.
- There are two main types:
- `kubernetes.io/dockercfg`: The old format for Docker config files.
- `kubernetes.io/dockerconfigjson`: The new format for Docker config files.

### Why Use Docker Config Secrets?
- When you need to pull container images from a private registry.
- This Secret securely stores login credentials (e.g., username and password).

**Example: Creating a Docker Config Secret**
1. Create a Secret of Type `kubernetes.io/dockercfg`:
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-dockercfg
type: kubernetes.io/dockercfg
data:
  .dockercfg: |
    eyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=
```
#### What does it do?
- This Secret stores Docker registry login credentials. The `.dockercfg` value is Base64-encoded.

2. Create a Secret Using kubectl:
Instead of writing a YAML file, you can use the following command:
```bash
kubectl create secret docker-registry secret-tiger-docker \
     --docker-email=tiger@acme.example \
     --docker-username=tiger \
     --docker-password=pass1234 \
     --docker-server=my-registry.example:5000
```
#### What does it do?
- This command creates a Secret of type `kubernetes.io/dockerconfigjson`.
To view the content of the Secret, run:
```bash
kubectl get secret secret-tiger-docker -o jsonpath='{.data.*}' | base64 -d
```
Output:
```json
{
  "auths": {
    "my-registry.example:5000": {
      "username": "tiger",
      "password": "pass1234",
      "email": "tiger@acme.example",
      "auth": "dGlnZXI6cGFzczEyMzQ="
    }
  }
}
```

---

# Important Notes
- ServiceAccount Tokens:
    - Long-lived tokens are less secure. It's better to use short-lived tokens.
    - If you use long-lived tokens, make sure to restrict their permissions.

- Docker Config Secrets:
- The `auth` value in the Docker config file is Base64-encoded. This encoding is not fully secure, so you should manage the Secret securely.
- It is recommended to use Credential Providers to dynamically and securely provide login credentials.

## Summary
- **ServiceAccount Token Secrets**: Used for authenticating Pods to the cluster API. Short-lived tokens are more secure and recommended.
- **Docker Config Secrets**: Used to store login credentials for Docker registries. You can use the old format (`dockercfg`) or the new format (`dockerconfigjson`).

---

Note: 
1. Secrets are not Encrypted. Only encoded.
2. Secrets are not Encrypted in ETCD.
3. Anyone able to create pods/deployments in the same namespace can access the secrets..
Imperative
```bash
kubectl get secrets
kubectl describe secrets <secret-name>

kubectl create secret generic db-secret <secret-name> --from-literal=<key>=<value>
kubectl create secret generic db-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd

kubectl create secret generic db-secret <secret-name> --from-file=<path-to-file>
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
```
```bash
kubectl -n webhook-demo create secret tls webhook-server-tls \
    --cert "/root/keys/webhook-server-tls.crt" \
    --key "/root/keys/webhook-server-tls.key"
```
---
Declarative
```bash
nano secret-data.yaml
```
```bash
echo -n mysql | base64
# Output is bXlzcWw=
echo -n root | base64
# Output is cm9vdA==
echo -n paswrd | base64
# Output is cGFzd3Jk
```
```yaml
apiVersion: v1 
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk
```
```bash
kubectl create -f secret-data.yaml
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - image: simple-webapp-color
    name: simple-webapp-color
    envFrom:
    - secretRef:
        name: app-secret
```
# Secrets in Pods
ENV
```yaml
envFrom:
  - secretRef:
      name: app-secret
```
SINGLE ENV
```yaml
env:
  - name: DB_Password
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password        
```
Volume
```yaml
volumes:
- name: app-secret-volume
  secret:
    secretName: app-secret
```

---

# [Liveness, Readiness, and Startup Probes in Kubernetes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)

In **Kubernetes**, three types of **probes** are used to check the health of containers:
**1. What is a Container Probe?**
A Container Probe is essentially a health check mechanism used by Kubernetes to check the status of a container in three states:
* **Is the container healthy?** (Liveness)
* **Is the container ready to serve traffic?** (Readiness)
* **Is the container ready to start?** (Startup)
Each Probe performs a test, and based on the test result, Kubernetes decides what to do with the container. For example, it might restart the container, stop sending requests to it, or even terminate it.
**2. Type of Probe?**
## **a. Liveness Probe**
- Checks if the **container is still alive**.
- If the probe **fails**, Kubelet **restarts** the container.
- Useful for detecting **deadlocks**.
- **Example:**
  ```yaml
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 3
    periodSeconds: 5
  ```

## **b. Readiness Probe**
- Checks if the **container is ready** to receive traffic.
- If the probe **fails**, the Pod is **removed from the service endpoints**.
- Useful for **delayed service initialization**.
- When the application crashes, the container is restarted. During this period the service directs users to the available POD, since the POD status is not READY.
- **Example:**
  ```yaml
  readinessProbe:
    tcpSocket:
      port: 8080
    initialDelaySeconds: 3
    periodSeconds: 5
  ```

## **c. Startup Probe**
- Ensures that the **application has started successfully**.
- If the probe **fails**, Kubelet **restarts** the container.
- Useful for **applications that take a long time to start**.
- **Example:**
  ```yaml
  startupProbe:
    exec:
      command: [ "cat", "/tmp/healthy" ]
    initialDelaySeconds: 30
    periodSeconds: 10
  ```

**3. Methods of Executing Probes**
Kubernetes provides three different methods for executing Probes. These methods determine how Kubernetes checks the container's status, which we call Handlers:

**a. HTTP Get Action**
* Kubernetes sends an HTTP request to a specific endpoint (inside the container).
* If the HTTP response code is 200-399, the Probe is successful.
* If an error code (4xx or 5xx) is returned or the request times out, the Probe fails.
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
```
- Kubernetes sends an HTTP request to `/health` on port 8080 every 10 seconds.
- If no response is received within 5 seconds after the container starts, the Probe fails.

**b. TCP Socket**
* Kubernetes establishes a TCP connection to a specific port.
* If the connection is successful, the Probe is successful.
* If the connection fails or times out, the Probe fails.
```yaml
readinessProbe:
  tcpSocket:
    port: 3306
  initialDelaySeconds: 5
  periodSeconds: 10
```
- Here, Kubernetes establishes a TCP connection to port 3306 every 10 seconds. If the connection fails, the Probe fails.

**c. Command Probe (Exec Action)**
* Kubernetes executes a command inside the container.
* If the command executes with exit code 0, the Probe is successful.
* If the command executes with an error code, the Probe fails.
```yaml
livenessProbe:
  exec:
    command:
      - cat
      - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 10
```
- Here, Kubernetes executes the command cat /tmp/healthy every 10 seconds.
- If the file /tmp/healthy exists, the Probe is successful. Otherwise, the Probe fails.

**4. Important Parameters in Probes**
You can use these parameters to configure each Probe:
| Parameter             | Description   |
|-----------------------|---------------|
|initialDelaySeconds    | The time Kubernetes should wait before executing the first probe (e.g., during application startup). |
|periodSeconds          | The interval between each check. |
|timeoutSeconds         | The time Kubernetes waits to receive a response. |
|failureThreshold       | The number of consecutive failures that must occur for the container to be considered unhealthy. |
|successThreshold       | The number of consecutive successes that must occur for the container to be considered healthy or ready (usually set to 1). |


---

## **Key Differences**

| **Probe Type**  | **Purpose** | **Failure Result** | **Best For** |
|------------|------|--------------------------------|------------|
| **Liveness** | Checks if the container is alive | Container **restarts** | Apps that may freeze or deadlock |
| **Readiness** | Checks if the container is ready | Pod is **removed from service** | Apps that take time to initialize |
| **Startup** | Ensures successful startup | Container **restarts** | Apps that require a long startup time |

**Liveness** ensures the **container is running**,  
**Readiness** ensures the **service is available**,  
**Startup** ensures the **application starts correctly**.

---

Ex:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-1
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80
```
---

# Logging & Monitoring
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl top node
kubectl top node --sort-by='memory' --no-headers | head -1
kubectl top pod
kubectl top pod --sort-by='memory' --no-headers | head -1
kubectl top pod <pod-name>

kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>

kubectl logs <pod-name> -c <container-name> | grep WARNING > /opt/test.txt
```

---

# Rolling Updates and Rollbacks
```bash
kubectl create -f nginx-deployment.yaml --record
# We can use the – -record flag to save the command used to create/update a deployment against the revision number.
nano nginx-deployment.yaml # for example change image
kubectl apply -f nginx-deployment.yaml
# or
kubectl set image <resource-type> <resource-name> <container-name>=<new-image>
kubectl set image deployment nginx-deployment nginx-container=nginx:1.25
# info
kubectl rollout status deployment <deployment-name> -n <namespace>
kubectl rollout status deployment nginx-deployment
kubectl rollout history deployment nginx-deployment
kubectl rollout history deployment nginx-deployment --revision=3
# undo
kubectl rollout undo deployment nginx-deployment
kubectl rollout undo deployment nginx-deployment --to-revision=1
# To rollback to specific revision we will use the --to-revision flag.
```
### Important Notes About Rollback

1. **Saving Version History**: Kubernetes, by default, only saves the history of the latest versions. If you want to store more versions, you need to set the `revisionHistoryLimit` in the Deployment. Example:
```yaml
spec:
  revisionHistoryLimit: 5
```
2. **CHANGE-CAUSE**: To record the reason for each change in the history, you can use `annotations`. Example:
```bash
kubectl annotate deployment <deployment-name> kubernetes.io/change-cause="Updated image to nginx:1.22"
```
You can also include CHANGE-CAUSE directly in the manifest. Example:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: devops
  annotations:
    kubernetes.io/change-cause: "Updated image to nginx:1.22 for security fixes"
```
### RollingUpdate  
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```
### Recreate
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
```
**A Complete Example of Pause and Unpause**
Let's assume you have a Deployment named `nginx-deployment`. Now you want to change the Nginx version, but first, you want to pause it to make sure everything is correct.

**1. Pausing the Deployment**
First, you pause the Deployment so that Kubernetes does not apply the changes yet:
```bash
kubectl rollout pause deployment nginx-deployment
# deployment.apps/nginx-deployment paused
```
**2. Applying Changes**
Now, for example, you want to change the Nginx version from 1.22 to 1.23:
```Bash
kubectl set image deployment/nginx-deployment nginx=nginx:1.23
# deployment.apps/nginx-deployment image updated
```
But since the Deployment is paused, Kubernetes saves this change but does not execute it yet.

**3. Checking the Status**
You want to make sure that the changes are done correctly and the Deployment is paused. Check the Deployment status:
```Bash
kubectl rollout status deployment nginx-deployment
# deployment "nginx-deployment" is paused
```
Here, Kubernetes tells you that your Deployment is in the paused state.

**4. Unpausing the Deployment**
When you are sure everything is okay, you unpause the Deployment:
```Bash
kubectl rollout resume deployment nginx-deployment
# deployment.apps/nginx-deployment resumed
```
Now Kubernetes will proceed with the rollout and apply the changes you made.

**5. Checking the Rollout Status**
If you want to see the rollout status, run this command:
```Bash
kubectl rollout status deployment nginx-deployment
# Waiting for deployment "nginx-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
# deployment "nginx-deployment" successfully rolled out
```
**Summary**
- `Pause`: Means telling Kubernetes, "Hold on for now, don't apply the changes."
- `Unpause`: Means telling Kubernetes, "Now apply the changes."
This feature allows you to make changes with more control and step-by-step.

---

# Commands and Arguments
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"
```
```yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "green"]
```
# env
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color
```
---

# Configmaps
Stores key-value configuration data for Pods.

ConfigMap is a Kubernetes tool that allows you to manage application configurations separately from the main code. In simple terms, ConfigMap is a place where you can store information like environment variables, configuration files, or settings parameters and then inject this information into your applications.

---

## What is it Used For?
Imagine you have an application that needs information such as:
- Database URL
- Server port
- API keys
- Specific settings like Debug or Production mode

If you write this information directly into the application code, every time you need to change something, you have to modify the code and redeploy the application. This is time-consuming and makes the application code dependent on the settings. ConfigMap solves this problem. By using ConfigMap, you can keep the settings separate from the code and change them without needing to modify the code.

---

## Why Should We Use ConfigMap?
1. **Separation of Configuration from Code**:
   - This makes the application code cleaner and more maintainable.
2. **Simpler Configuration Management**:
   - You can easily change settings without redeploying the application.
3. **Usage in Different Environments**:
   - For example, you can have one ConfigMap for the Development environment and another for the Production environment.
4. **Enhanced Security**:
   - You can manage sensitive information like database URLs or API keys separately (though for highly sensitive information, it's better to use Secrets).

---

## When is it Good to Use ConfigMap?
- When application settings might change (e.g., database URL or server port).
- When you want to manage settings separately for different environments (Development, Staging, Production).
- When you want to run the application with new settings without changing the code.
- When you want to manage settings centrally and avoid duplicating information in multiple places.

---

## How Does ConfigMap Work?
ConfigMap injects information into the application in one of the following ways:
1. **As Environment Variables**:
   - Information from ConfigMap is injected as environment variables into the application.
2. **As Files Inside the Container**:
   - Information from ConfigMap is placed as files in a specified path inside the container.
3. **As Command-Line Arguments**:
   - You can pass ConfigMap information as arguments to the application.

---

## Advantages of ConfigMap
- **Simplicity and Flexibility**:
  - You can easily change and manage settings.
- **Separation of Configuration from Code**:
  - Makes the application code cleaner and more maintainable.
- **Support for Multiple Injection Methods**:
  - You can inject information as environment variables, files, or arguments.
- **Centralized Configuration Management**:
  - You can manage all settings in one centralized location.

---

## When Should You Not Use ConfigMap?
- **For Sensitive Information**:
  - ConfigMap is not suitable for storing sensitive information like passwords or API keys because the information in ConfigMap is not encrypted. For such cases, you should use Secrets.

---

## Summary
- ConfigMap is a tool for managing application settings separately from the code.
- It helps you change settings easily without needing to modify the code.
- You can use ConfigMap to inject settings into the application as environment variables, files, or arguments.
- For sensitive information, it's better to use Secrets.

---

## ConfigMap in Kubernetes can be used in two main ways:
### 1. As Environment Variables:
In this method, values stored in ConfigMap are injected into the application as environment variables. The application can read these variables and use them for its settings.

**Advantages**:
- Simple and fast for applications that support environment variables.
- No need to change the application code.
**Example**:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_MODE: production
  APP_PORT: "8080"
---
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app-container
    image: my-app:latest
    env:
    - name: APP_MODE
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_MODE
    - name: APP_PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_PORT
```
#### What does it do?
The environment variables `APP_MODE` and `APP_PORT` are injected from the ConfigMap into the container, and the application can use them.

---

### 2. As Files Inside the Container:
In this method, ConfigMap values are placed as files in a specified path inside the container. The application can read these files and use the values inside them.

**Advantages**:
- Suitable for applications that need configuration files.
- More flexibility for managing complex settings.
**Example**:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  config.json: |
    {
      "mode": "production",
      "port": 8080
    }
---
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app-container
    image: my-app:latest
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: app-config
```
#### What does it do?
The config.json file from the ConfigMap is copied to the path /etc/config/config.json inside the container, and the application can read it.

---

### When to Use Each Method?
* **Environment Variables:**
    - When settings are simple and small.
    - When the application supports environment variables.

* **Files Inside the Container:**
    - When settings are more complex (e.g., JSON or YAML files).
    - When the application needs configuration files.

---

## Summary
ConfigMap is used in two main ways in applications:
1. **As Environment Variables**: Simple and fast for small settings.
2. **As Files Inside the Container**: Suitable for complex settings and configuration files.

---

# Difference Between These Two Methods:

| Feature | Environment Variables (Environment Variables) |
|---|---|
| Simplicity | Simpler and faster for small settings. |
| Usage | For environment variables and simple settings. |
| Flexibility | Limited to environment variables. |
| Complexity | Simple and quick implementation. |
| Practical Example | Injecting values like port or application mode (production). |

---

## How:
- **Example 1**: (key:value): Suitable for simple settings and environment variables.
- **Example 2**: (file): Suitable for complex and structured configuration files.

---

```bash
kubectl get cm
kubectl get configmaps
kubectl describe configmaps
kubectl describe cm

kubectl create configmap <config-name> --from-literal=<key>=<value>
kubectl create configmap  webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

kubectl create configmap <config-name> --from-file=<path-to-file>

```
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color
```

---

# ConfigMap in Pods
```bash
nano config-map.yaml
```
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod
```
```bash
kubectl create -f config-map.yaml
```
ENV
```yaml
envFrom:
  - configMapRef:
      name: app-config 
```
SINGLE ENV
```yaml
env:
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:
        name: app-config
        key: APP_COLOR
```
Volume
```yaml
volumes:
- name: app-config-volume
  configMap:
    name: app-config
```

---

# [initContainer](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)
- initContainer is for initial setup, while sidecar is used for ongoing tasks such as logging, proxying, and monitoring.
- initContainer runs before the main container starts.
- It is useful for checking prerequisites, downloading data, configuring settings, and waiting for other services to be ready.
- If an initContainer fails, the Pod will not start.
- Use a shared Volume to exchange data between the initContainer and the main container.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app-container
    image: myapp:latest
  initContainers:
  - name: check-db
    image: busybox
    command: ['sh', '-c', 'until nc -z db-service 3306; do echo waiting for database; sleep 2; done;']
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: \['sh', '-c', 'echo The app is running! && sleep 3600'\]
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;'\]
  - name: init-mydb
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;'\]
```

---

# Horizontal Scaling & Horizontal Pod Autoscaler (HPA)
```bash
kubectl join ...
kubectl scale ...
kubectl edit ...
kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80
```
Or user the yaml file:
```bash
nano autoscale.yml
```
```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 0
```
```bash
kubectl apply -f autoscale.yml
```
```bash
kubectl get hpa
kubectl describe hpa <deployment-name>
kubectl describe hpa nginx-deployment
kubectl events hpa nginx-deployment | grep -i "ScalingReplicaSet"
kubectl events hpa nginx-deployment | grep -i "FailedGetResourceMetric"
```
Another Ex:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kkapp-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50 # Target CPU Utilization (50%)
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300 # Wait 5 minutes before scaling down
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      selectPolicy: Max
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
```
## Explanation of HPA File

- **scaleTargetRef**: This section specifies which Deployment the HPA will target. In this example, `kkapp-deploy` is the target of the HPA.
- **minReplicas and maxReplicas**:
  - `minReplicas`: The minimum number of Pods that should always be running (here, 2).
  - `maxReplicas`: The maximum number of Pods that Kubernetes can create (here, 10).
- **metrics**: Here, we define the criteria based on which the HPA should perform scaling.
  - `type: Resource`: This means that resources like CPU or Memory will be used for decision-making.
  - `averageUtilization`: The goal is that if the average CPU usage exceeds 50%, Kubernetes will increase the number of Pods.
**The `behavior` section in the Horizontal Pod Autoscaler (HPA) file** allows you to control scaling behavior (increasing or decreasing the number of Pods) more precisely. This section includes settings that specify how and how fast to scale up (increase) and scale down (decrease). In this example, only the `scaleDown` section is configured.
**Description of the `behavior` section:**
**`scaleDown`:**
* This section relates to settings for decreasing the number of Pods (scale down).
**`stabilizationWindowSeconds`:**
* This value (here, 300 seconds or 5 minutes) creates a "stabilization window." This means that the HPA waits 5 minutes before deciding to reduce Pods to ensure that the resource reduction is real and not due to short-term fluctuations.
**`policies`:**
* This section contains policies that specify how Pods are reduced.
    * `type: Percent`: This policy operates based on a percentage of the current Pods.
    * `value: 10`: At each step, a maximum of 10% of the current Pods are reduced.
    * `periodSeconds: 60`: This reduction occurs every 60 seconds.
**Summary:**
* **`stabilizationWindowSeconds`:** The HPA waits 5 minutes before reducing Pods to ensure the stability of the resource reduction.
* **`policies`:** The HPA reduces a maximum of 10% of the current Pods every 60 seconds at each step.
These settings help you prevent sudden and excessive Pod reduction and ensure that resource reduction occurs slowly and steadily.

**Important Points:**
* **Setting Resources in Deployment:** Always specify `resources.requests` and `resources.limits` in your Deployment, as HPA makes decisions based on these values.
* **HPA Only for CPU and Memory:** By default, HPA works with CPU and Memory consumption. If you want to use other metrics (such as request count or latency), you need to use Custom Metrics.

**Summary**
* With HPA, you can automatically increase or decrease the number of Deployment Pods based on specified metrics, such as CPU consumption.
* Set the minimum and maximum number of Pods using `minReplicas` and `maxReplicas`.
* Kubernetes always tries to maintain the status based on the target you set (such as 50% CPU consumption).
* **Scaling Time:** HPA does not work instantly. It usually takes a few minutes to apply changes because it needs to monitor the status.

---

# Vertical Pod Autoscaling (VPA)
```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
# To check the installed VPA CRDs
kubectl get crds | grep verticalpodautoscaler

kubectl get deployments -n kube-system | grep vpa 
# vpa-admission-controller, vpa-recommender, vpa-updater

kubectl get vpa
```
```yaml
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: flask-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: flask-app-4
  updatePolicy:
    updateMode: "Off"  # You can set this to "Auto" if you want automatic updates
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
        maxAllowed:
          cpu: 1000m
        controlledResources: ["cpu"]
```
Another Ex:
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: "*"
        minAllowed:
          cpu: "100m"
          memory: "100Mi"
        maxAllowed:
          cpu: "2"
          memory: "4Gi"
```
---

# OS Upgrades

```bash
kubectl get nodes
kubectl get deployments
kubectl get pods -o wide
```
### Before maintenance or removing a Node.
- Evacuates all regular Pods (except DaemonSets).
- Marks the Node as Unschedulable.
```bash
kubectl drain <node-name> --ignore-daemonsets
```
### To temporarily stop scheduling new Pods on a Node.
- Existing Pods remain.
- Node becomes Unschedulable, but no Pods are evicted.
```bash
kubectl cordon <node-name> 
```
### To re-enable scheduling on a Node after an issue is resolved.
- Node becomes Schedulable again.
- New Pods can be scheduled on it.
```bash
kubectl uncordon <node-name>
```

---

# Cluster upgrade
```bash
kubectl get nodes
# node names: controlplane, node01
kubectl describe nodes  controlplane | grep -i taint
# Taints:             <none>
kubectl describe nodes  node01 | grep -i taint
# Taints:             <none>
# This means that both nodes have the ability to schedule workloads on them.
kubeadm upgrade plan
kubectl drain controlplane --ignore-daemonsets
kubectl get nodes
# NAME           STATUS                     ROLES           AGE   VERSION
# controlplane   Ready,SchedulingDisabled   control-plane   23m   v1.31.0
# node01         Ready                      <none>          22m   v1.31.0
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
kubeadm upgrade plan v1.32.0
kubeadm upgrade apply v1.32.0

apt-get install kubelet=1.32.0-1.1

systemctl daemon-reload
systemctl restart kubelet

kubectl uncordon controlplane

# ---

kubectl drain node01 --ignore-daemonsets

ssh node01
vim /etc/apt/sources.list.d/kubernetes.list
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.32.0-1.1
# Upgrade the node 
kubeadm upgrade node
apt-get install kubelet=1.32.0-1.1
systemctl daemon-reload
systemctl restart kubelet
# Type `exit` or `logout` or enter `CTRL + d` to go back to the controlplane node.
kubectl uncordon node01
```

---

# Backup and Restore
## Types:
- Resource Configuration
  ```bash
  kubectl get all --all-namespaces - yaml > all-deploy-service.yaml
  ```
- ETCD Cluster
- Persistent Volumes
## etcd
```bash
kubectl -n kube-system logs etcd-controlplane | grep -i 'etcd-version'
kubectl -n kube-system describe pod etcd-controlplane | grep Image:
kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'
kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'
```
### Take a snapshot
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db
```
### Restore the snapshot
```bash
ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db
```
##### Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the `--data-dir`.


##### Next, update the `/etc/kubernetes/manifests/etcd.yaml`:

##### We have now restored the etcd snapshot to a new path on the controlplane - `/var/lib/etcd-from-backup`, so, the only change to be made in the YAML file, is to change the hostPath for the volume called `etcd-data` from old directory (`/var/lib/etcd`) to the new directory (`/var/lib/etcd-from-backup`).
```
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
```
##### With this change, `/var/lib/etcd` on the container points to `/var/lib/etcd-from-backup` on the controlplane (which is what we want).

##### When this file is updated, the `ETCD` pod is automatically re-created as this is a static pod placed under the `/etc/kubernetes/manifests` directory.

- ##### Note 1: As the ETCD pod has changed it will automatically restart, and also `kube-controller-manager` and `kube-scheduler`. Wait 1-2 to mins for this pods to restart. You can run the command: `watch "crictl ps | grep etcd"` to see when the ETCD pod is restarted.

- ##### Note 2: If the etcd pod is not getting `Ready 1/1`, then restart it by `kubectl delete pod -n kube-system etcd-controlplane` and wait 1 minute.

- ##### Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.


##### If you do change `--data-dir` to `/var/lib/etcd-from-backup` in the ETCD YAML file, make sure that the `volumeMounts` for `etcd-data` is updated as well, with the `mountPath` pointing to `/var/lib/etcd-from-backup` (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

---

# ETCD - Commands
```bash
etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set

etcdctl snapshot save
etcdctl endpoint health
etcdctl get
etcdctl put

kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / \
  --prefix --keys-only --limit=10 / \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --cert /etc/kubernetes/pki/etcd/server.crt \
  --key /etc/kubernetes/pki/etcd/server.key"

kubectl -n kube-system describe pod etcd-controlplane | grep data-dir
ps -ef | grep --color=auto etcd
```

---

# Backup 
```bash
kubectl describe pod -n kube-system etcd-controlplane
kubectl describe pod -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls
#  --advertise-client-urls   =>   --endpoints
kubectl describe pod -n kube-system etcd-controlplane  | grep pki
#  --trusted-ca-file         =>   --cacert
#  --cert-file               =>   --cert
#  --key-file                =>   --key

ETCDCTL_API=3 etcdctl \
  --endpoints=https://192.168.139.36:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/cluster1.db
```

---

# Check the members of the cluster:
```bash
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
   member list
```

---

# TLS Security
```bash
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text
ls -l /etc/kubernetes/pki/etcd/server* | grep .crt

crictl ps -a | grep kube-apiserver # like docker ps -a but in container.d
crictl logs --tail=2 <container-id>

nano /etc/kubernetes/manifests/kube-apiserver.yaml
nano /etc/kubernetes/manifests/etcd.yaml
```
```bash
cat saeed.csr | base64 -w 0
```
```yaml
---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: saeed
spec:
  groups:
  - system:authenticated
  request: <Paste the base64 encoded value of the CSR file>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
```
```bash
kubectl apply -f saeed-csr.yaml
kubectl get csr
kubectl certificate approve saeed

kubectl get csr <csr-name> -o yaml
# Reject that request
kubectl certificate deny <csr-name>
kubectl delete csr <csr-name>
```
---

# kube config
## What is kubeconfig?

**kubeconfig** is a configuration file used by **kubectl** to interact with a **Kubernetes API Server**. It contains details about clusters, users, namespaces, and authentication mechanisms, allowing `kubectl` to communicate with the appropriate cluster.

### 1. Default kubeconfig Path
By default, kubeconfig is stored in:
```sh
~/.kube/config
```
This file enables `kubectl` to manage the cluster without requiring manual authentication each time.

### 2. Viewing kubeconfig
To display the current kubeconfig settings, use:
```sh
kubectl config view
```
This shows all clusters, users, and contexts configured in the file.

### 3. Components of kubeconfig
A sample `kubeconfig` file:
```yaml
apiVersion: v1
kind: Config
clusters:
- name: my-cluster
  cluster:
    server: https://192.168.1.100:6443
    certificate-authority: /path/to/ca.crt
contexts:
- name: my-context
  context:
    cluster: my-cluster
    user: my-user
    namespace: default
current-context: my-context
users:
- name: my-user
  user:
    client-certificate: /path/to/client.crt
    client-key: /path/to/client.key
```

#### **Key Sections:**
- **clusters**: Defines Kubernetes clusters and API server endpoints.
- **users**: Contains authentication credentials.
- **contexts**: Maps a user to a cluster and namespace.
- **current-context**: Specifies the active context used by `kubectl`.

### 4. Managing kubeconfig
#### Change the Active Context:
```sh
kubectl config use-context my-context
```

#### Add a New Cluster:
```sh
kubectl config set-cluster my-new-cluster --server=https://192.168.1.200:6443 --certificate-authority=/path/to/ca.crt
```

#### Add a New User:
```sh
kubectl config set-credentials new-user --client-certificate=/path/to/client.crt --client-key=/path/to/client.key
```

#### Add a New Context:
```sh
kubectl config set-context new-context --cluster=my-new-cluster --user=new-user
```

### 5. Using a Custom kubeconfig File
If kubeconfig is stored in a non-default location, specify it using:
```sh
export KUBECONFIG=/path/to/kubeconfig
kubectl get nodes
```
Or use it directly:
```sh
kubectl --kubeconfig=/path/to/kubeconfig get pods
```

### Summary
- **kubeconfig** is a configuration file that allows `kubectl` to authenticate and interact with Kubernetes clusters.
- The default location is `~/.kube/config`, but multiple kubeconfig files can be used.
- Clusters, users, and contexts can be managed using `kubectl config` commands.
- Custom kubeconfig files can be specified with the `KUBECONFIG` environment variable or the `--kubeconfig` flag.

Proper management of kubeconfig simplifies cluster access and administration.

---

### Example
`cat my-kube-config`
```yaml
apiVersion: v1
kind: Config

clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key

current-context: test-user@development
preferences: {}
```
```bash
# To use context from kubeconfig file
kubectl config --kubeconfig=/root/my-kube-config use-context research
# Output => Switched to context "research"
# Show current-context
kubectl config --kubeconfig=/root/my-kube-config current-context
# Output => research
cat /root/my-kube-config
# Look current-context => current-context: research
```
## Set the my-kube-config file as the default kubeconfig file 
```bash
mv /root/my-kube-config root/.kube/config
```
Or
```bash
nano ~/.bashrc
```
Add the following line to export the variable:
```bash
export KUBECONFIG=/root/my-kube-config
```
```bash
source ~/.bashrc
```
#### Some issue
```bash
kubectl get pod
# error: unable to read client-cert /etc/kubernetes/pki/users/dev-user/developer-user.crt for dev-user due to open /etc/kubernetes/pki/users/dev-user/developer-user.crt: no such file or directory
```
```bash
ls /etc/kubernetes/pki/users/dev-user/
# dev-user.crt   dev-user.csr   dev-user.key
nano my-kube-config
# /etc/kubernetes/pki/users/dev-user/developer-user.crt ===> /etc/kubernetes/pki/users/dev-user/dev-user.crt
```

---

# Troubleshooting and Fixing `admin.kubeconfig`

A kubeconfig file named `admin.kubeconfig` is located in `/root/CKA`. There is an issue with the configuration. Follow these steps to diagnose and fix it.

## **1️⃣ Check the Configuration File**
Inspect the kubeconfig file:
```sh
cat /root/CKA/admin.kubeconfig
```
Or view it in a readable format:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig
```

## **2️⃣ Verify Contexts and Clusters**
Check available contexts and the current context:
```sh
kubectl config get-contexts --kubeconfig=/root/CKA/admin.kubeconfig
kubectl config current-context --kubeconfig=/root/CKA/admin.kubeconfig
```
If the current context is missing or incorrect, set it:
```sh
kubectl config use-context <correct-context-name> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **3️⃣ Check API Server Connectivity**
Ensure the API server is reachable:
```sh
kubectl cluster-info --kubeconfig=/root/CKA/admin.kubeconfig
```
If the connection fails, check the API server address:
```sh
grep "server:" /root/CKA/admin.kubeconfig
```
```
kubectl describe pod -n kube-system kube-apiserver-controlplane kube-system

kubectl describe pod -n kube-system kube-apiserver-controlplane kube-system | grep endpoint
```
Update it if necessary:
```sh
kubectl config set-cluster <cluster-name> --server=https://<control-plane-ip>:6443 --kubeconfig=/root/CKA/admin.kubeconfig
```

## **4️⃣ Validate Certificates**
Check if certificate files exist:
```sh
grep "certificate-authority" /root/CKA/admin.kubeconfig
grep "client-certificate" /root/CKA/admin.kubeconfig
grep "client-key" /root/CKA/admin.kubeconfig
```
If any are missing, update them:
```sh
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/pki/admin.crt \
  --client-key=/etc/kubernetes/pki/admin.key \
  --kubeconfig=/root/CKA/admin.kubeconfig
```

## **5️⃣ Verify User Authentication**
Check if the token is valid:
```sh
kubectl config view --kubeconfig=/root/CKA/admin.kubeconfig -o jsonpath='{.users[*].user.token}'
```
If missing or incorrect, set a new token:
```sh
kubectl config set-credentials admin --token=<correct-token> --kubeconfig=/root/CKA/admin.kubeconfig
```

## **6️⃣ Test Kubernetes Access**
Run the following command to verify that everything works:
```sh
kubectl get nodes --kubeconfig=/root/CKA/admin.kubeconfig
```
If the command succeeds, the issue is resolved! 🚀

---

# RBAC (Role-based Access Control)
### Key Components of RBAC:
- **Role and ClusterRole**:
  - **Role**: Used to define access at the `namespace level`.
  - **ClusterRole**: Used to define access at the `cluster-wide level`.
- **RoleBinding and ClusterRoleBinding**:
  - **RoleBinding**: Binds a Role to a user, group, or service account within a specific `namespace`.
  - **ClusterRoleBinding**: Binds a ClusterRole to a user, group, or service account across the entire `cluster`.

---

## Summary:
- **RBAC** is a tool for managing access in Kubernetes, providing security and fine-grained control.
- It has two main levels: `**Namespace Scope**` and `**Cluster Scope**`.
- With RBAC, you can define who (user or service) has access to what and what actions they can perform.
- It offers many advantages, such as high security and simplified management, but if not configured correctly, it can become complex.

---

```bash
kubectl describe pod kube-apiserver-controlplane -n kube-system
# looking for   =>   --authorization-mode=

kubectl get roles
kubectl get roles --all-namespaces
kubectl get roles -A
kubectl describe role <role-name> -n kube-system
kubectl describe role kube-proxy -n kube-system

kubectl get rolebinding
kubectl get rolebinding --all-namespaces
kubectl get rolebinding -A
kubectl describe rolebinding <rolebinding-name> -n kube-system
kubectl describe rolebinding kube-proxy -n kube-system

kubectl get pods --as dev-user
kubectl auth can-i get pods
kubectl auth can-i get pods --as dev-user

kubectl edit role <role-name> -n <namespace-name>
```
## Create role andd rolebinding
```bash
kubectl create role <role-name> --namespace=default --verb=list,create,delete --resource=pods
kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods

kubectl create rolebinding <rolebinding-name> --namespace=default --role=developer --user=dev-user
kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
```
#### Or use this yaml file
```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

---

# Cluster Roles
```bash
kubectl get clusterroles --no-headers  | wc -l
kubectl get clusterroles --no-headers  -o json | jq '.items | length'
kubectl get clusterrolebindings --no-headers  | wc -l
kubectl get clusterrolebindings --no-headers  -o json | jq '.items | length'
kubectl describe clusterrolebinding cluster-admin
kubectl describe clusterrole cluster-admin
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```
```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
```
```bash
kubectl create -f <file-name>.yaml
```

---

# Service Accounts
### Service Accounts used by machine.
- We do not create ServiceAccounts to give access to human users, nor do we provide ServiceAccounts to them.
- ServiceAccounts are intended for services and workloads running inside the cluster.
  - For example, if a Deployment needs to pull images from a private Docker registry or access Kubernetes resources like ConfigMaps or Secrets, we create a ServiceAccount, assign the necessary permissions (using Roles and RoleBindings), and attach it to the Deployment.
```bash
kubectl get sa
kubectl get serviceaccounts
kubectl get serviceaccount -n kube-system
kubectl describe serviceaccount <serviceaccount-name>
kubectl describe serviceaccount default

kubectl create serviceaccount <serviceaccount-name>
kubectl create serviceaccount dashboard-sa
kubectl create token dashboard-sa
```
Update the Deployment yaml file:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-dashboard
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
```
```bash
kubectl apply -f <FILE-NAME>.yaml
```
##### Or
```bash
kubectl set serviceaccount deploy/<deploy-name> <serviceaccount-name>
kubectl set serviceaccount deploy/web-dashboard dashboard-sa
```

---

# Kubernetes User Access Scenario for `my-project` Namespace
## Note:
In Kubernetes and RBAC, a user means a Kubernetes identity used for access control. It is not the same as a Linux user account. Instead, Kubernetes users are managed through things like certificates (often created using tools like OpenSSL), tokens, or external identity providers—like service accounts, OAuth tokens, or client certificates.

## ✅ Step 0: Admin access to the Kubernetes cluster
The cluster administrator should copy the admin `kubeconfig` file to their local laptop to gain access to the cluster:
```bash
scp user@master-node:~/.kube/config ~/.kube/config
```
Or Copy the `kubeconfig` file from the admin server to your laptop:
```bash
scp ~/.kube/config your-username@your-laptop-ip:~/.kube/config
```
Be sure to check that the IP or Hostname of the laptop and the paths are correct. If the SSH port is other than `22`, use `-P` as well.

---

## ✅ Step 1: Create the Namespace
```bash
kubectl create namespace my-project
```

---

## ✅ Step 2: Access Method 1 — Using ServiceAccount and Secret
### 🔹 2.1 Create a ServiceAccount
```bash
kubectl create serviceaccount dev-team-user --namespace=my-project
```
Check secret:
```bash
kubectl -n my-project get secrets
```
Output
```pgsql
NAME                            TYPE                                  DATA   AGE
dev-team-user-token-abcde       kubernetes.io/service-account-token   3      5s
```
What should we do if it is not built?
```bash
nano manual-secret.yaml
```
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: dev-team-user-token
  annotations:
    kubernetes.io/service-account.name: dev-team-user
type: kubernetes.io/service-account-token
```
```bash
kubectl -n my-project apply -f manual-secret.yaml
```

### 🔹 2.2 Create Role and RoleBinding
**role.yaml**:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev-team-role
  namespace: my-project
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: [""]
  resources: ["serviceaccounts"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]
```

**rolebinding.yaml**:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-team-binding
  namespace: my-project
subjects:
- kind: ServiceAccount
  name: dev-team-user
  namespace: my-project
roleRef:
  kind: Role
  name: dev-team-role
  apiGroup: rbac.authorization.k8s.io
```

Apply them:
```bash
kubectl apply -f role.yaml
kubectl apply -f rolebinding.yaml
```

### 🔹 2.3 Extract Token and Generate kubeconfig
```bash
kubectl cluster-info
```
Get TOKEN
```bash
kubectl -n my-project get secret $(kubectl -n my-project get sa dev-team-user -o jsonpath="{.secrets[0].name}") -o jsonpath="{.data.token}" | base64 -d
```
```bash
SECRET_NAME=$(kubectl -n my-project get sa dev-team-user -o jsonpath="{.secrets[0].name}")
TOKEN=$(kubectl -n my-project get secret $SECRET_NAME -o jsonpath="{.data.token}" | base64 -d)
CLUSTER_NAME=$(kubectl config view --minify -o jsonpath='{.clusters[0].name}')
CLUSTER_SERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
CLUSTER_CA=$(kubectl config view --raw --minify -o jsonpath='{.clusters[0].cluster.certificate-authority-data}')
```

Then create `dev-user.kubeconfig`:
```bash
nano dev-user.kubeconfig
```
```yaml
apiVersion: v1
kind: Config
clusters:
- name: {{CLUSTER_NAME}}
  cluster:
    server: {{CLUSTER_SERVER}}
    certificate-authority-data: {{CLUSTER_CA}}

contexts:
- name: dev-user-context
  context:
    cluster: {{CLUSTER_NAME}}
    namespace: my-project     # Restrict to a specific Namespace
    user: dev-user            # Here

current-context: dev-user-context

users:
- name: dev-user              # And here should be exactly the same.
  user:
    token: {{TOKEN}}          # dev-user ServiceAccount Token
```
* Note: user: dev-user has nothing to do with the actual operating system user. It is just an arbitrary identifier (name) to define the user for whom the token or certificate is defined.
Replace placeholders with the actual values.

## ✅ Step 3: Copy the `dev-user.kubeconfig` file to your colleague's laptop.
```bash
scp dev-user.kubeconfig colleague-username@colleague-laptop-ip:~/.kube/config
```

---

## ✅ Step 2: Access Method 2 — Using OpenSSL and Certificate-based Authentication
### 🔹 2.1 Generate Private Key and CSR
```bash
openssl genrsa -out dev-user.key 2048
openssl req -new -key dev-user.key -out dev-user.csr -subj "/CN=dev-user/O=my-project"
```

### 🔹 2.2 Sign the Certificate with Kubernetes CA
> Use your control plane CA key and cert (`ca.crt` and `ca.key` from `/etc/kubernetes/pki/`):
```bash
openssl x509 -req -in dev-user.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out dev-user.crt -days 365
```

### 🔹 2.3 Generate `dev-user.kubeconfig` with certificate auth
```bash
nano dev-user.kubeconfig
```
```yaml
apiVersion: v1
kind: Config
clusters:
- name: kubernetes
  cluster:
    certificate-authority: /path/to/ca.crt # This file is usually located in the path /etc/kubernetes/pki/ca.crt on the master server.
    server: https://<api-server-ip>:6443 # The value of <api-server-ip> should be the IP or hostname of the server where the Kubernetes master (or control plane) is installed.

users:
- name: dev-user
  user:
    client-certificate: /path/to/dev-user.crt
    client-key: /path/to/dev-user.key

contexts:
- name: dev-user-context
  context:
    cluster: kubernetes
    namespace: my-project
    user: dev-user

current-context: dev-user-context
```
Move the dev-user.crt & dev-user.key to colleague lap-top:
```bash
scp /etc/kubernetes/pki/users/dev-user.crt user@teammate-laptop:/home/user/.kube/
scp /etc/kubernetes/pki/users/dev-user.key user@teammate-laptop:/home/user/.kube/
```
### 💡 Safer alternative:
If you don't want to transfer the files as separate files, you can embed them in `base64` format in kubeconfig:

Commands you need to run on the `master server`:
```bash
export CA=$(base64 -w0 /etc/kubernetes/pki/ca.crt)
export CERT=$(base64 -w0 /etc/kubernetes/pki/users/dev-user.crt)
export KEY=$(base64 -w0 /etc/kubernetes/pki/users/dev-user.key)
```
```bash
nano kubeconfig-dev-user.yaml
```
```yaml
apiVersion: v1
kind: Config
clusters:
- name: kubernetes
  cluster:
    certificate-authority-data: ${CA}
    server: https://<api-server-ip>:6443

users:
- name: dev-user
  user:
    client-certificate-data: ${CERT}
    client-key-data: ${KEY}

contexts:
- name: dev-user-context
  context:
    cluster: kubernetes
    namespace: my-project
    user: dev-user

current-context: dev-user-context
```
```bash
envsubst < kubeconfig-dev-user.yaml > dev-user.kubeconfig
```

### 🔹 2.4 Create RoleBinding for the CN user
```bash
kubectl create rolebinding dev-user-cert-binding \
  --role=dev-team-role \
  --user=dev-user \
  --namespace=my-project
```
Or Use this:
**rolebinding.yaml**:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-user-cert-binding
  namespace: my-project
subjects:
- kind: User
  name: dev-user         # Must be the same as the value in kubeconfig (user section)
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role             # If it was ClusterRole, this value would be ClusterRole
  name: dev-team-role    # The name you created for the Role
  apiGroup: rbac.authorization.k8s.io
```

## ✅ Step 3: Copy the `dev-user.kubeconfig` file to your colleague's laptop.
```bash
scp dev-user.kubeconfig colleague-username@colleague-laptop-ip:~/.kube/config
```
Or
```bash
envsubst < kubeconfig-dev-user.yaml > final-kubeconfig.yaml
```
Now give the `final-kubeconfig.yaml` file to your colleague.
He just needs to type:
```bash
export KUBECONFIG=/path/to/final-kubeconfig.yaml
kubectl get pods
```

---

## Colleague Test
```bash
kubectl config get-contexts
kubectl get pods -n my-project
```
```bash
kubectl auth can-i create deployments --namespace=my-project
```
```bash
kubectl auth can-i create deployments --namespace=my-project --as=dev-user
```
Make sure the file permission is set:
```bash
chmod 600 ~/.kube/config
```

## 🔐 Security Notes
- Token-based authentication is quick and simple.
- Certificate-based auth provides expiration and revocation, better for long-term or production use.
- Always scope RBAC to namespace-level for limited access.

---

## ClusterRole
Reduce duplication using ClusterRole (if multiple namespaces are needed)
If you have multiple namespaces with similar access in the future, it is better to define a ClusterRole to avoid duplicate code.
### 🎯 Scenario
Suppose you have three namespaces for different teams, but they all need to be able to:
- Create deployment
- Create PVC
- Create ServiceAccount
- And only see ingress
Instead of writing a separate Role for each namespace, you can create a ClusterRole and define a RoleBinding for each separate namespace that binds this ClusterRole to the serviceaccount or user of that namespace.
```bash
nano clusterrole-dev-team.yaml
```
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: dev-team-common-role
rules:
- apiGroups: [""]
  resources: ["persistentvolumeclaims", "serviceaccounts"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]
```
```bash
nano project-a-rolebinding.yaml
```
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-team-bind
  namespace: project-a
subjects:
- kind: ServiceAccount
  name: dev-user
  namespace: project-a
roleRef:
  kind: ClusterRole
  name: dev-team-common-role
  apiGroup: rbac.authorization.k8s.io
```
```bash
nano project-b-rolebinding.yaml
```
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-team-bind
  namespace: project-b
subjects:
- kind: ServiceAccount
  name: dev-user
  namespace: project-b
roleRef:
  kind: ClusterRole
  name: dev-team-common-role
  apiGroup: rbac.authorization.k8s.io
```
### ✅ Advantages of this method
- No duplication of access logic in YAML files
- Changes to accesses are only made in ClusterRole
- Simplifies permissions management in the future

---


# Practical Example: Granting Access to a User in Kubernetes
Let's create a user in Kubernetes, grant them access, and configure the necessary files so they can connect to the cluster. We'll use `Role`, `RoleBinding`, `Secret`, and the `kubeconfig` file. Finally, I'll explain how authentication works and how the user logs into the cluster.

---

## Scenario:
Assume you have a user named "Ali" who should only be able to view Pods in a specific Namespace (e.g., `team-a`) (read-only, no deletion or modification). Now, you want to set up this access from start to finish.

---

## Step 1: Create a ServiceAccount for the User
In Kubernetes, users or `ServiceAccounts` are used as "identities" to access the cluster. So, the first step is to create a ServiceAccount for our user.
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ali-sa
  namespace: team-a
```
### What does it do?
This file creates a ServiceAccount named `ali-sa` in the `team-a` Namespace. This ServiceAccount defines the identity of the user "Ali" in the cluster.

---

## Step 2: Create a Role to Limit Access
Now, we need to define the necessary permissions. The user should only be able to read Pods in the `team-a` Namespace, so we create a Role.
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: team-a
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
```
### What does it do?
This Role allows anyone bound to it to:
* Read Pod information (`get`).
* List Pods (`list`).
* Watch Pod changes (`watch`).
* All of this is limited to the `team-a` Namespace.

---

## Step 3: Bind the Role to the ServiceAccount with RoleBinding
Now, we need to bind the Role to the `ali-sa` ServiceAccount so that the permissions defined in the Role are granted to this ServiceAccount.
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pod-reader-binding
  namespace: team-a
subjects:
- kind: ServiceAccount
  name: ali-sa
  namespace: team-a
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
### What does it do?
This file says, "Bind the `pod-reader` Role we defined to the `ali-sa` ServiceAccount in the `team-a` Namespace."

---

## Step 4: Get a Token from the ServiceAccount
Now that the ServiceAccount is created and bound to a Role, we need to get a Token from this ServiceAccount so the user can authenticate.

Command to get the Secret related to the ServiceAccount:
```bash
kubectl get secret -n team-a
```
Output:
```
NAME                        TYPE                                  DATA   AGE
ali-sa-token-abc123         kubernetes.io/service-account-token   3      1m
```
View the Token content:
```bash
kubectl describe secret ali-sa-token-abc123 -n team-a
```
In the output, you'll see the JWT token. It looks something like this:
```
eyJhbGciOiJSUzI1NiIsImtpZCI6Im... (a long string)
```
This Token is what the user "Ali" needs for authentication.

---

## Step 5: Create a kubeconfig File for the User
Now, we need to create a kubeconfig file for the user so they can connect to the cluster. This file stores cluster information, user details, and the Token.
```yaml
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: <CA_DATA> # Cluster certificate (Base64)
    server: https://<KUBERNETES_API_SERVER> # API server address
  name: my-cluster
contexts:
- context:
    cluster: my-cluster
    namespace: team-a # Restrict to a specific Namespace
    user: ali-user
  name: ali-context
current-context: ali-context
users:
- name: ali-user
  user:
    token: <ALI_TOKEN> # Ali's ServiceAccount Token
```
### Explanation:
* `certificate-authority-data`: This is the cluster certificate, which you can get using the command:
 - `kubectl config view --raw`
* `server`: The API server address of the cluster.
* `token`: The JWT token we got from the Secret.

---

## How Does Authentication Work in Kubernetes?
When the user "Ali" connects to the cluster using the kubeconfig file:

1. Authentication:
- Kubernetes checks the JWT token to ensure it is valid (issued by the ali-sa ServiceAccount).

2. Authorization:
- Kubernetes checks whether this ServiceAccount has permission to perform the requested action (e.g., reading Pods). This is done by checking the Role and RoleBinding.

---

## Step 6: How User Ali Uses the Access
1. Save the above file as `config`.
3. Tell user Ali to copy this file to the `~/.kube/config` path:
```bash
mkdir -p ~/.kube
cp config ~/.kube/config
```
3. Now, user Ali can run `kubectl` commands. For example:
```bash
kubectl get pods -n team-a
```
User Ali can now view the list of Pods in the `team-a` Namespace.

---

## Summary:
1. Create a ServiceAccount:
- To define the user's identity.

2. Define a Role:
- To specify permissions.

3. Create a RoleBinding:
- To bind the Role to the ServiceAccount.

4. Get a Token:
- For user authentication.

5. Create a kubeconfig:
- So the user can connect to the cluster.


---

# How to Create a New User in Kubernetes
In Kubernetes, users are not managed directly by the system. Instead, Kubernetes uses kubeconfig files and authentication mechanisms (like certificates, tokens, etc.) to manage users. Below are the steps to create a new user in Kubernetes.
---
1. Generate a Certificate for the New User
To create a new user, you need to generate an SSL certificate for authentication.

    Steps to Generate a Certificate:
    1. Generate a Private Key:
    ```bash
    openssl genpkey -algorithm RSA -out user.key
    ```
    2. Create a Certificate Signing Request (CSR):
    ```bash
    openssl req -new -key user.key -out user.csr -subj "/CN=<username>/O=<group>"
    ```
    - Replace `<username>` with the desired username.

    - Replace `<group>` with the group the user belongs to (optional).

    3. Sign the Certificate Using the Kubernetes CA:
    ```bash
    openssl x509 -req -in user.csr -CA /path/to/ca.crt -CAkey /path/to/ca.key -CAcreateserial -out user.crt -days 365
    ```
    - Replace `/path/to/ca.crt` and `/path/to/ca.key` with the paths to your Kubernetes CA files.

2. Add the User to the `kubeconfig` File
After generating the certificate, add the new user to the `kubeconfig` file.

    1. Add the User to kubeconfig:
    ```bash
    kubectl config set-credentials <username> --client-certificate=user.crt --client-key=user.key
    ```
    2. Create a New Context for the User:
    ```bash
    kubectl config set-context <context-name> --cluster=<cluster-name> --namespace=<namespace> --user=<username>
    ```
    - Replace `<context-name>` with a name for the new context.
    - Replace `<cluster-name>` with the name of the cluster the user will access.
    - Replace `<namespace>` with the default namespace for the user (optional).
    3.Switch to the New Context:
    ```bash
    kubectl config use-context <context-name>
    ```

3. Set Up Permissions (RBAC) for the New User
After creating the user, you need to assign the necessary permissions using **Role-Based Access Control (RBAC)**.

    1. Create a Role or ClusterRole:
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      namespace: <namespace>
      name: <role-name>
    rules:
      - apiGroups: [""]
        resources: ["pods"]
        verbs: ["get", "list", "create"]
    ```
    - Replace `<namespace>` with the namespace where the role applies.
    - Replace `<role-name>` with a name for the role.

    2. Create a RoleBinding or ClusterRoleBinding:
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: <rolebinding-name>
      namespace: <namespace>
    subjects:
      - kind: User
        name: <username>
        apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: Role
      name: <role-name>
      apiGroup: rbac.authorization.k8s.io
    ```
    - Replace `<rolebinding-name>` with a name for the RoleBinding.
    - Replace `<username>` with the username you created.
    - Replace `<role-name>` with the name of the role you created.

    3. Apply the RBAC Configuration:
    ```bash
    kubectl apply -f role.yaml
    kubectl apply -f rolebinding.yaml
    ```

4. Test the New User's Access
To verify that the new user is set up correctly, use the `kubectl auth can-i` command:
```bash
kubectl auth can-i create pods --as <username>
```
### Important Notes:
- Kubernetes does not manage users in an internal database by default. Users are defined through certificates, tokens, or other authentication methods.
- If you are using an external service like LDAP or OIDC for authentication, the process will differ.
- For internal users (e.g., Service Accounts), use `ServiceAccount` instead.

By following these steps, you can create a new user in Kubernetes and assign the necessary permissions.

---

# Image Security
```bash
kubectl create secret --help
kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
```

---

# Security Contexts
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  securityContext:
    runAsUser: 1010
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
```
### nano multi-pod.yaml 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]
```
### run as root 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]
```

---

# Types of Storage in Kubernetes
In Kubernetes, storage is divided into several categories, each designed for specific scenarios:

## 1. EmptyDir
**What is it?**
A temporary storage space that is created when a Pod is created and deleted when the Pod is removed.
**Where is it used?**
For storing temporary data that is only needed during the lifetime of the Pod.
**Example:**
Storing temporary files that do not need to be retained after the Pod is deleted.
**Advantages:**
- Very simple and fast.
**Disadvantages:**
- Data is lost when the Pod is deleted.
**emptyDir configuration example:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:
      sizeLimit: 500Mi
```

## 2. HostPath
**What is it?**
Storage space on the physical disk of the node where the Pod is running.
**Where is it used?**
For scenarios where data must remain on the same node, such as logs.
**Example:**
Storing application logs on the local disk of the node.
**Advantages:**
- Simple and straightforward.
**Disadvantages:**
- If the Pod is moved to another node, it loses access to the data.
**hostPath configuration example:**
```yaml
---
# This manifest mounts /data/foo on the host as /foo inside the
# single container that runs within the hostpath-example-linux Pod.
#
# The mount into the container is read-only.
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-example-linux
spec:
  os: { name: linux }
  nodeSelector:
    kubernetes.io/os: linux
  containers:
  - name: example-container
    image: registry.k8s.io/test-webserver
    volumeMounts:
    - mountPath: /foo
      name: example-volume
      readOnly: true
  volumes:
  - name: example-volume
    # mount /data/foo, but only if that directory already exists
    hostPath:
      path: /data/foo # directory location on host
      type: Directory # this field is optional
```

## 3. Persistent Volume (PV) and Persistent Volume Claim (PVC)
**What is it?**
These are fundamental concepts for managing storage in Kubernetes:
- **PV (Persistent Volume)**: A persistent storage space defined independently.
- **PVC (Persistent Volume Claim)**: A request from an application to use a PV.
**Where is it used?**
For storing important and persistent data.
**Example:**
Storing database data (e.g., MySQL or MongoDB).
**Advantages:**
- Persistent and independent of Pods.
- Can be used with various storage backends (e.g., NFS, AWS EBS, GCP Persistent Disk).
**Disadvantages:**
- Requires more configuration.

## 4. ConfigMap and Secret
**What is it?**
For storing indirect data such as:
- **ConfigMap**: For storing application configurations.
- **Secret**: For storing sensitive data like passwords and API keys.
**Where is it used?**
For storing configurations and sensitive data.
**Example:**
Storing database passwords or server addresses.
**Advantages:**
- Secure and practical.
**Disadvantages:**
- Not suitable for large data.
**ConfigMap configuration example:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox:1.28
      command: ['sh', '-c', 'echo "The app is running!" && tail -f /dev/null']
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level.conf
```

## 5. Cloud Volumes
**What is it?**
Storage that uses cloud services:
- AWS EBS (Elastic Block Store)
- Azure Disk
- Google Cloud Persistent Disk
**Where is it used?**
For clusters running on the cloud.
**Example:**
Storing important database data on AWS EBS.
**Advantages:**
- Scalable and reliable.
**Disadvantages:**
- Only works on the cloud.

## 6. Network File System (NFS)
**What is it?**
A network-based storage space that can be accessed from multiple nodes.
**Where is it used?**
When multiple Pods need access to shared data.
**Example:**
Storing user-uploaded files.
**Advantages:**
- Shareable across nodes.
**Disadvantages:**
- May have slower performance.
**NFS configuration example:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /my-nfs-data
      name: test-volume
  volumes:
  - name: test-volume
    nfs:
      server: my-nfs-server.example.com
      path: /my-nfs-volume
      readOnly: true
```

---

## Components of Storage in Kubernetes
### Persistent Volume (PV):
- A storage resource defined by the admin.
- Like an independent storage space ready for use.
### Persistent Volume Claim (PVC):
- Applications use PVCs to request storage space.
- For example, an application might say: “I need 5 GB of storage.”
### Storage Class:
- A template for defining types of PVs.
- For example, you can say:
  - “Any PV using this Storage Class must be on SSD.”

## Summary:
- Kubernetes offers various storage options for different use cases:
  - **EmptyDir**: Temporary storage.
  - **HostPath**: Local node storage.
  - **PV/PVC**: Persistent storage.
  - **ConfigMap/Secret**: For configurations and sensitive data.
  - **Cloud Volumes**: Cloud-based storage.
  - **NFS**: Network-based shared storage.

---

# Storage Engine
The Storage Engine determines how data is stored and managed. Depending on your needs, you can use different types of Storage Engines:
- **File Storage**: For regular files.
- **Block Storage**: For databases and large volumes of data.
- **Object Storage**: For large and unstructured data.
- **Relational and NoSQL**: For structured or unstructured data.
- **In-Memory**: For high-speed access.
- **Distributed**: For scalability and large systems.

## Comparison of Storage Engine Types

| Storage Engine Type | Application | Disadvantages | Advantages |
|---|---|---|---|
| File Storage | Storing regular files like photos and documents | Difficult to manage at large scale | Simple and understandable |
| Block Storage | Databases and large volumes of data | Complex management | High speed |
| Object Storage | Storing large data like videos and backups | Slower speed compared to Block Storage | High scalability |
| Relational Storage | Relational databases like MySQL | Not suitable for large data | Suitable for structured data |
| NoSQL Storage | Unstructured data like logs | Limited support for complex queries | High scalability |
| In-Memory Storage | Cache and real-time systems | High cost, temporary data | Extremely high speed |
| Distributed Storage | Cloud systems and large data | Complex management | Scalability and fault tolerance |

---

# Persistent Volume (PV) & Persistent Volume Claim (PVC)
- `PV`: Persistent volume configured by the administrator.
  It is independent of the Pod and is connected to the PVC.
- `PVC`: A request from a Pod to use a PV.
- A Pod cannot directly connect to a PV; it must use a PVC.

---

## Access Modes:             
- RWO - ReadWriteOnce         => Only for One Pod.
- ROX - ReadOnlyMany          => Just to Read by Several Pods.
- RWX - ReadWriteMany         => Read and Write by Several Pods.
- RWOP - ReadWriteOncePod     => Only for a Specific Pod.

### Summary of Access Modes Comparison
| Access Mode | Simple Explanation | Practical Example |
|---|---|---|
| ReadWriteOnce (RWO) | Only one node can access the Volume in read-write mode. | Suitable for databases where only one Pod needs access. |
| ReadOnlyMany (ROX) | Multiple nodes can access the Volume in read-only mode. | Suitable for static files or data that only need to be read. |
| ReadWriteMany (RWX) | Multiple nodes can access the Volume in read-write mode. | Suitable for shared systems like NFS or applications that require shared access. |
| ReadWriteOncePod (RWOP) | Only one Pod can access the Volume in read-write mode. (Newer) | Suitable for scenarios where access needs to be restricted to a specific Pod. |

### Important Notes About Access Modes
1. **Compatibility with Storage Systems**:
   - Not all storage systems support all Access Modes:
     - **AWS EBS**: Supports only **RWO** and **RWOP**.
     - **NFS**: Supports **RWX** and **ROX**.
2. **RWOP for Enhanced Security**:
   - If you want to ensure that only a specific Pod has access to the PV, use **ReadWriteOncePod (RWOP)**.
3. **Choosing Based on Application Needs**:
   - Select the Access Mode based on the application's requirements and how it interacts with the data.

---

## Types of **Reclaim Policy**
1. **Retain**:
   - The PV and its data are retained, and the admin must manually decide what to do with them.
   - Example: For sensitive data that should not be automatically deleted.
2. **Recycle**:
   - The PV is automatically cleaned up (e.g., files are deleted) and made available again.
   - This method is no longer widely used.
3. **Delete**:
   - The PV and its stored data are completely deleted.
   - Example: For temporary or non-sensitive data or data that is stored for production or logs that are stored for a short period of time.

| **Reclaim Policy** | **Description** | **Use Case** |
|----------------|--------------------------------------------------|------------------------------|
| **Retain**    | Keeps the PV and its data after PVC deletion, but the PV enters a **Released** state and requires manual cleanup. | When you want to preserve data and reuse the PV with a new PVC. |
| **Delete**    | Deletes the PV and its data when the PVC is deleted (if dynamically provisioned). | Suitable for cloud storage (AWS EBS, GCP PD) where storage should be freed. |
| **Recycle** *(Deprecated)* | Wipes the data (like `rm -rf /data/*`) and makes the PV available again. **(Removed in newer Kubernetes versions.)** | Used in older Kubernetes versions for PV resetting. |

---

### Matching Based on Specifications (Binding)
Kubernetes automatically binds a PVC to a matching PV if the following specifications **match**:
- StorageClass (if used)
- Capacity (
  - The PV capacity can be greater than the requested PVC size, but it must not be smaller.
- Access Modes 
Binding PV to PVC
- When a PVC matches a PV, that PV gets bound to the PVC, and its status changes to Bound.

---

# Lifecycle in Persistent Volume (PV)
A Persistent Volume (PV) in Kubernetes has a defined lifecycle that includes various stages. These stages determine how a PV is created, used, and eventually released. The lifecycle of a PV is as follows:

## 1. Available
- When a PV is created and no application is using it, its status is **Available**.
- This means the PV is ready for use and waiting for a request (PVC) from an application.

## 2. Bound
- When an application creates a Persistent Volume Claim (PVC) and Kubernetes binds that PVC to a PV, the PV's status changes to **Bound**.
- In this state, the PV cannot be used by another application.

## 3. Using
- In this stage, the PV is actively being used by a Pod or application. This is part of the **Bound** stage but specifically refers to the time when data is being written to or read from the PV.

## 4. Released
- When the PVC is deleted or no longer needs the PV, the PV's status changes to **Released**.
- Note that the data inside the PV remains intact and must be manually cleaned up or managed by an admin.

## 5. Reclaimed
- If the admin decides to make the PV available again, they can clean up the previous data and return the PV to the **Available** state.
- This stage depends on the **Reclaim Policy**.

---

# Lifecycle in Storage Engine
The Storage Engine also has its own lifecycle, which determines how data is stored, managed, and eventually deleted. This lifecycle may vary slightly depending on the type of Storage Engine (e.g., File Storage, Block Storage, or Object Storage), but generally includes the following stages:

## 1. Provisioning
- In this stage, storage space is created. This can be done manually (Static Provisioning) or automatically (Dynamic Provisioning).
- Example: Creating a disk on AWS EBS or defining an NFS Share.

## 2. Allocation
- The storage space is allocated to an application or user.
- Example: When a PVC is bound to a PV.

## 3. Utilization
- In this stage, applications use the storage space. Data is written and read.
- Example: A MySQL database stores its data on a PV.

## 4. Deallocation
- When the application no longer needs the storage space, it is deallocated.
- Example: Deleting a PVC causes the PV to enter the **Released** state.

## 5. Deletion
- In this stage, the storage space and its data are completely deleted.
- Example: When the Reclaim Policy is set to **Delete**.

---

# Provisioning in Storage
## 1. Static Provisioning
**What is it?**
- The admin manually defines Persistent Volumes (PVs) and makes them available for use.
**Features**:
- Full control over storage space.
- Suitable for small environments.
**Disadvantages**:
- Requires manual management.
- Time-consuming for large environments.

## 2. Dynamic Provisioning
**What is it?**
- Kubernetes automatically creates PVs based on Persistent Volume Claims (PVCs).
**Features**:
- Automated management.
- Suitable for large and dynamic environments.
**Disadvantages**:
- Requires initial setup for the Provisioner.

---

# Comparison of Lifecycle in Storage Engine and PV

| Feature | Storage Engine Lifecycle | PV Lifecycle |
|---|---|---|
| **Provisioning** | Storage space is created manually or automatically by the admin or system. | PV is created manually (Static) or automatically (Dynamic). |
| **Allocation** | Storage space is allocated to an application or user. | PV is bound to a PVC and ready for use by a Pod. |
| **Utilization** | Data is written or read by applications. | PV is in use by an application, and data is written or read from it. |
| **Deallocation** | Storage space is released and ready for reuse. | PV is released, but data remains. |
| **Deletion** | The storage space and data are completely deleted. | Reclaim Policy (Retain/Recycle/Delete) determines if data is deleted or retained. |

## Summary
- **Lifecycle in PV**:
  - Determines how a PV is created, used, and released. This lifecycle includes the stages: **Available**, **Bound**, **Released**, and **Reclaimed**.
- **Lifecycle in Storage Engine**:
  - Determines how data is stored, managed, and deleted. This lifecycle includes the stages: **Provisioning**, **Allocation**, **Utilization**, **Deallocation**, and **Deletion**.
- **Provisioning**:
  - Can be done manually (Static) or automatically (Dynamic).

---

# Lifecycle of PVC
The lifecycle of a Persistent Volume Claim (PVC) includes the following stages:

## 1. Pending
- When a PVC is created, it initially enters the **Pending** state while Kubernetes searches for or creates a suitable Persistent Volume (PV).

## 2. Bound
- Once Kubernetes finds a suitable PV, the PVC is bound to that PV, and its status changes to **Bound**.

## 3. Released
- When the PVC is deleted, the PV enters the **Released** state. In this state, the data in the PV remains intact, but it is no longer used by the PVC.

## 4. Reclaimed
- Depending on the **Reclaim Policy**, the PV can either be made available again or deleted.

## Summary
- **Pending**: PVC is waiting for a suitable PV.
- **Bound**: PVC is successfully bound to a PV.
- **Released**: PVC is deleted, and the PV is no longer in use.
- **Reclaimed**: PV is either made available again or deleted based on the Reclaim Policy.

---

# PVC Limitation

| Description | Limitation |
|---|---|
| Storage Size | PVC cannot request more than the PV capacity. |
| Access Modes | The storage system may not support the desired mode (e.g., RWX). |
| Dynamic Provisioning | A Storage Class is needed to create a PV, otherwise a PVC cannot be created. |
| Simultaneous PVCs | Limitation on the number of volumes that can be attached to each node (e.g., in AWS EBS). |
| Reclaim Policy | PV and data may be accidentally deleted or remain. |
| Volume Mode | Not all storage systems support Block or Filesystem mode. |
| Cross-Zone Access | PVC and PV must be in the same zone. |
| RWX Limitation in Simultaneous Access | Not all systems support concurrent access from multiple Pods. |

---

# Volume Mode
## Difference between Block and FileSystem
| Feature | Block | FileSystem |
|---|---|---|
| Access Type | Raw, unformatted disk | Filesystem (folders and files) |
| Filesystem Management | Application manages the filesystem | Kubernetes or the storage system manages the filesystem |
| Use Cases | Specific applications requiring direct access | General applications like databases and web servers |
| Complexity | More complex, requires manual management | Simpler, ready to use |
| Speed | Higher speed for performance-sensitive applications | Sufficient for most applications |

## Summary:
### FileSystem Mode:
* More suitable for applications that need file management (such as databases and web servers).
* This mode is simpler, and Kubernetes manages the filesystem.

### Block Mode:
* Used for applications that require direct disk access (such as specific databases or storage systems).
* This mode is more flexible but more complex.

### Types of Block Devices:
* Includes local, network, cloud, and virtual disks, each used for specific scenarios.

---

```bash
nano pod-vol.yaml
# This pod don't use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    # directory location on container
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
```

---

```bash
nano pv-definition.yaml
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
```
```bash
kubectl create -f pv-definition.yaml
```
```bash
nano pvc-definition.yaml
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
```
```bash
kubectl create -f pvc-definition.yaml
```
```bash
nano pod-pv.yaml
# This pod use pvc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
```
---
```bash
kubectl get pv
kubectl get pvc
kubectl delete pvc <pvc-name>
kubectl delete pvc claim-log-1
```

---


# Difference between Object Storage, File Storage, and Block Storage
| Feature | Object Storage | File Storage | Block Storage |
|---|---|---|---|
| Data Structure | Data is stored as objects | Data is stored as files and folders | Data is divided into small blocks |
| Access Speed | Slower than Block and File | Medium | Very Fast |
| Use Cases | Storing large data (like images and videos) | File sharing, web servers | Databases, virtual machines |
| Flexibility | Very High | Limited | Very High |
| Scalability | Highly scalable (suitable for large data) | Limited by server capacity | Limited by disk capacity |
| Cost | Cheaper | Medium | More Expensive |

---

# Storage Class
#### Defines different types of storage and allows dynamic provisioning.

A **Storage Class** in Kubernetes defines the type and properties of storage, such as disk speed, type, encryption, or IOPS. It is used to dynamically provision Persistent Volumes (PVs) for Pods.

### Key Components of a Storage Class:
- **Provisioner**: Specifies the provider (e.g., GCEPersistentDisk, AWS EBS).
- **Parameters**: Configuration options for the storage type (e.g., disk size, SSD/HDD).
- **ReclaimPolicy**: Defines what happens to the PV when it's deleted (e.g., Retain or Delete).
- **VolumeBindingMode**: Determines when the PV is bound to the Pod (immediate or lazy binding).

### Do You Need to Manually Create a Persistent Volume (PV)?
No, when using a **Storage Class**, Kubernetes automatically creates a PV when a Persistent Volume Claim (PVC) is made. You don't need to create a PV manually unless you have specific storage requirements that need to be handled manually. The Storage Class handles provisioning and management of the PV automatically.

```bash
kubectl get sc
kubectl describe sc local-storage
kubectl describe pvc local-pvc | grep -A3 Events
```
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
```
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

---

# Network
## Ingress → Traffic entering a Pod or network.
- Definition: Ingress refers to requests that enter a Pod or network from outside.
- In Kubernetes We have two types of Ingress controls:
  - `NetworkPolicy`: Restricts access between Pods and the network.
  - `Ingress Controller`: Controls access to services from outside the cluster (such as `nginx-ingress`)
## Egress → Traffic leaving a Pod or network.
- Definition: Egress refers to requests that leave a Pod or network.
  - In Kubernetes By default, Pods can connect to anywhere on the internet or internal network.
  - Egress Policy can restrict this access.

---

# **Network Policies**
```bash
kubectl get netpol
kubectl get networkpolicy
kubectl describe networkpolicy
kubectl get svc -n kube-system
```
## When Should You Use Network Policy?
1. **To Enhance Security**:
   - If you don’t want all Pods to freely communicate with each other, you should use Network Policy.
2. **To Restrict Access**:
   - For example, only specific Pods should be allowed to connect to a database.
3. **To Control Outbound Traffic**:
   - For instance, a Pod should only be able to connect to a specific API on the internet.
4. **To Prevent Attacks**:
   - If a Pod gets compromised, you can use Network Policy to restrict its access and prevent the attack from spreading.
Note: Netowrk Plolicy Cubernetis does not work in defects and has a knife to the Networking model you choose when installing.

---

## A Complete NetworkPolicy Manifest Example

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
          podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978
```
This NetworkPolicy defines a set of rules to control incoming (Ingress) and outgoing (Egress) network traffic for Pods with the label `role: db`. Let’s break it down line by line to understand what this Policy does.
1. `podSelector` Section:
- Here, we specify that this Policy applies only to Pods with the label `role: db`.
- Any Pod without this label will not be affected by this Policy.

2. `policyTypes` Section:
- We specify that this Policy controls both incoming (Ingress) and outgoing (Egress) traffic.
- This means we define what traffic can enter the Pod and what traffic can leave the Pod.

3. `ingress` Section (Incoming Traffic Control):
- `from:`:
    - We specify which traffic is allowed to enter the Pod:
        - `ipBlock:`:
            - Only traffic from IPs within the range 172.17.0.0/16 is allowed.
            - However, IPs in the range 172.17.1.0/24 are excluded and not allowed.
        - `namespaceSelector + podSelector:`:
            - Only Pods in Namespaces with the label project: myproject and with the label role: frontend are allowed to access this Pod.
- `ports:`:
    - Only traffic on TCP protocol and port 6379 (commonly used for Redis) is allowed.

4. `egress` Section (Outgoing Traffic Control):
- `to:`:
    - We specify which traffic is allowed to leave the Pod:
        - Only traffic destined for IPs within the range 10.0.0.0/24 is allowed.
- `ports:`:
    - Only traffic on TCP protocol and port 5978 is allowed.

### What Happens When You Apply This Policy?
Incoming Traffic (Ingress):
- Only traffic from IPs within the range `172.17.0.0/16` (except `172.17.1.0/24`) is allowed.
- Or, traffic from Pods with the label `role: frontend` in Namespaces with the label `project: myproject` is allowed.
- Only traffic on port `6379` is allowed.

Outgoing Traffic (Egress):
- Only traffic destined for IPs within the range `10.0.0.0/24` is allowed.
- Only traffic on port `5978` is allowed.

---

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```
```
- Policy Name: internal-policy
- Policy Type: Egress
- Egress Allow: payroll
- Payroll Port: 8080
- Egress Allow: mysql
- MySQL Port: 3306
```
- **Solutions that Support Network Policies:`Kube-router`, `Calico`, `Roman`, `Weave-net`**
- **Solutions that DO NOT Support Network Policies: `Flannel`**

---

# CNI (Container Network Interface)

CNI is a standard or protocol that helps Kubernetes manage the network for Pods. In simple terms, CNI specifies:
- How each Pod gets assigned an IP address.
- How Pods can communicate with each other and with the outside world (e.g., the internet).

---

## Types of CNI Plugins and Their Uses
Let’s explore some popular CNI plugins and their use cases.

### 1. Flannel
**What is it?**
Flannel is one of the simplest and most popular CNI plugins. It creates an overlay network that allows all Pods to communicate with each other.

**Where is it used?**
- For simple and small clusters.
- When you don’t need complex configurations.

**Advantages:**
- Very easy to install and set up.
- Great for getting started with Kubernetes.

**Disadvantages:**
- Lacks advanced features like security or fine-grained traffic control.

---

### 2. Calico
**What is it?**
Calico is a more advanced CNI that focuses on both networking and security. It supports Network Policies.

**Where is it used?**
- For large and complex clusters.
- When you need high security.

**Advantages:**
- Supports Network Policies for precise traffic control.
- Fast performance without needing an overlay.

**Disadvantages:**
- Configuration is more complex compared to Flannel.

---

### 3. Weave Net
**What is it?**
Weave Net is a simple and user-friendly CNI that easily establishes a network between Pods. It also supports security policies.

**Where is it used?**
- For medium-sized clusters.
- When you need simple setup and basic security.

**Advantages:**
- Easy to install.
- Supports Network Policies.

**Disadvantages:**
- Performance may slow down in very large clusters.

---

### 4. Cilium
**What is it?**
Cilium is a modern and advanced CNI that uses eBPF (a new technology in Linux). It is designed for high security and performance.

**Where is it used?**
- For large and complex clusters.
- When you need advanced security and high performance.

**Advantages:**
- Very high security.
- Fast performance.
- Supports Network Policies and advanced features.

**Disadvantages:**
- More complex than other CNIs.

---

### 5. AWS VPC CNI
**What is it?**
This CNI is specifically for clusters running on AWS. It uses Amazon’s VPC network.

**Where is it used?**
- When your cluster is running on AWS.

**Advantages:**
- Full integration with AWS.
- Utilizes VPC features.

**Disadvantages:**
- Only works on AWS.

---

### 6. Azure CNI
**What is it?**
This CNI is specifically for clusters running on Azure. It uses Azure Virtual Network.

**Where is it used?**
- When your cluster is running on Azure.

**Advantages:**
- Full integration with Azure.
- Utilizes Azure Networking features.

**Disadvantages:**
- Only works on Azure.

---

### 7. Google Cloud CNI
**What is it?**
This CNI is specifically for clusters running on Google Cloud. It uses Google’s VPC network.

**Where is it used?**
- When your cluster is running on Google Cloud.

**Advantages:**
- Full integration with Google Cloud.
- Utilizes GCP Networking features.

**Disadvantages:**
- Only works on Google Cloud.

---

## How to Choose the Right CNI?

- **For simple and small clusters**:
  - Use Flannel or Weave Net. They are simple and easy to use.

- **For large and complex clusters**:
  - Use Calico or Cilium. They offer advanced features like security and Network Policies.

- **For clusters on Cloud**:
  - If on AWS: Use AWS VPC CNI.
  - If on Azure: Use Azure CNI.
  - If on Google Cloud: Use Google Cloud CNI.

- **For advanced security**:
  - Use Cilium or Calico.

---

## Summary:
- CNI in Kubernetes is responsible for managing the network between Pods and nodes.
- There are various types of CNIs, each designed for specific scenarios:
  - **Flannel**: Simple and great for beginners.
  - **Calico**: Advanced and great for security.
  - **Weave Net**: Simple and user-friendly.
  - **Cilium**: Modern and great for security and performance.
  - **Cloud-Specific CNIs**: Like AWS VPC CNI, Azure CNI, and Google Cloud CNI.

---

| The CNI binaries are located under | `/opt/cni/bin` by default.|
|---|---|
| `ls /etc/cni/net.d/` | Identify the name of the plugin. |
| `cat /etc/cni/net.d/10-flannel.conflist` | Look at the `type` field |

---

```bash
kubectl exec <pod-name> -- ip route
```

---

What is the range of IP addresses configured for PODs on this cluster?
- `kubectl logs <weave-pod-name> -n kube-system` and look for `ipalloc-range`.

What is the IP Range configured for the services within the cluster?
- Inspect the setting on kube-api server by running on command
- `cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range`

What type of proxy is the kube-proxy configured to use?
- `kubectl logs <kube-proxy-pod-name> -n kube-system`

---

# Ingress Networking 
Summary of Steps:
1. Install Ingress Controller
2. Create Deployment and Service
3. Create Ingress Resource
4. Verify and Debug
5. Configure /etc/hosts (if needed)
6. Final Testing with curl or Browser

---

```bash
kubectl get ing
kubectl get ingress --all-namespaces

kubectl describe ingress --namespace <namespace-name>
kubectl describe ingress --namespace app-space

kubectl edit ingress --namespace <namespace-name>
kubectl edit ingress --namespace app-space
```

---

# Why Do We Need Ingress?
1. **Simpler Management of Incoming Traffic**:
   - Without Ingress, you would need to create a LoadBalancer or NodePort for each service. This becomes complex and costly, especially if you have many services.
2. **Routing Requests Based on Domain or Path**:
   - With Ingress, you can route requests to different services based on specific URLs or domains. For example:
     - `api.example.com` goes to the API service.
     - `app.example.com` goes to the web application.
3. **Advanced Features**:
   - Ingress can provide features like SSL/TLS (for HTTPS), Load Balancing, and even URL rewriting.

---

# How many ways can we define Ingress?
1. Ingress simple with a domain (Single Host)
This type of ingress is when you only have one domain and you want to direct all the requests to a specific service.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: single-host-ingress
spec:
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80
```
- Any request to test.com is directed to My-Service Service on Port 80.

---

2. Ingress with multiple domains (Multiple Hosts)
When you have a few domains and want each domain to be directed to a specific service.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-host-ingress
spec:
  rules:
  - host: foo.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: bar.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service2
            port:
              number: 80
```
- Requests to foo.test.com are directed to Service1 service.
- Requests to Bar.test.com are directed to Service2 service.

---

3. Ingress with different paths (Path-Based Routing)
When you have a domain, but you want to direct the PATH to different services.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: path-based-ingress
spec:
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/api"
        backend:
          service:
            name: api-service
            port:
              number: 80
      - pathType: Prefix
        path: "/blog"
        backend:
          service:
            name: blog-service
            port:
              number: 80
```
- Requests to example.com/api are redirected to the API-Service service.
- Requests to example.com/blog are directed to the blog-service service.

---

4. Ingress with https (TLS)
When you want requests to be directed to the services via HTTPS (SSL Certificate).
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: https-ingress
spec:
  tls:
  - hosts:
    - test.com
    secretName: tls-secret
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80
```
- Requests to https://test.com to the My-Service service.
- The SSL certificate uses TLS-SECRET.

---

5. Ingress with multiple paths and multiple domains (Complex Routing)
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: complex-ingress
spec:
  rules:
  - host: foo.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: bar.test.com
    http:
      paths:
      - pathType: Prefix
        path: "/api"
        backend:
          service:
            name: service2
            port:
              number: 80
      - pathType: Prefix
        path: "/blog"
        backend:
          service:
            name: service3
            port:
              number: 80
```
- Requests to foo.test.com are directed to Service1 service.
- Requests to Bar.test.com/api are directed to Service2 service.
- Requests to Bar.test.com/blog are directed to Service3 service.

---

6. Ingress to rewrite the path (Path Rewrite)
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rewrite-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: test.com
    http:
      paths:
      - pathType: Prefix
        path: "/app"
        backend:
          service:
            name: app-service
            port:
              number: 80
```

---

# Comparison of These Three Types of Controllers

| SCE Ingress Controller | AWS Load Balancer Controller | NGINX Ingress Controller | Feature |
|---|---|---|---|
| Less known             | AWS specific                 | Very popular             | Popularity |
| Specific networks       | AWS only                     | Everywhere (On-Prem and Cloud) | Infrastructure |
| Very high               | Excellent (Security Groups and IAM) | Good | Security |
| More complex            | Requires AWS configuration   | Simple                   | Ease of installation |
| Advanced security       | Auto Scaling, ALB/NLB        | Load Balancing, HTTPS    | Advanced features |

### Summary:
- **NGINX Ingress Controller**:
  - A popular and general-purpose controller that works on any infrastructure. It is simple, fast, and flexible.
- **AWS Load Balancer Controller**:
  - Specifically designed for clusters running on AWS. It uses AWS Load Balancers and is fully integrated with AWS.
- **SCE Ingress Controller**:
  - Designed for specific and high-security scenarios. It is mostly used in specialized networks and sensitive organizations.

---

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  name: ingress-wear-watch
  namespace: app-space
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port: 
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port: 
              number: 8080
        path: /stream
        pathType: Prefix
```
---
```bash
kubectl get svc -n critical-space
# Use this command to know the service and port details.
```
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282
```

---

```bash
kubectl create configmap <configmap-name> --namespace <namespace-name>
kubectl create configmap ingress-nginx-controller --namespace ingress-nginx

kubectl create serviceaccount ingress-nginx --namespace ingress-nginx
kubectl create serviceaccount ingress-nginx-admission --namespace ingress-nginx
```
We need to look at the Deployment's `namespace`, `containerPort`, and Service's `name`, `nodePort`.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
```

---

### Annotations are a way to configure and customize the Ingress Controller.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port: 
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080
```
---
1. Enable Path Rewrite
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
```
#### This annotation causes the request path to be rewritten to the / path.
---
2. Enable WebSocket
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
```
#### This annotation enables WebSocket and enables long-term communication.
---
3. Add Rate Limiting (limiting requests)
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "10"
```
#### This annotation limits the maximum number of requests to 10 requests per second.
---
4. Enable HTTPS Redirect
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
```
#### This annotation redirects all HTTP requests to HTTPS.
---
5. Configuring Load Balancer based on client's primary IP
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/load-balance: "ip_hash"
```
#### This annotation ensures that each client's requests are always sent to a specific server.
---
### CORS (Cross-Origin Resource Sharing) to control access to resources from different domains.
1. Enabling CORS for all requests
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type"
```
2. Only allowing a specific domain
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://example.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Authorization, Content-Type, X-Requested-With"
```
3. Restricting access methods and setting Max Age
```yaml
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://myapp.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-credentials: "true"
    nginx.ingress.kubernetes.io/cors-max-age: "600"
```

---

# IngressClass
- **IngressClass** specifies which Ingress Controller an Ingress resource should use. 
- When you have multiple Ingress Controllers or need specific configurations, **IngressClass** helps you manage traffic correctly. 
- Using **IngressClass** makes incoming traffic management simpler, more flexible, and more compatible with different infrastructures.

---

## Practical Example:
Assume you have a cluster with two types of Ingress Controllers installed:
- **NGINX Ingress Controller** for public services.
- **AWS Load Balancer Controller** for services running on AWS.

---

### Defining IngressClass for NGINX:
```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: nginx-ingress-class
spec:
  controller: k8s.io/ingress-nginx
```
### Defining IngressClass for AWS:
```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: aws-ingress-class
spec:
  controller: ingress.k8s.aws/alb
```
## Using IngressClass in Ingress:
For Public Services:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
spec:
  ingressClassName: nginx-ingress-class
  rules:
  - host: test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: public-service
            port:
              number: 80
```
For AWS Services:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: aws-ingress
spec:
  ingressClassName: aws-ingress-class
  rules:
  - host: aws.test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: aws-service
            port:
              number: 80
```
#### Summary:
- IngressClass helps you specify which Ingress Controller to use for an Ingress resource.
- It simplifies and enhances traffic management, especially when multiple Ingress Controllers are involved.
- By using IngressClass, you can ensure that traffic is routed correctly based on your infrastructure needs.

---

# Gateway API
```bash
# 1. Install the Gateway API resources
kubectl kustomize "https://github.com/nginx/nginx-gateway-fabric/config/crd/gateway-api/standard?ref=v1.5.1" | kubectl apply -f -
# 2. Deploy the NGINX Gateway Fabric CRDs
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/crds.yaml
# 3. Deploy NGINX Gateway Fabric
kubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.1/deploy/nodeport/deploy.yaml
# 4. Verify the Deployment
kubectl get pods -n nginx-gateway
# 5. View the nginx-gateway service
kubectl get svc -n nginx-gateway nginx-gateway -o yaml
# 6. Update the nginx-gateway service to expose ports 30080 for HTTP and 30081 for HTTPS
kubectl patch svc nginx-gateway -n nginx-gateway --type='json' -p='[
  {"op": "replace", "path": "/spec/ports/0/nodePort", "value": 30080},
  {"op": "replace", "path": "/spec/ports/1/nodePort", "value": 30081}
]'
```
---
```bash
nano gateway.yaml
```
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: nginx-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      port: 80
      protocol: HTTP
      allowedRoutes: 
       namespaces: 
        from: All
```
```bash
kubectl apply -f gateway.yaml
kubectl get gateways -n nginx-gateway
```

---

# HTTPRoute
```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: frontend-route
  namespace: default
spec:
  parentRefs:
  - name: nginx-gateway
    namespace: nginx-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: frontend-svc
      port: 80
```

---

# Application Failure
### Change the current namespace
```bash
kubectl config set-context --current --namespace=<namespace-name>
kubectl config set-context --current --namespace=alpha
```
# Check:
- Service
  - match the name of service and DB_HOST in deployment
  - match the `endpoint` and `port` of service and pod
  ```bash
  kubectl get pods -o wide
  kubectl describe svc <service-name>
  kubectl edit svc <service-name>
  ```
- Labels & Selector
  - If labels and selectors are not the same, the resources will not be connected correctly!
  - `selector.matchlabels` in `Deployment` must match `labels` in `Pods`.
  - `selector` in `Service` must match `labels` in `Pods` to send them traffic.
  - `podselector.matchlabels` in `NetworkPolicy` must match `labels` in `Pods`.
  - `backend.service.name` on `Ingress` must match `metadata.name` in `Service`.
  ```bash
  kubectl describe svc <service-name> | grep -i selector
  kubectl describe pod <pod-name> | grep -i label
  ```
- Deployment
  - Check the `env`
  ```bash
  kubectl describe deploy <deploy-name>
  kubectl edit deploy <deploy-name>
  ```
- Pod
  - Check the `Environment`
  ```bash
  kubectl describe pod <pod-name>
  kubectl logs <pod-name>
  kubectl logs <pod-name> -f --previous
  kubectl edit pod <pod-name>
  kubectl replace --force -f /tmp/kubectl-edit.yaml
  ```

---

# Control Plane Failure
If the control plane components are deployed as services, check the states of the services.
```bash
# On master nodes
service kube-apiserver status
service kube-scheduler status
service kube-controller-manager status
# On worker nodes
service kubelet status
service kube-proxy status
```
```bash
kubectl get pods -n kube-system
kubectl describe pod -n kube-system <pod-name>
kubectl logs -n kube-system <pod-name>
# If any Control Plane Components encounter an error, you can fix them using the YAML files in this directory.
cd /etc/kubernetes/manifests

# if pod is pending
kubectl describe pod -n kube-system kube-scheduler-controlplane
nano /etc/kubernetes/manifests/kube-scheduler.yaml

# if scale up/down not working
kubectl describe pod -n kube-system kube-controller-manager-controlplane
kubectl logs -n kube-system kube-controller-manager-controlplane
nano /etc/kubernetes/manifests/kube-controller-manager.yaml
```

---

# Fix Kube-apiserver Access Issue with Proxy
When you configure a system-wide proxy, it might cause issues accessing the Kubernetes API server (`kube-apiserver`).
You may encounter errors such as 403 Forbidden, Connection Refused, or Timeout.

This usually happens because your requests to the API server are being routed through the proxy.

### Solution
You need to configure the `no_proxy` environment variable to bypass the proxy for localhost, 127.0.0.1, and the API server's IP or URL.

Example Command:
```bash
export no_proxy="localhost,127.0.0.1,<api-server-ip-or-url:6443>"
```
Replace <api-server-ip-or-url:6443> with your actual Kubernetes API server address and port.

Note:
- No space after the commas in the `no_proxy` list.
- You can also add other internal services (like etcd, internal load balancers) to this list if needed.

Optional
If you want this setting to persist across reboots or shell sessions, add the line to your shell config file (e.g., `~/.bashrc`, `~/.zshrc`) and reload it:
```bash
echo 'export no_proxy="localhost,127.0.0.1,<api-server-ip-or-url:6443>"' >> ~/.bashrc
source ~/.bashrc
```

---

# Worker Node Failure
```bash
kubectl get nodes
kubectl describe node <node-name>

# Check the possible CPU, memory and disk space on the nodes.
top
htop
df -h

# Check container run time
systemctl status containerd

# Check kubelet
service kubelet status
service kubelet start
service kubelet restart
#or
systemctl status kubelet
systemctl start kubelet
systemctl daemon-reload
systemctl restart kubelet

journalctl -u kubelet -f

nano /var/lib/kubelet/config.yaml
nano /etc/kubernetes/kubelet.conf

# Check the Kubelet Certificates
openssl x509 -in /var/lib/kubelet/worker-1.crt -text
```
# Check kube-proxy
```bash
service kube-proxy status
sudo journalctl -u kube-proxy
```
```bash
# kube-proxy pod
kubectl get pods -n kube-system | grep kube-proxy
kubectl -n kube-system logs <name_of_the_kube_proxy_pod>
kubectl -n kube-system describe configmap kube-proxy
```
```bash
kubectl -n kube-system edit ds kube-proxy
# Correct this path to /var/lib/kube-proxy/config.conf as per the ConfigMap and recreate the kube-proxy pod.
cat /var/lib/kube-proxy/config.conf
```
```yaml
spec:
    containers:
    - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
```
```bash
kubectl get pods -n kube-system | grep kube-proxy
```

---

# JSON PATH
```bash
kubectl get nodes -o json > /opt/outputs/nodes.json

kubectl get node node01 -o json > /opt/outputs/node01.json

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

kubectl config view --kubeconfig=my-kube-config  -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt

kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
```

---

# Job
**For tasks that need to be executed only once, such as database migrations, file processing, or sending batch emails.**
A **Job** is one of the workload types in Kubernetes used for running tasks or short-lived operations. Unlike **Deployment** or **DaemonSet**, which are designed for long-running applications, a **Job** is used for tasks that terminate automatically after completion.

---

## What is a Job Used For?
Imagine you have tasks that need to run only once or a specific number of times, such as:
- **Batch Processing**: For example, processing data from a file or database.
- **Running Scripts**: Like executing a script to apply specific configurations in your infrastructure.
- **Temporary Changes**: Such as restarting a specific service or cleaning up old logs.
- **Data Collection**: For example, taking a database backup or generating daily reports.

---

## How Does It Work?
A Job creates one or more Pods, and these Pods perform the specified task. When all Pods complete their tasks successfully, the Job is considered **Complete**. If a Pod fails or doesn’t complete its task, the Job automatically creates a new Pod to continue the work.

---

## Key Components of a Job
### 1. RestartPolicy
This determines how Kubernetes should behave when a Pod fails or crashes.
- **RestartPolicy = Never**: If a Pod fails, Kubernetes does not restart it. Instead, it creates a new Pod.
- **RestartPolicy = OnFailure**: If a Pod fails, Kubernetes restarts the same Pod.

### 2. Completions and Parallelism
- **Completions**: The number of times the Job must be successfully executed. For example, if `completions: 3`, the Job must succeed 3 times to be considered complete.
- **Parallelism**: The number of Pods that can run simultaneously. For example, if `parallelism: 2`, two Pods will run concurrently.

### 3. Active Deadline Seconds
This sets a time limit for the Job. If the Job does not complete within this time, Kubernetes stops it.

### 4. Backoff Limit
The number of times Kubernetes will attempt to retry a Job in case of failure. For example, if `backoffLimit: 4`, Kubernetes will stop the Job after 4 unsuccessful attempts.

---

## Practical Example of a Job

### Scenario:
Suppose you want to create a Job that runs a simple Bash script and prints a message.

### YAML File:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  completions: 3
  parallelism: 2
  backoffLimit: 4
  activeDeadlineSeconds: 60
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello, Kubernetes!"]
```

### Explanation of the YAML File:
* `kind`: Job: Specifies that this workload is a Job.

* `completions`: 3: The Job must succeed 3 times.
* `parallelism`: 2: A maximum of 2 Pods can run simultaneously.
* `backoffLimit`: 4: If a Pod fails, Kubernetes will retry the Job up to 4 times.
* `activeDeadlineSeconds`: 60: If the Job does not complete within 60 seconds, Kubernetes will stop it.
* `restartPolicy`: Never: If a Pod fails, Kubernetes will not restart it and will create a new Pod instead.
* `containers`: A container using the busybox image that prints the message: Hello, Kubernetes!.

## Summary
* Job is ideal for running short-lived tasks or batch operations.
* It ensures that tasks are completed successfully, even if Pods fail.
* You can control the number of completions, parallelism, retries, and time limits for a Job.

---

# Cleaning Up Pods in Kubernetes Jobs
In Kubernetes, when a Job is executed, the Pods created by the Job remain in the cluster after they finish their tasks (whether they succeed or fail). This means the Pods are not deleted and can still be seen in the following states:
- **Completed**: If the Pod successfully completed its task.
- **Failed**: If the Pod failed for any reason.
This behavior is designed to allow you to review execution history (logs, statuses, and outputs). However, it can lead to some issues:

---

## Why is Cleaning Up Pods Important?
1. **Accumulation of Old Pods (Resource Usage)**:
   - When old Pods are not deleted, a large number of Pods in the `Completed` or `Failed` state accumulate in the cluster.
   - This unnecessarily consumes storage space and management resources like ETCD.
2. **Reduced Cluster Readability and Management**:
   - The presence of many old Pods makes it harder to find active or running Pods.
   - For operations teams, it can be time-consuming to manually review and delete old Pods.
3. **Resource Limitations in the Cluster**:
   - Every Pod in the cluster (even in the `Completed` state) still consumes management resources. Excessive accumulation of such Pods can reduce cluster efficiency.

---

## How to Solve This Problem?
Kubernetes provides several solutions to address this issue, depending on your needs:

### 1. TTL for Jobs (TTL Controller)
Starting from Kubernetes v1.21, the **TTL Controller** feature was introduced. This feature allows Pods associated with a Job to be automatically deleted after the Job completes (whether successfully or unsuccessfully).
- **TTL (Time To Live)**: The amount of time Pods remain in the cluster after the Job finishes.

#### How to Configure TTL?
You can set the `ttlSecondsAfterFinished` field in the Job manifest. For example:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  ttlSecondsAfterFinished: 30
  template:
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello, Kubernetes!"]
      restartPolicy: Never
```
#### Explanation:
- `ttlSecondsAfterFinished: 30`: This means that 30 seconds after the Job reaches the `Completed` or `Failed` state, the Pods and Job will be automatically deleted.

### 2. Manually Deleting Jobs and Pods
If you have Jobs without TTL configured or need to clean them up manually, you can use the following commands:

Delete a Job:
```bash
kubectl delete job <job-name>
```
Delete Pods Associated with a Job:
```bash
kubectl delete pod -l job-name=<job-name>
```
#### Explanation:
- Deleting a Job does not automatically delete its associated Pods unless TTL is configured.
- The second command allows you to delete Pods associated with a specific Job.

### 3. Using CronJob for Periodic Cleanup
If you have many Jobs and don’t want to manage them manually or with TTL, you can create a CronJob to periodically clean up old Pods.

### 4. Configuring TTL Controller for the Entire Cluster
If you want the TTL Controller to be enabled by default for all Jobs in the cluster, you can activate this feature in the cluster settings. This requires access to the API Server configuration.

```bash
kubectl get job
```
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 3
  parallelism: 3
  backoffLimit: 25 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never # Does not automatically restart the terminated container.
```

---

# CronJob
**For tasks that need to be executed regularly or at specific times, such as regular backups, service status monitoring, or sending periodic reports.**
**CronJob** is one of the workload types in Kubernetes used for scheduling Jobs at specific times. If you have experience with `cron` in Linux, CronJob works similarly but within the Kubernetes ecosystem, leveraging Kubernetes' capabilities. CronJob is designed for tasks like scheduled processing, routine email sending, database backups, or running automated scripts. It overcomes the limitations of regular Jobs and allows you to manage Jobs with precise scheduling.

---

## What Problems Does CronJob Solve?

Before CronJob, if you wanted to run Jobs on a schedule, you had to design a separate mechanism or use external tools. CronJob solves this issue entirely and provides the following features:

1. **Precise Scheduling**:
   - CronJob allows you to specify the execution time using the cron format.
   - For example, daily, weekly, or even every minute.
2. **Control Over Old Jobs**:
   - You can define how many successful or failed Jobs are stored in the history.
3. **Automatic Job Management**:
   - You don’t need to manually handle Job reruns or manage old Jobs. Kubernetes handles this automatically.
4. **Native Kubernetes Integration**:
   - Job execution and management are handled natively within Kubernetes, eliminating the need for external tools.

---

## Important Note: CronJob Time Must Align with Kube Controller Manager
When configuring the CronJob schedule, you must ensure that the time zone of the Kubernetes server (i.e., `kube-controller-manager`) matches the time zone you set for the CronJob. If these are not aligned, the CronJob may run at the wrong time.

---

## Interval in CronJob
The interval or time schedule in CronJob is defined using the cron format. The cron format consists of 5 main fields:
```bash
* * * * *
- - - - -
| | | | |
| | | | +----- Day of Week (0 - 7) (0 or 7 = Sunday)
| | | +------- Month (1 - 12)
| | +--------- Day of Month (1 - 31)
| +----------- Hour (0 - 23)
+------------- Minute (0 - 59)
```
### Examples:
- `*/5 * * * *`: Runs every 5 minutes.
- `0 0 * * *`: Runs every day at midnight.
- `0 0 1 * *`: Runs on the first day of every month at midnight.

---

## History in CronJob
Kubernetes allows you to limit the number of successful and failed Jobs stored in the history. This is done using the following fields:
1. **`.spec.successfulJobsHistoryLimit`**:
   - The number of successfully completed Jobs to keep in history.
   - Default value: `3`.
2. **`.spec.failedJobsHistoryLimit`**:
   - The number of failed Jobs to keep in history.
   - Default value: `1`.
You can adjust these values in the CronJob definition.

---

## Where is CronJob Useful?
- **Taking Backups**: Take a database backup every night at midnight.
- **Cleaning Old Logs**: Delete old logs once a week.
- **Sending Reports**: Send a system status report every morning.
- **System Checks**: Run a script every 10 minutes to check the system status.

---

## Summary
- **CronJob** is ideal for running scheduled tasks in Kubernetes.
- It provides precise scheduling, automatic Job management, and native Kubernetes integration.
- You can control the history of successful and failed Jobs to keep your cluster clean and efficient.

```bash
kubectl get cronjob
```
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never
```
**Using these resources helps you manage various processes automatically and with proper scheduling.**

Another Ex:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dice
spec:
  schedule: "*/1 * * * *"  # runs every one minute
  jobTemplate:
    spec:
      completions: 1
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      activeDeadlineSeconds: 20 # If the task is not completed within 20 seconds the job will be fail and pods will be terminated.
      template:
        spec:
          containers:
          - name: dice
            image: kodekloud/throw-dice
          restartPolicy: Never
```

---

# StatefulSets
## Why Use StatefulSets?

**StatefulSets** in Kubernetes are used for applications that require persistent state and stable, unique network identifiers. They are designed for workloads that need to maintain state across pod restarts, like databases or services that need specific network identities.

### What is a StatefulSet?

A **StatefulSet** is a controller in Kubernetes used to manage stateful applications. Unlike **Deployments** (used for stateless apps), StatefulSets ensure that pods maintain their identity, persistent storage, and stable network names.
### **Important Note**:
* Data is not automatically merged between different replicas in a StatefulSet. Each replica in a StatefulSet has its own separate and independent Persistent Volume Claim (PVC), and its data is completely isolated unless you implement a sync or replication mechanism within your application.
### Key Features:
- **Stable, Unique Network Identifiers**: Pods are assigned fixed names (`pod-0`, `pod-1`, ...) that remain consistent across restarts.
- **Persistent Storage**: Each pod gets its own persistent volume (PV) that retains data across pod restarts.
- **Ordered Deployment and Scaling**: Pods are created and terminated in a specific order (e.g., `pod-0` first, then `pod-1`).
- **DNS Names**: Each pod gets a unique DNS name that allows it to be easily discovered in the network.

### Use Cases:
- **Databases**: StatefulSets are ideal for distributed databases like **Cassandra** or **MongoDB** where each pod holds state.
- **Applications** that need stable network identities and persistent storage.

### Example:
For a database like MongoDB, each pod in the StatefulSet might hold part of the database's data and have a stable hostname, such as `mongodb-0`, `mongodb-1`, etc. These names ensure that nodes can communicate reliably and retain their data.

---

Here’s an example of a StatefulSet YAML file for a MongoDB deployment:
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: "mongo"  # Use the headless service for stable networking
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo
        image: mongo:latest
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: mongo-data
          mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: mongo-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
```
Explanation:
- `replicas`: 3: Creates 3 MongoDB pods.
- `volumeClaimTemplates`: Defines persistent storage for each pod. Each pod will get its own persistent volume (PVC).
- `statefulSet Name`: The StatefulSet is named `mongo`, and each pod will get a name like `mongo-0`, `mongo-1`, `mongo-2`.
- `Service`: The `serviceName: "mongo"` allows the StatefulSet to use a headless service to manage networking and DNS for stable pod communication.
Each MongoDB pod gets its own unique volume and network identity, which is ideal for stateful applications like databases.

## Headless Service in Kubernetes

A **Headless Service** in Kubernetes is a service without a cluster IP. Instead of routing traffic to a single IP, it allows direct access to individual pods via DNS. This is particularly useful for stateful applications like **StatefulSets**.

### Key Features:
- **No ClusterIP**: Set `clusterIP: None` to create a headless service with no shared IP.
- **DNS for Individual Pods**: Each pod gets a unique DNS record (e.g., `pod-0.service-name`, `pod-1.service-name`).
- **Direct Pod Access**: Allows applications to connect directly to individual pods, useful for distributed databases or services.

### Use Cases:
- **StatefulSets**: Ensures stable networking for applications like **MongoDB** or **Cassandra**.
- **Direct Pod Communication**: Enables direct communication between pods for applications that require specific network identities.

### Example:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: mongo
spec:
  clusterIP: None  # Headless service
  selector:
    app: mongo
  ports:
    - port: 27017
      targetPort: 27017
```
A headless service provides unique DNS names for each pod, ensuring stable and direct communication, ideal for stateful applications.

---

# Pod Management Policy in StatefulSet

## 1. OrderedReady (Default)
This is the default mode for StatefulSet. When this setting is active, Kubernetes manages Pods in a specific order. Specifically:

- **Creating Pods**: First, `pod-0` is created, and until this Pod reaches the `Ready` state, `pod-1` is not created. After `pod-1` is ready, `pod-2` is created, and so on.
- **Deleting Pods**: Opposite to creation, when you delete a StatefulSet or reduce the number of replicas, Kubernetes starts from the last Pod. For example, `pod-2` is deleted first, then `pod-1`, and finally `pod-0`.
- **Updating Pods**: During updates (e.g., changing the container image), Kubernetes also follows the order. For example, `pod-0` is updated first, then `pod-1`, and so on.

### When to Use OrderedReady?
This mode is ideal for applications that require a specific order for startup or shutdown. Examples include:
- Databases with replica sets (e.g., MongoDB or Cassandra).
- Applications where Pods need to communicate with each other in a specific order.

### What’s the Problem with OrderedReady?
- **Slowness**: Since Pods are created or updated one by one, it may take a long time for all Pods to become ready. For example, if you have 10 Pods, you must wait for each one to become ready sequentially.

---

## 2. Parallel
In this mode, Kubernetes manages Pods in parallel. Specifically:

- **Creating Pods**: All Pods are created simultaneously. For example, `pod-0`, `pod-1`, and `pod-2` start being created at the same time.
- **Deleting Pods**: All Pods can be deleted in parallel.
- **Updating Pods**: Pod updates can also be done in parallel (though this depends on other settings like `partition`).

### When to Use Parallel?
This mode is great for applications that do not require a specific order and need Pods to be created quickly. Examples include:
- Stateless applications that just need to start up quickly.
- When you have a large number of Pods and don’t want to wait for them to be created one by one.

### What’s the Problem with Parallel?
- **Lack of Order**: Since Pods are created simultaneously, applications that require a specific order may face issues.

---

## Comparison: OrderedReady vs. Parallel

### OrderedReady:
1. Kubernetes creates `pod-0` first.
2. When `pod-0` reaches the `Ready` state, `pod-1` is created.
3. When `pod-1` is ready, `pod-2` is created.

### Parallel:
1. Kubernetes creates all Pods (`pod-0`, `pod-1`, `pod-2`) simultaneously.
2. Each Pod becomes `Ready` as soon as it is prepared.

---

## Important Note About Changing Pod Management Policy
You **cannot** change this setting for an existing StatefulSet. If you created a StatefulSet with a specific `podManagementPolicy`, you cannot modify it later. To change it, you must delete the StatefulSet and recreate it with the new settings.

---

## Summary
- **OrderedReady**: Ideal for applications that require a specific order (e.g., databases), but slower.
- **Parallel**: Better for applications where order is not important, and speed is a priority.
The choice between these two depends on your application’s needs and your priorities.

---

# Update Strategy in StatefulSet

The **Update Strategy** in StatefulSet tells Kubernetes how to manage Pods during updates. This feature allows you to specify how and in what order Pods should be updated. There are two main types of update strategies: **RollingUpdate** and **OnDelete**.

---

## 1. RollingUpdate
This strategy allows Kubernetes to update Pods gradually and in a specific order. In this mode, Kubernetes updates Pods incrementally, changing only one Pod at a time. This ensures that your application remains available during the update.

### Features of RollingUpdate:
- **Incremental**: Only one Pod is updated at a time.
- **Availability**: A certain number of Pods are always running during the update, so your application remains available.
- **Configuration**: You can use parameters like `partition` to determine which Pods should be updated.

### Example of RollingUpdate:
Assume you have a StatefulSet with 3 Pods, and you want to update the `nginx` image to a newer version. With RollingUpdate, Kubernetes works as follows:
1. **Update pod-0**: Kubernetes first updates `web2-0`.
2. **Check status**: After `web2-0` reaches the `Ready` state, it moves to `web2-1`.
3. **Update pod-1**: Now, `web2-1` is updated.
4. **Check status**: When `web2-1` is ready, it moves to `web2-2`.
5. **Update pod-2**: Finally, `web2-2` is updated.
This process is gradual, and at least one Pod remains running at all times.

---

## 2. OnDelete
In this strategy, Pods are only updated when you manually delete them. This means you have more control over when and how the update happens, but your application may become unavailable during the update.

### Features of OnDelete:
- **Manual Control**: You must manually delete Pods for Kubernetes to create new ones with the updated version.
- **Potential Downtime**: During the update, the number of running Pods may decrease, and your application may become unavailable.

### Example of OnDelete:
Assume you have a StatefulSet with 3 Pods, and you want to update the `nginx` image. With OnDelete, you must manually delete the Pods:
1. **Delete pod-0**: You delete `web2-0`.
2. **Recreate**: Kubernetes creates a new Pod with the updated image.
3. **Delete pod-1**: Now, you delete `web2-1`.
4. **Recreate**: Kubernetes creates another new Pod.
5. **Delete pod-2**: Finally, you delete `web2-2`, and Kubernetes creates the last Pod.
In this case, you have more control, but your application may become unavailable during the update.

---

## Summary
- **RollingUpdate**: Ensures gradual updates and maintains application availability.
- **OnDelete**: Provides manual control over updates but may result in downtime.
The choice between these strategies depends on your application’s requirements and your need for control versus availability.

## Summary
- **OrderedReady**: Ideal for applications that require a specific order (e.g., databases), but slower.
- **Parallel**: Better for applications where order is not important, and speed is a priority.

---

# Understanding Partition in Kubernetes (StatefulSet)

The **Partition** concept in Kubernetes (especially in StatefulSets) is a feature that allows you to have more control over how Pods are updated. It is typically used with the **RollingUpdate** strategy and lets you specify which Pods should be updated and which should not.

## What is Partition?
- **Partition** is a number that tells Kubernetes from which Pod onward updates should be applied. Pods with an index **less than** this number will **not** be updated.
- This feature is particularly useful for controlling gradual updates and testing changes on a subset of Pods.

## Example:
Assume you have a StatefulSet with 5 Pods (`pod-0`, `pod-1`, `pod-2`, `pod-3`, `pod-4`) and you set the `partition` to `3`. In this case:
- Only Pods with an index **greater than or equal to 3** (i.e., `pod-3` and `pod-4`) will be updated.
- Pods with an index **less than 3** (i.e., `pod-0`, `pod-1`, `pod-2`) will **not** be updated.

## How to Use Partition:
In the StatefulSet manifest, you can specify the `partition` under the `updateStrategy` section. Example:
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 3  # Only Pods with index >= 3 will be updated
```

## Use Cases for Partition:
1. **Testing Changes**: You can test new changes on a subset of Pods first and, if everything works well, update the rest of the Pods.

2. **Controlled Rollouts**: If you want updates to happen gradually and with more control, you can use Partition.

3. **Stability Checks**: By updating Pods incrementally, you can ensure that new changes do not cause issues in the system.

## Practical Example:
Assume you have a StatefulSet with 5 Pods and want to test new changes on pod-3 and pod-4 first. Here’s how it works:
1. Set the `partition` to `3`.
2. Kubernetes will only update `pod-3` and `pod-4`.
3. If the changes are successful, you can set the partition to 0 to update the remaining Pods.

## Important Notes:
1. If you set the `partition` to `0`, **all Pods** will be updated.
2. If you set the `partition` to a **number greater than the number of Pods, no Pods** will be updated.

## Summary:
* Partition is a powerful tool for controlling updates in StatefulSets.
* It allows you to perform updates gradually and in a controlled manner.
* This feature is particularly useful for testing changes and reducing risks associated with updates.

---

## Another Wxample with StorageClass
* When you have a `StorageClass` defined, you only need to use `volumeClaimTemplates` in your `StatefulSet`. Kubernetes will automatically create the corresponding `PVC` and `PV` for each replica.
* It’s important to note that data replication and consistency between database instances are handled internally by the MySQL application itself.
* In case of a pod crash or deletion, the associated PVC will persist (depending on the retention policy settings), and when the pod is recreated, the same PVC will be re-attached and mounted automatically.
```yaml
apiVersion: apps/v1
 kind: StatefulSet
 metadata:
  name: mysql
  namespace: dev
  labels:
    app: mysql
 spec:
  serviceName: hs
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql
          volumeMounts:
            - mountPath: /var/lib/mysql
              name: data-volume
  volumeClaimTemplates:
    - metadata:
        name: data-volume
      spec:
        accessModes:
          - ReadWriteOnce
      storageClassName: google-storage 
      resources:
        requests:
          storage: 500Mi
```
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd
```

---

# **Admission Controller in Kubernetes**

**Admission Controller** is an **internal component in Kubernetes** that intercepts API requests **before they are persisted in etcd** to validate, modify, or reject them. These controllers help enforce **policies** and enhance security within the cluster.

---

### **How Admission Controllers Work**

When an API request (such as creating, updating, or deleting a resource) is made, it follows these steps:

1️⃣ **Authentication & Authorization** → Verifies the user's identity and permissions.  
2️⃣ **Admission Controllers Execution** → Validates or modifies the request before saving it in etcd.  
3️⃣ **Persistence in etcd** → The final state is stored in etcd.  

---

### **Types of Admission Controllers**

Admission Controllers are categorized into two types:

1. **Mutating Admission Controllers** 🚀  
   - Can **modify** requests (e.g., adding default fields).  
   - Example: **MutatingAdmissionWebhook**, which can modify Pod specs before scheduling.  

2. **Validating Admission Controllers** 🔒  
   - Only **validate requests** and decide whether to accept or reject them.  
   - Example: **ValidatingAdmissionWebhook**, which enforces security policies.  

---

### **Example: Mutating Webhook to Add Labels to Pods**

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: add-label-webhook
webhooks:
  - name: add-label.k8s.io
    clientConfig:
      service:
        name: webhook-service
        namespace: default
        path: "/mutate"
    rules:
      - operations: ["CREATE"]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
    admissionReviewVersions: ["v1"]
    sideEffects: None
```
✅ This webhook adds a **specific label** to Pods before they are created.  

---

### **Common Admission Controllers in Kubernetes**

| **Admission Controller**         | **Type**     | **Description** |
|----------------------------------|-------------|--------------------------------------------------|
| **MutatingAdmissionWebhook**     | Mutating    | Allows modifying Kubernetes resource specs. |
| **ValidatingAdmissionWebhook**   | Validating  | Validates and rejects requests if necessary. |
| **NamespaceLifecycle**           | Validating  | Prevents deletion of system-critical namespaces. |
| **PodSecurity** *(New)*         | Validating  | Enforces security policies for Pods (replaces PSP). |
| **ResourceQuota**                | Validating  | Enforces resource limits in namespaces. |

---

### **Summary**
✅ **Admission Controllers** validate and modify API requests before persistence.  
✅ Two types: **Mutating (modifying)** and **Validating (enforcing rules)**.  
✅ Webhooks allow creating custom Admission Controllers.  
✅ Security policies like **PodSecurity** and **ResourceQuota** rely on Admission Controllers.  

---

# **API Version in Kubernetes**  

In Kubernetes, different resources (such as **Pod, Deployment, Service**) are defined as **API Objects**, and each has its own **API Version**.  

---

### **API Version Structure in Kubernetes**  

API versions in Kubernetes follow this format:  

```
<group>/<version>
```
Or for core APIs:
```
<version>
```

Examples:  
- **Pods** → `v1`  *(No group, as it belongs to the Core API)*  
- **Deployments** → `apps/v1`  
- **NetworkPolicies** → `networking.k8s.io/v1`  

---

### **API Version Types and Their Meaning**  

| **API Version**  | **Description** |
|-----------------|--------------------------------------------------|
| **alpha (e.g., v1alpha1)** | Experimental, unstable, may change. **Not recommended for production!** |
| **beta (e.g., v1beta1)** | More stable but still subject to changes. |
| **stable (e.g., v1)** | Fully stable and ready for production use. |

**Example:**  
- `networking.k8s.io/v1alpha1` → **Experimental version**  
- `apps/v1` → **Stable version**  

---

### **How to Check API Version of a Resource?**  

🔹 To list all available API versions:  
```bash
kubectl api-versions
```

🔹 To check the API version of a specific resource:  
```bash
kubectl explain deployment | grep -i "API Version"
```

---

### **Summary**  
✅ Every Kubernetes resource has an **API Version**.  
✅ **Alpha** versions are unstable, **Beta** versions are somewhat stable, and **v1** is fully stable.  
✅ Use **kubectl api-versions** to check available API versions.  

---

# kubectl port-forward
The `kubectl port-forward` command is a quick and secure way to directly access Pods or Services within a Kubernetes cluster. This command is very useful for local testing, debugging, or temporary access to internal services.
```bash
kubectl port-forward <resource-type>/<resource-name> [local-port]:<remote-port> [options]
```
`[local-port]:<remote-port>`:

- **`local-port`**: The port on your local machine (localhost) that will be connected to the resource inside the cluster.
- **`remote-port`**: The port inside the resource (Pod or Service) where the desired service is running.

`[options]`: Optional flags that you can add to the command. Some common options include:

- **`-n <namespace>`**: Specify the namespace if the resource is in a different namespace.
- **`--address`**: The local IP address that the command should listen on (default is `localhost`).
```bash
kubectl port-forward pod/my-pod 8080:80
kubectl port-forward svc/my-service 8080:80

kubectl port-forward pod/my-pod 8080:80 -n my-namespace
kubectl port-forward pod/my-pod 8080:80 --address 192.168.1.100
```
### Stopping Port Forwarding:
To stop Port Forwarding, you can terminate the command by pressing `Ctrl+C` in the terminal.

### After Running This Command:
- You can open your browser and navigate to `http://localhost:8080`.
- Any request sent to `localhost:8080` will be forwarded to port `80` inside the Pod, and the response will be displayed to you.

---

# RoutingAdd commentMore actions
```bash
kubectl port-forward pod/my-app 8080:80 --v=9
```

1. Request to API Server
```log
I0609 11:15:42.123456   12345 round_trippers.go:432] GET https://<api-server>/api/v1/namespaces/default/pods/my-app
```
Here kubectl gets Pod information from the API Server.
2. Response includes Node
```log
I0609 11:15:42.123789   12345 request.go:1065] Response Body: {
  "spec": {
    "nodeName": "node-2"
  },
  ...
}
```
Now `kubectl` understands that the Pod is on `node-2`.
3. Create SPDY Tunnel to Pod via API Server
```log
I0609 11:15:42.123999   12345 portforward.go:400] Creating port forward to pod 'my-app' on node 'node-2'
```
4. Setting the return path
```log
I0609 11:15:42.124111   12345 portforward.go:425] Forwarding from 127.0.0.1:8080 -> 80
```

## Actual port-forward path:
```pgsql
Local App → kubectl → API Server → Kubelet (on target Node) → Container Port
```
```scss
kubectl → API Server → (find pod.nodeName) → Kubelet@node-2 → Pod container:80
         ← Tunnel traffic ←
         ← local port 8080
```
```scss
curl http://localhost:8080  <-- (On a machine outside the cluster)
     ↓
kubectl port-forward process
     ↓
HTTPS/WebSocket to kube-apiserver
     ↓
kube-apiserver → kubelet of Node
     ↓
kubelet → Pod's container:80
```

**Note: In `kubectl port-forward`, `kube-proxy` is not involved at all.**
**Note: Port opening location: Only on the machine running `kubectl` and on `127.0.0.1` `(localhost)` unless you change it with `--address`.**

